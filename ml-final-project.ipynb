{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11569757,"sourceType":"datasetVersion","datasetId":7253662}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install pip3-autoremove\n!pip-autoremove torch torchvision torchaudio -y\n!pip install torch torchvision torchaudio xformers --index-url https://download.pytorch.org/whl/cu121\n!pip install unsloth vllm\n!pip install triton==3.1.0\n!pip install -U pynvml","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T19:10:55.838256Z","iopub.execute_input":"2025-04-26T19:10:55.838752Z","iopub.status.idle":"2025-04-26T19:16:40.806067Z","shell.execute_reply.started":"2025-04-26T19:10:55.838712Z","shell.execute_reply":"2025-04-26T19:16:40.805141Z"}},"outputs":[{"name":"stdout","text":"Collecting pip3-autoremove\n  Downloading pip3_autoremove-1.2.2-py2.py3-none-any.whl.metadata (2.2 kB)\nRequirement already satisfied: pip in /usr/local/lib/python3.11/dist-packages (from pip3-autoremove) (24.1.2)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from pip3-autoremove) (75.1.0)\nDownloading pip3_autoremove-1.2.2-py2.py3-none-any.whl (6.7 kB)\nInstalling collected packages: pip3-autoremove\nSuccessfully installed pip3-autoremove-1.2.2\ncryptography 44.0.2 is installed but cryptography<44 is required\nRedoing requirement with just package name...\npyOpenSSL 25.0.0 is installed but pyOpenSSL<=24.2.1,>=19.1.0 is required\nRedoing requirement with just package name...\nrich 14.0.0 is installed but rich<14,>=12.4.4 is required\nRedoing requirement with just package name...\nfsspec 2025.3.2 is installed but fsspec[http]<=2024.12.0,>=2023.1.0 is required\nRedoing requirement with just package name...\ngymnasium 0.29.0 is installed but gymnasium>=1.0.0 is required\nRedoing requirement with just package name...\nfsspec 2025.3.2 is installed but fsspec==2024.10.0 is required\nRedoing requirement with just package name...\nscipy 1.15.2 is installed but scipy<1.14.0,>=1.7.0 is required\nRedoing requirement with just package name...\ngoogle-api-core 1.34.1 is installed but google-api-core[grpc]<3.0.0dev,>=2.16.0 is required\nRedoing requirement with just package name...\nnotebook 6.5.4 is installed but notebook==6.5.5 is required\nRedoing requirement with just package name...\npandas 2.2.3 is installed but pandas==2.2.2 is required\nRedoing requirement with just package name...\ngoogle-api-core 1.34.1 is installed but google-api-core>=2.19.1 is required\nRedoing requirement with just package name...\nrich 14.0.0 is installed but rich<14,>=12.4.4 is required\nRedoing requirement with just package name...\ntoolz 1.0.0 is installed but toolz<1,>=0.11 is required\nRedoing requirement with just package name...\nscikit-learn 1.2.2 is installed but scikit-learn<2,>=1.3.2 is required\nRedoing requirement with just package name...\nscikit-learn 1.2.2 is installed but scikit-learn>=1.3.1 is required\nRedoing requirement with just package name...\nscikit-learn 1.2.2 is installed but scikit-learn>=1.4.0 is required\nRedoing requirement with just package name...\ngoogle-api-core 1.34.1 is installed but google-api-core<3.0.0dev,>=2.10.2 is required\nRedoing requirement with just package name...\nmatplotlib 3.7.5 is installed but matplotlib>=3.8.0 is required\nRedoing requirement with just package name...\nnltk 3.9.1 is installed but nltk==3.2.4 is required\nRedoing requirement with just package name...\npylibraft-cu12 25.2.0 is installed but pylibraft-cu12==24.12.* is required\nRedoing requirement with just package name...\nrmm-cu12 25.2.0 is installed but rmm-cu12==24.12.* is required\nRedoing requirement with just package name...\nrich 14.0.0 is installed but rich~=13.0 is required\nRedoing requirement with just package name...\nprotobuf 3.20.3 is installed but protobuf<6.0.0dev,>=4.25.2; python_version >= \"3.11\" is required\nRedoing requirement with just package name...\nnvidia-cudnn-cu12 9.3.0.75 is installed but nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\" is required\nRedoing requirement with just package name...\nnvidia-cublas-cu12 12.8.4.1 is installed but nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\" is required\nRedoing requirement with just package name...\nnvidia-cufft-cu12 11.3.3.83 is installed but nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\" is required\nRedoing requirement with just package name...\nnvidia-curand-cu12 10.3.9.90 is installed but nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\" is required\nRedoing requirement with just package name...\nnvidia-cusolver-cu12 11.7.3.90 is installed but nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\" is required\nRedoing requirement with just package name...\nnvidia-cusparse-cu12 12.5.8.93 is installed but nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\" is required\nRedoing requirement with just package name...\nnvidia-nvjitlink-cu12 12.8.93 is installed but nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\" is required\nRedoing requirement with just package name...\nThe 'pycairo>=1.16.0' distribution was not found and is required by the application\nSkipping pycairo\ntorchvision 0.20.1+cu124 (/usr/local/lib/python3.11/dist-packages)\n    torch 2.5.1+cu124 (/usr/local/lib/python3.11/dist-packages)\nnvidia-cudnn-cu12 9.3.0.75 is installed but nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\" is required\nRedoing requirement with just package name...\nnvidia-cublas-cu12 12.8.4.1 is installed but nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\" is required\nRedoing requirement with just package name...\nnvidia-cufft-cu12 11.3.3.83 is installed but nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\" is required\nRedoing requirement with just package name...\nnvidia-curand-cu12 10.3.9.90 is installed but nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\" is required\nRedoing requirement with just package name...\nnvidia-cusolver-cu12 11.7.3.90 is installed but nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\" is required\nRedoing requirement with just package name...\nnvidia-cusparse-cu12 12.5.8.93 is installed but nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\" is required\nRedoing requirement with just package name...\nnvidia-nvjitlink-cu12 12.8.93 is installed but nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\" is required\nRedoing requirement with just package name...\n        nvidia-cuda-nvrtc-cu12 12.4.127 (/usr/local/lib/python3.11/dist-packages)\n        nvidia-cuda-runtime-cu12 12.4.127 (/usr/local/lib/python3.11/dist-packages)\n        nvidia-cuda-cupti-cu12 12.4.127 (/usr/local/lib/python3.11/dist-packages)\n        nvidia-cudnn-cu12 9.3.0.75 (/usr/local/lib/python3.11/dist-packages)\n        nvidia-nccl-cu12 2.21.5 (/usr/local/lib/python3.11/dist-packages)\n        nvidia-nvtx-cu12 12.4.127 (/usr/local/lib/python3.11/dist-packages)\n        triton 3.1.0 (/usr/local/lib/python3.11/dist-packages)\n        sympy 1.13.1 (/usr/local/lib/python3.11/dist-packages)\n            mpmath 1.3.0 (/usr/local/lib/python3.11/dist-packages)\ntorchaudio 2.5.1+cu124 (/usr/local/lib/python3.11/dist-packages)\n    torch 2.5.1+cu124 (/usr/local/lib/python3.11/dist-packages)\nnvidia-cudnn-cu12 9.3.0.75 is installed but nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\" is required\nRedoing requirement with just package name...\nnvidia-cublas-cu12 12.8.4.1 is installed but nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\" is required\nRedoing requirement with just package name...\nnvidia-cufft-cu12 11.3.3.83 is installed but nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\" is required\nRedoing requirement with just package name...\nnvidia-curand-cu12 10.3.9.90 is installed but nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\" is required\nRedoing requirement with just package name...\nnvidia-cusolver-cu12 11.7.3.90 is installed but nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\" is required\nRedoing requirement with just package name...\nnvidia-cusparse-cu12 12.5.8.93 is installed but nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\" is required\nRedoing requirement with just package name...\nnvidia-nvjitlink-cu12 12.8.93 is installed but nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\" is required\nRedoing requirement with just package name...\n        nvidia-cuda-nvrtc-cu12 12.4.127 (/usr/local/lib/python3.11/dist-packages)\n        nvidia-cuda-runtime-cu12 12.4.127 (/usr/local/lib/python3.11/dist-packages)\n        nvidia-cuda-cupti-cu12 12.4.127 (/usr/local/lib/python3.11/dist-packages)\n        nvidia-cudnn-cu12 9.3.0.75 (/usr/local/lib/python3.11/dist-packages)\n        nvidia-nccl-cu12 2.21.5 (/usr/local/lib/python3.11/dist-packages)\n        nvidia-nvtx-cu12 12.4.127 (/usr/local/lib/python3.11/dist-packages)\n        triton 3.1.0 (/usr/local/lib/python3.11/dist-packages)\n        sympy 1.13.1 (/usr/local/lib/python3.11/dist-packages)\n            mpmath 1.3.0 (/usr/local/lib/python3.11/dist-packages)\ntorch 2.5.1+cu124 (/usr/local/lib/python3.11/dist-packages)\nnvidia-cudnn-cu12 9.3.0.75 is installed but nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\" is required\nRedoing requirement with just package name...\nnvidia-cublas-cu12 12.8.4.1 is installed but nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\" is required\nRedoing requirement with just package name...\nnvidia-cufft-cu12 11.3.3.83 is installed but nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\" is required\nRedoing requirement with just package name...\nnvidia-curand-cu12 10.3.9.90 is installed but nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\" is required\nRedoing requirement with just package name...\nnvidia-cusolver-cu12 11.7.3.90 is installed but nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\" is required\nRedoing requirement with just package name...\nnvidia-cusparse-cu12 12.5.8.93 is installed but nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\" is required\nRedoing requirement with just package name...\nnvidia-nvjitlink-cu12 12.8.93 is installed but nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\" is required\nRedoing requirement with just package name...\n    nvidia-cuda-nvrtc-cu12 12.4.127 (/usr/local/lib/python3.11/dist-packages)\n    nvidia-cuda-runtime-cu12 12.4.127 (/usr/local/lib/python3.11/dist-packages)\n    nvidia-cuda-cupti-cu12 12.4.127 (/usr/local/lib/python3.11/dist-packages)\n    nvidia-cudnn-cu12 9.3.0.75 (/usr/local/lib/python3.11/dist-packages)\n    nvidia-nccl-cu12 2.21.5 (/usr/local/lib/python3.11/dist-packages)\n    nvidia-nvtx-cu12 12.4.127 (/usr/local/lib/python3.11/dist-packages)\n    triton 3.1.0 (/usr/local/lib/python3.11/dist-packages)\n    sympy 1.13.1 (/usr/local/lib/python3.11/dist-packages)\n        mpmath 1.3.0 (/usr/local/lib/python3.11/dist-packages)\nFound existing installation: nvidia-cuda-runtime-cu12 12.4.127\nUninstalling nvidia-cuda-runtime-cu12-12.4.127:\n  Successfully uninstalled nvidia-cuda-runtime-cu12-12.4.127\nFound existing installation: triton 3.1.0\nUninstalling triton-3.1.0:\n  Successfully uninstalled triton-3.1.0\nFound existing installation: sympy 1.13.1\nUninstalling sympy-1.13.1:\n  Successfully uninstalled sympy-1.13.1\nFound existing installation: torchaudio 2.5.1+cu124\nUninstalling torchaudio-2.5.1+cu124:\n  Successfully uninstalled torchaudio-2.5.1+cu124\nFound existing installation: nvidia-cuda-nvrtc-cu12 12.4.127\nUninstalling nvidia-cuda-nvrtc-cu12-12.4.127:\n  Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.4.127\nFound existing installation: mpmath 1.3.0\nUninstalling mpmath-1.3.0:\n  Successfully uninstalled mpmath-1.3.0\nFound existing installation: nvidia-cudnn-cu12 9.3.0.75\nUninstalling nvidia-cudnn-cu12-9.3.0.75:\n  Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\nFound existing installation: nvidia-cuda-cupti-cu12 12.4.127\nUninstalling nvidia-cuda-cupti-cu12-12.4.127:\n  Successfully uninstalled nvidia-cuda-cupti-cu12-12.4.127\nFound existing installation: nvidia-nccl-cu12 2.21.5\nUninstalling nvidia-nccl-cu12-2.21.5:\n  Successfully uninstalled nvidia-nccl-cu12-2.21.5\nFound existing installation: torchvision 0.20.1+cu124\nUninstalling torchvision-0.20.1+cu124:\n  Successfully uninstalled torchvision-0.20.1+cu124\nFound existing installation: nvidia-nvtx-cu12 12.4.127\nUninstalling nvidia-nvtx-cu12-12.4.127:\n  Successfully uninstalled nvidia-nvtx-cu12-12.4.127\nFound existing installation: torch 2.5.1+cu124\nUninstalling torch-2.5.1+cu124:\n  Successfully uninstalled torch-2.5.1+cu124\nLooking in indexes: https://download.pytorch.org/whl/cu121\nCollecting torch\n  Downloading https://download.pytorch.org/whl/cu121/torch-2.5.1%2Bcu121-cp311-cp311-linux_x86_64.whl (780.5 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m780.5/780.5 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting torchvision\n  Downloading https://download.pytorch.org/whl/cu121/torchvision-0.20.1%2Bcu121-cp311-cp311-linux_x86_64.whl (7.3 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.3/7.3 MB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting torchaudio\n  Downloading https://download.pytorch.org/whl/cu121/torchaudio-2.5.1%2Bcu121-cp311-cp311-linux_x86_64.whl (3.4 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m85.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hCollecting xformers\n  Downloading https://download.pytorch.org/whl/cu121/xformers-0.0.29.post1-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (1.0 kB)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\nCollecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m84.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m41.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m108.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n  Downloading https://download.pytorch.org/whl/cu121/nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n  Downloading https://download.pytorch.org/whl/cu121/nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n  Downloading https://download.pytorch.org/whl/cu121/nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch)\n  Downloading https://download.pytorch.org/whl/cu121/nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n  Downloading https://download.pytorch.org/whl/cu121/nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n  Downloading https://download.pytorch.org/whl/cu121/nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-nccl-cu12==2.21.5 (from torch)\n  Downloading https://download.pytorch.org/whl/nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch)\n  Downloading https://download.pytorch.org/whl/cu121/nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting triton==3.1.0 (from torch)\n  Downloading https://download.pytorch.org/whl/triton-3.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.5 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.5/209.5 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting sympy==1.13.1 (from torch)\n  Downloading https://download.pytorch.org/whl/sympy-1.13.1-py3-none-any.whl (6.2 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m91.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.8.93)\nCollecting mpmath<1.4,>=1.1.0 (from sympy==1.13.1->torch)\n  Downloading https://download.pytorch.org/whl/mpmath-1.3.0-py3-none-any.whl (536 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (1.26.4)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.1.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (2.4.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->torchvision) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->torchvision) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->torchvision) (2024.2.0)\nDownloading https://download.pytorch.org/whl/cu121/xformers-0.0.29.post1-cp311-cp311-manylinux_2_28_x86_64.whl (15.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.3/15.3 MB\u001b[0m \u001b[31m93.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: mpmath, triton, sympy, nvidia-nvtx-cu12, nvidia-nccl-cu12, nvidia-cusparse-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusolver-cu12, nvidia-cudnn-cu12, torch, torchaudio, xformers, torchvision\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.8.93\n    Uninstalling nvidia-cusparse-cu12-12.5.8.93:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.8.93\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.9.90\n    Uninstalling nvidia-curand-cu12-10.3.9.90:\n      Successfully uninstalled nvidia-curand-cu12-10.3.9.90\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.3.3.83\n    Uninstalling nvidia-cufft-cu12-11.3.3.83:\n      Successfully uninstalled nvidia-cufft-cu12-11.3.3.83\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.8.4.1\n    Uninstalling nvidia-cublas-cu12-12.8.4.1:\n      Successfully uninstalled nvidia-cublas-cu12-12.8.4.1\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.7.3.90\n    Uninstalling nvidia-cusolver-cu12-11.7.3.90:\n      Successfully uninstalled nvidia-cusolver-cu12-11.7.3.90\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npylibcugraph-cu12 24.12.0 requires pylibraft-cu12==24.12.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires rmm-cu12==24.12.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed mpmath-1.3.0 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.21.5 nvidia-nvtx-cu12-12.1.105 sympy-1.13.1 torch-2.5.1+cu121 torchaudio-2.5.1+cu121 torchvision-0.20.1+cu121 triton-3.1.0 xformers-0.0.29.post1\nCollecting unsloth\n  Downloading unsloth-2025.4.1-py3-none-any.whl.metadata (46 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.4/46.4 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting vllm\n  Downloading vllm-0.8.4-cp38-abi3-manylinux1_x86_64.whl.metadata (27 kB)\nCollecting unsloth_zoo>=2025.4.1 (from unsloth)\n  Downloading unsloth_zoo-2025.4.1-py3-none-any.whl.metadata (8.0 kB)\nRequirement already satisfied: torch>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from unsloth) (2.5.1+cu121)\nRequirement already satisfied: xformers>=0.0.27.post2 in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.0.29.post1)\nCollecting bitsandbytes (from unsloth)\n  Downloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl.metadata (5.0 kB)\nRequirement already satisfied: triton>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from unsloth) (3.1.0)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from unsloth) (24.2)\nCollecting tyro (from unsloth)\n  Downloading tyro-0.9.19-py3-none-any.whl.metadata (9.9 kB)\nRequirement already satisfied: transformers!=4.47.0,>=4.46.1 in /usr/local/lib/python3.11/dist-packages (from unsloth) (4.51.1)\nRequirement already satisfied: datasets>=2.16.0 in /usr/local/lib/python3.11/dist-packages (from unsloth) (3.5.0)\nRequirement already satisfied: sentencepiece>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.2.0)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from unsloth) (4.67.1)\nRequirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from unsloth) (7.0.0)\nRequirement already satisfied: wheel>=0.42.0 in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.45.1)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from unsloth) (1.26.4)\nRequirement already satisfied: accelerate>=0.34.1 in /usr/local/lib/python3.11/dist-packages (from unsloth) (1.3.0)\nCollecting trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<=0.15.2,>=0.7.9 (from unsloth)\n  Downloading trl-0.15.2-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: peft!=0.11.0,>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.14.0)\nRequirement already satisfied: protobuf<4.0.0 in /usr/local/lib/python3.11/dist-packages (from unsloth) (3.20.3)\nRequirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.30.2)\nRequirement already satisfied: hf_transfer in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.1.9)\nRequirement already satisfied: diffusers in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.32.2)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.20.1+cu121)\nRequirement already satisfied: cachetools in /usr/local/lib/python3.11/dist-packages (from vllm) (5.5.2)\nRequirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from vllm) (2.32.3)\nCollecting blake3 (from vllm)\n  Downloading blake3-1.0.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\nRequirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from vllm) (9.0.0)\nRequirement already satisfied: tokenizers>=0.19.1 in /usr/local/lib/python3.11/dist-packages (from vllm) (0.21.0)\nCollecting fastapi>=0.115.0 (from fastapi[standard]>=0.115.0->vllm)\n  Downloading fastapi-0.115.12-py3-none-any.whl.metadata (27 kB)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from vllm) (3.11.16)\nRequirement already satisfied: openai>=1.52.0 in /usr/local/lib/python3.11/dist-packages (from vllm) (1.61.1)\nRequirement already satisfied: pydantic>=2.9 in /usr/local/lib/python3.11/dist-packages (from vllm) (2.11.3)\nRequirement already satisfied: prometheus_client>=0.18.0 in /usr/local/lib/python3.11/dist-packages (from vllm) (0.21.1)\nRequirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from vllm) (11.1.0)\nCollecting prometheus-fastapi-instrumentator>=7.0.0 (from vllm)\n  Downloading prometheus_fastapi_instrumentator-7.1.0-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: tiktoken>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from vllm) (0.9.0)\nCollecting lm-format-enforcer<0.11,>=0.10.11 (from vllm)\n  Downloading lm_format_enforcer-0.10.11-py3-none-any.whl.metadata (17 kB)\nCollecting llguidance<0.8.0,>=0.7.9 (from vllm)\n  Downloading llguidance-0.7.19-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\nCollecting outlines==0.1.11 (from vllm)\n  Downloading outlines-0.1.11-py3-none-any.whl.metadata (17 kB)\nCollecting lark==1.2.2 (from vllm)\n  Downloading lark-1.2.2-py3-none-any.whl.metadata (1.8 kB)\nCollecting xgrammar==0.1.18 (from vllm)\n  Downloading xgrammar-0.1.18-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\nRequirement already satisfied: typing_extensions>=4.10 in /usr/local/lib/python3.11/dist-packages (from vllm) (4.13.1)\nRequirement already satisfied: filelock>=3.16.1 in /usr/local/lib/python3.11/dist-packages (from vllm) (3.18.0)\nCollecting partial-json-parser (from vllm)\n  Downloading partial_json_parser-0.2.1.1.post5-py3-none-any.whl.metadata (6.1 kB)\nRequirement already satisfied: pyzmq in /usr/local/lib/python3.11/dist-packages (from vllm) (24.0.1)\nCollecting msgspec (from vllm)\n  Downloading msgspec-0.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)\nCollecting gguf>=0.13.0 (from vllm)\n  Downloading gguf-0.16.2-py3-none-any.whl.metadata (4.4 kB)\nRequirement already satisfied: importlib_metadata in /usr/local/lib/python3.11/dist-packages (from vllm) (8.6.1)\nCollecting mistral_common>=1.5.4 (from mistral_common[opencv]>=1.5.4->vllm)\n  Downloading mistral_common-1.5.4-py3-none-any.whl.metadata (4.5 kB)\nRequirement already satisfied: opencv-python-headless>=4.11.0 in /usr/local/lib/python3.11/dist-packages (from vllm) (4.11.0.86)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from vllm) (6.0.2)\nRequirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from vllm) (0.8.1)\nCollecting compressed-tensors==0.9.3 (from vllm)\n  Downloading compressed_tensors-0.9.3-py3-none-any.whl.metadata (7.0 kB)\nCollecting depyf==0.18.0 (from vllm)\n  Downloading depyf-0.18.0-py3-none-any.whl.metadata (7.1 kB)\nRequirement already satisfied: cloudpickle in /usr/local/lib/python3.11/dist-packages (from vllm) (3.1.1)\nCollecting watchfiles (from vllm)\n  Downloading watchfiles-1.0.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\nRequirement already satisfied: python-json-logger in /usr/local/lib/python3.11/dist-packages (from vllm) (3.3.0)\nRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from vllm) (1.15.2)\nRequirement already satisfied: ninja in /usr/local/lib/python3.11/dist-packages (from vllm) (1.11.1.4)\nCollecting opentelemetry-sdk<1.27.0,>=1.26.0 (from vllm)\n  Downloading opentelemetry_sdk-1.26.0-py3-none-any.whl.metadata (1.5 kB)\nCollecting opentelemetry-api<1.27.0,>=1.26.0 (from vllm)\n  Downloading opentelemetry_api-1.26.0-py3-none-any.whl.metadata (1.4 kB)\nCollecting opentelemetry-exporter-otlp<1.27.0,>=1.26.0 (from vllm)\n  Downloading opentelemetry_exporter_otlp-1.26.0-py3-none-any.whl.metadata (2.3 kB)\nCollecting opentelemetry-semantic-conventions-ai<0.5.0,>=0.4.1 (from vllm)\n  Downloading opentelemetry_semantic_conventions_ai-0.4.3-py3-none-any.whl.metadata (1.2 kB)\nCollecting numba==0.61.2 (from vllm)\n  Downloading numba-0.61.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.8 kB)\nCollecting ray!=2.44.*,>=2.43.0 (from ray[cgraph]!=2.44.*,>=2.43.0->vllm)\n  Downloading ray-2.43.0-cp311-cp311-manylinux2014_x86_64.whl.metadata (19 kB)\nCollecting torch>=2.4.0 (from unsloth)\n  Downloading torch-2.6.0-cp311-cp311-manylinux1_x86_64.whl.metadata (28 kB)\nCollecting torchaudio==2.6.0 (from vllm)\n  Downloading torchaudio-2.6.0-cp311-cp311-manylinux1_x86_64.whl.metadata (6.6 kB)\nCollecting torchvision (from unsloth)\n  Downloading torchvision-0.21.0-cp311-cp311-manylinux1_x86_64.whl.metadata (6.1 kB)\nCollecting xformers>=0.0.27.post2 (from unsloth)\n  Downloading xformers-0.0.29.post2-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (1.0 kB)\nCollecting astor (from depyf==0.18.0->vllm)\n  Downloading astor-0.8.1-py2.py3-none-any.whl.metadata (4.2 kB)\nRequirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from depyf==0.18.0->vllm) (0.3.8)\nCollecting llvmlite<0.45,>=0.44.0dev0 (from numba==0.61.2->vllm)\n  Downloading llvmlite-0.44.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.8 kB)\nCollecting interegular (from outlines==0.1.11->vllm)\n  Downloading interegular-0.3.3-py37-none-any.whl.metadata (3.0 kB)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from outlines==0.1.11->vllm) (3.1.6)\nRequirement already satisfied: nest_asyncio in /usr/local/lib/python3.11/dist-packages (from outlines==0.1.11->vllm) (1.6.0)\nCollecting diskcache (from outlines==0.1.11->vllm)\n  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\nRequirement already satisfied: referencing in /usr/local/lib/python3.11/dist-packages (from outlines==0.1.11->vllm) (0.36.2)\nRequirement already satisfied: jsonschema in /usr/local/lib/python3.11/dist-packages (from outlines==0.1.11->vllm) (4.23.0)\nCollecting pycountry (from outlines==0.1.11->vllm)\n  Downloading pycountry-24.6.1-py3-none-any.whl.metadata (12 kB)\nCollecting airportsdata (from outlines==0.1.11->vllm)\n  Downloading airportsdata-20250224-py3-none-any.whl.metadata (9.0 kB)\nCollecting outlines_core==0.1.26 (from outlines==0.1.11->vllm)\n  Downloading outlines_core-0.1.26-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (3.4.2)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (2025.3.2)\nCollecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (9.1.0.70)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparselt-cu12==0.6.2 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (2.21.5)\nCollecting nvidia-nvtx-cu12==12.4.127 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.7 kB)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting triton>=3.0.0 (from unsloth)\n  Downloading triton-3.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.4.0->unsloth) (1.3.0)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.34.1->unsloth) (0.5.2)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0->unsloth) (19.0.1)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0->unsloth) (2.2.3)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0->unsloth) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0->unsloth) (0.70.16)\nCollecting fsspec (from torch>=2.4.0->unsloth)\n  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\nCollecting starlette<0.47.0,>=0.40.0 (from fastapi>=0.115.0->fastapi[standard]>=0.115.0->vllm)\n  Downloading starlette-0.46.2-py3-none-any.whl.metadata (6.2 kB)\nCollecting fastapi-cli>=0.0.5 (from fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm)\n  Downloading fastapi_cli-0.0.7-py3-none-any.whl.metadata (6.2 kB)\nRequirement already satisfied: httpx>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from fastapi[standard]>=0.115.0->vllm) (0.28.1)\nCollecting python-multipart>=0.0.18 (from fastapi[standard]>=0.115.0->vllm)\n  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\nRequirement already satisfied: email-validator>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from fastapi[standard]>=0.115.0->vllm) (2.2.0)\nCollecting uvicorn>=0.12.0 (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm)\n  Downloading uvicorn-0.34.2-py3-none-any.whl.metadata (6.5 kB)\nCollecting hf-xet>=0.1.4 (from huggingface-hub[hf_xet]>=0.30.0->vllm)\n  Downloading hf_xet-1.0.5-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (494 bytes)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->unsloth) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->unsloth) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->unsloth) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->unsloth) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->unsloth) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->unsloth) (2.4.1)\nRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.52.0->vllm) (3.7.1)\nRequirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.52.0->vllm) (1.9.0)\nRequirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.52.0->vllm) (0.8.2)\nRequirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai>=1.52.0->vllm) (1.3.1)\nRequirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api<1.27.0,>=1.26.0->vllm) (1.2.18)\nCollecting importlib_metadata (from vllm)\n  Downloading importlib_metadata-8.0.0-py3-none-any.whl.metadata (4.6 kB)\nRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.11/dist-packages (from importlib_metadata->vllm) (3.21.0)\nCollecting opentelemetry-exporter-otlp-proto-grpc==1.26.0 (from opentelemetry-exporter-otlp<1.27.0,>=1.26.0->vllm)\n  Downloading opentelemetry_exporter_otlp_proto_grpc-1.26.0-py3-none-any.whl.metadata (2.3 kB)\nCollecting opentelemetry-exporter-otlp-proto-http==1.26.0 (from opentelemetry-exporter-otlp<1.27.0,>=1.26.0->vllm)\n  Downloading opentelemetry_exporter_otlp_proto_http-1.26.0-py3-none-any.whl.metadata (2.3 kB)\nRequirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc==1.26.0->opentelemetry-exporter-otlp<1.27.0,>=1.26.0->vllm) (1.67.0)\nRequirement already satisfied: grpcio<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc==1.26.0->opentelemetry-exporter-otlp<1.27.0,>=1.26.0->vllm) (1.70.0)\nCollecting opentelemetry-exporter-otlp-proto-common==1.26.0 (from opentelemetry-exporter-otlp-proto-grpc==1.26.0->opentelemetry-exporter-otlp<1.27.0,>=1.26.0->vllm)\n  Downloading opentelemetry_exporter_otlp_proto_common-1.26.0-py3-none-any.whl.metadata (1.8 kB)\nCollecting opentelemetry-proto==1.26.0 (from opentelemetry-exporter-otlp-proto-grpc==1.26.0->opentelemetry-exporter-otlp<1.27.0,>=1.26.0->vllm)\n  Downloading opentelemetry_proto-1.26.0-py3-none-any.whl.metadata (2.3 kB)\nCollecting opentelemetry-semantic-conventions==0.47b0 (from opentelemetry-sdk<1.27.0,>=1.26.0->vllm)\n  Downloading opentelemetry_semantic_conventions-0.47b0-py3-none-any.whl.metadata (2.4 kB)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9->vllm) (0.7.0)\nRequirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9->vllm) (2.33.1)\nRequirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9->vllm) (0.4.0)\nRequirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from ray!=2.44.*,>=2.43.0->ray[cgraph]!=2.44.*,>=2.43.0->vllm) (8.1.8)\nRequirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from ray!=2.44.*,>=2.43.0->ray[cgraph]!=2.44.*,>=2.43.0->vllm) (1.1.0)\nRequirement already satisfied: aiosignal in /usr/local/lib/python3.11/dist-packages (from ray!=2.44.*,>=2.43.0->ray[cgraph]!=2.44.*,>=2.43.0->vllm) (1.3.2)\nRequirement already satisfied: frozenlist in /usr/local/lib/python3.11/dist-packages (from ray!=2.44.*,>=2.43.0->ray[cgraph]!=2.44.*,>=2.43.0->vllm) (1.5.0)\nRequirement already satisfied: cupy-cuda12x in /usr/local/lib/python3.11/dist-packages (from ray[cgraph]!=2.44.*,>=2.43.0->vllm) (13.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->vllm) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->vllm) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->vllm) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->vllm) (2025.1.31)\nRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken>=0.6.0->vllm) (2024.11.6)\nRequirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<=0.15.2,>=0.7.9->unsloth) (14.0.0)\nCollecting cut_cross_entropy (from unsloth_zoo>=2025.4.1->unsloth)\n  Downloading cut_cross_entropy-25.1.1-py3-none-any.whl.metadata (9.3 kB)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->vllm) (2.6.1)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->vllm) (25.3.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->vllm) (6.2.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->vllm) (0.3.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->vllm) (1.19.0)\nRequirement already satisfied: docstring-parser>=0.15 in /usr/local/lib/python3.11/dist-packages (from tyro->unsloth) (0.16)\nCollecting shtab>=1.5.6 (from tyro->unsloth)\n  Downloading shtab-1.7.2-py3-none-any.whl.metadata (7.4 kB)\nRequirement already satisfied: typeguard>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from tyro->unsloth) (4.4.1)\nRequirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.11/dist-packages (from deprecated>=1.2.6->opentelemetry-api<1.27.0,>=1.26.0->vllm) (1.17.2)\nRequirement already satisfied: dnspython>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from email-validator>=2.0.0->fastapi[standard]>=0.115.0->vllm) (2.7.0)\nRequirement already satisfied: typer>=0.12.3 in /usr/local/lib/python3.11/dist-packages (from fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.15.1)\nCollecting rich-toolkit>=0.11.1 (from fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm)\n  Downloading rich_toolkit-0.14.3-py3-none-any.whl.metadata (999 bytes)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.23.0->fastapi[standard]>=0.115.0->vllm) (1.0.7)\nRequirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.23.0->fastapi[standard]>=0.115.0->vllm) (0.14.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->outlines==0.1.11->vllm) (3.0.2)\nRequirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema->outlines==0.1.11->vllm) (2024.10.1)\nRequirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema->outlines==0.1.11->vllm) (0.22.3)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<=0.15.2,>=0.7.9->unsloth) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<=0.15.2,>=0.7.9->unsloth) (2.19.1)\nCollecting httptools>=0.6.3 (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm)\n  Downloading httptools-0.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\nCollecting python-dotenv>=0.13 (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm)\n  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\nCollecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm)\n  Downloading uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\nRequirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (14.2)\nRequirement already satisfied: fastrlock>=0.5 in /usr/local/lib/python3.11/dist-packages (from cupy-cuda12x->ray[cgraph]!=2.44.*,>=2.43.0->vllm) (0.8.3)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->unsloth) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->unsloth) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->unsloth) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->unsloth) (2024.2.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=2.16.0->unsloth) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=2.16.0->unsloth) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=2.16.0->unsloth) (2025.2)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->unsloth) (2024.2.0)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<=0.15.2,>=0.7.9->unsloth) (0.1.2)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.16.0->unsloth) (1.17.0)\nRequirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.12.3->fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (1.5.4)\nDownloading unsloth-2025.4.1-py3-none-any.whl (193 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.2/193.2 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading vllm-0.8.4-cp38-abi3-manylinux1_x86_64.whl (294.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.1/294.1 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading compressed_tensors-0.9.3-py3-none-any.whl (98 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.4/98.4 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading depyf-0.18.0-py3-none-any.whl (38 kB)\nDownloading lark-1.2.2-py3-none-any.whl (111 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.0/111.0 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading numba-0.61.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m81.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading outlines-0.1.11-py3-none-any.whl (87 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.6/87.6 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading torch-2.6.0-cp311-cp311-manylinux1_x86_64.whl (766.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m766.7/766.7 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading torchaudio-2.6.0-cp311-cp311-manylinux1_x86_64.whl (3.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading torchvision-0.21.0-cp311-cp311-manylinux1_x86_64.whl (7.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading triton-3.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (253.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.2/253.2 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading xformers-0.0.29.post2-cp311-cp311-manylinux_2_28_x86_64.whl (44.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 MB\u001b[0m \u001b[31m41.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading xgrammar-0.1.18-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m91.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m105.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m77.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m42.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl (150.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.1/150.1 MB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m85.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (99 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading outlines_core-0.1.26-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (343 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m343.3/343.3 kB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading fastapi-0.115.12-py3-none-any.whl (95 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading gguf-0.16.2-py3-none-any.whl (92 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.2/92.2 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading llguidance-0.7.19-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.0/14.0 MB\u001b[0m \u001b[31m93.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading lm_format_enforcer-0.10.11-py3-none-any.whl (44 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.2/44.2 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading mistral_common-1.5.4-py3-none-any.whl (6.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m99.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading opentelemetry_api-1.26.0-py3-none-any.whl (61 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.5/61.5 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading importlib_metadata-8.0.0-py3-none-any.whl (24 kB)\nDownloading opentelemetry_exporter_otlp-1.26.0-py3-none-any.whl (7.0 kB)\nDownloading opentelemetry_exporter_otlp_proto_grpc-1.26.0-py3-none-any.whl (18 kB)\nDownloading opentelemetry_exporter_otlp_proto_http-1.26.0-py3-none-any.whl (16 kB)\nDownloading opentelemetry_exporter_otlp_proto_common-1.26.0-py3-none-any.whl (17 kB)\nDownloading opentelemetry_proto-1.26.0-py3-none-any.whl (52 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.5/52.5 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading opentelemetry_sdk-1.26.0-py3-none-any.whl (109 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.5/109.5 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading opentelemetry_semantic_conventions-0.47b0-py3-none-any.whl (138 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.0/138.0 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading opentelemetry_semantic_conventions_ai-0.4.3-py3-none-any.whl (5.4 kB)\nDownloading prometheus_fastapi_instrumentator-7.1.0-py3-none-any.whl (19 kB)\nDownloading ray-2.43.0-cp311-cp311-manylinux2014_x86_64.whl (67.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.7/67.7 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading trl-0.15.2-py3-none-any.whl (318 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.9/318.9 kB\u001b[0m \u001b[31m420.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading unsloth_zoo-2025.4.1-py3-none-any.whl (128 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.9/128.9 kB\u001b[0m \u001b[31m199.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl (76.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading blake3-1.0.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (376 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m376.2/376.2 kB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading msgspec-0.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (210 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.7/210.7 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading partial_json_parser-0.2.1.1.post5-py3-none-any.whl (10 kB)\nDownloading tyro-0.9.19-py3-none-any.whl (124 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.3/124.3 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading watchfiles-1.0.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (454 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m454.8/454.8 kB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading fastapi_cli-0.0.7-py3-none-any.whl (10 kB)\nDownloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading hf_xet-1.0.5-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (54.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.0/54.0 MB\u001b[0m \u001b[31m32.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading interegular-0.3.3-py37-none-any.whl (23 kB)\nDownloading llvmlite-0.44.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (42.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.4/42.4 MB\u001b[0m \u001b[31m37.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\nDownloading shtab-1.7.2-py3-none-any.whl (14 kB)\nDownloading starlette-0.46.2-py3-none-any.whl (72 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading uvicorn-0.34.2-py3-none-any.whl (62 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading airportsdata-20250224-py3-none-any.whl (913 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m913.7/913.7 kB\u001b[0m \u001b[31m41.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading astor-0.8.1-py2.py3-none-any.whl (27 kB)\nDownloading cut_cross_entropy-25.1.1-py3-none-any.whl (22 kB)\nDownloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pycountry-24.6.1-py3-none-any.whl (6.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m96.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading httptools-0.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (459 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\nDownloading rich_toolkit-0.14.3-py3-none-any.whl (24 kB)\nDownloading uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m93.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: triton, nvidia-cusparselt-cu12, blake3, uvloop, uvicorn, shtab, python-multipart, python-dotenv, pycountry, partial-json-parser, opentelemetry-semantic-conventions-ai, opentelemetry-proto, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, msgspec, llvmlite, llguidance, lark, interegular, importlib_metadata, httptools, hf-xet, fsspec, diskcache, astor, airportsdata, watchfiles, starlette, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, nvidia-cusparse-cu12, depyf, tyro, rich-toolkit, prometheus-fastapi-instrumentator, opentelemetry-semantic-conventions, nvidia-cusolver-cu12, lm-format-enforcer, fastapi, torch, ray, outlines_core, opentelemetry-sdk, fastapi-cli, torchaudio, opentelemetry-exporter-otlp-proto-http, opentelemetry-exporter-otlp-proto-grpc, cut_cross_entropy, opentelemetry-exporter-otlp, trl, mistral_common, xgrammar, xformers, unsloth_zoo, torchvision, outlines, numba, gguf, compressed-tensors, bitsandbytes, vllm, unsloth\n  Attempting uninstall: triton\n    Found existing installation: triton 3.1.0\n    Uninstalling triton-3.1.0:\n      Successfully uninstalled triton-3.1.0\n  Attempting uninstall: nvidia-nvtx-cu12\n    Found existing installation: nvidia-nvtx-cu12 12.1.105\n    Uninstalling nvidia-nvtx-cu12-12.1.105:\n      Successfully uninstalled nvidia-nvtx-cu12-12.1.105\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.8.93\n    Uninstalling nvidia-nvjitlink-cu12-12.8.93:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.8.93\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.2.106\n    Uninstalling nvidia-curand-cu12-10.3.2.106:\n      Successfully uninstalled nvidia-curand-cu12-10.3.2.106\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.0.2.54\n    Uninstalling nvidia-cufft-cu12-11.0.2.54:\n      Successfully uninstalled nvidia-cufft-cu12-11.0.2.54\n  Attempting uninstall: nvidia-cuda-runtime-cu12\n    Found existing installation: nvidia-cuda-runtime-cu12 12.1.105\n    Uninstalling nvidia-cuda-runtime-cu12-12.1.105:\n      Successfully uninstalled nvidia-cuda-runtime-cu12-12.1.105\n  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n    Found existing installation: nvidia-cuda-nvrtc-cu12 12.1.105\n    Uninstalling nvidia-cuda-nvrtc-cu12-12.1.105:\n      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.1.105\n  Attempting uninstall: nvidia-cuda-cupti-cu12\n    Found existing installation: nvidia-cuda-cupti-cu12 12.1.105\n    Uninstalling nvidia-cuda-cupti-cu12-12.1.105:\n      Successfully uninstalled nvidia-cuda-cupti-cu12-12.1.105\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.1.3.1\n    Uninstalling nvidia-cublas-cu12-12.1.3.1:\n      Successfully uninstalled nvidia-cublas-cu12-12.1.3.1\n  Attempting uninstall: llvmlite\n    Found existing installation: llvmlite 0.43.0\n    Uninstalling llvmlite-0.43.0:\n      Successfully uninstalled llvmlite-0.43.0\n  Attempting uninstall: importlib_metadata\n    Found existing installation: importlib_metadata 8.6.1\n    Uninstalling importlib_metadata-8.6.1:\n      Successfully uninstalled importlib_metadata-8.6.1\n  Attempting uninstall: fsspec\n    Found existing installation: fsspec 2025.3.2\n    Uninstalling fsspec-2025.3.2:\n      Successfully uninstalled fsspec-2025.3.2\n  Attempting uninstall: opentelemetry-api\n    Found existing installation: opentelemetry-api 1.16.0\n    Uninstalling opentelemetry-api-1.16.0:\n      Successfully uninstalled opentelemetry-api-1.16.0\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.1.0.106\n    Uninstalling nvidia-cusparse-cu12-12.1.0.106:\n      Successfully uninstalled nvidia-cusparse-cu12-12.1.0.106\n  Attempting uninstall: opentelemetry-semantic-conventions\n    Found existing installation: opentelemetry-semantic-conventions 0.37b0\n    Uninstalling opentelemetry-semantic-conventions-0.37b0:\n      Successfully uninstalled opentelemetry-semantic-conventions-0.37b0\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.4.5.107\n    Uninstalling nvidia-cusolver-cu12-11.4.5.107:\n      Successfully uninstalled nvidia-cusolver-cu12-11.4.5.107\n  Attempting uninstall: torch\n    Found existing installation: torch 2.5.1+cu121\n    Uninstalling torch-2.5.1+cu121:\n      Successfully uninstalled torch-2.5.1+cu121\n  Attempting uninstall: ray\n    Found existing installation: ray 2.44.1\n    Uninstalling ray-2.44.1:\n      Successfully uninstalled ray-2.44.1\n  Attempting uninstall: opentelemetry-sdk\n    Found existing installation: opentelemetry-sdk 1.16.0\n    Uninstalling opentelemetry-sdk-1.16.0:\n      Successfully uninstalled opentelemetry-sdk-1.16.0\n  Attempting uninstall: torchaudio\n    Found existing installation: torchaudio 2.5.1+cu121\n    Uninstalling torchaudio-2.5.1+cu121:\n      Successfully uninstalled torchaudio-2.5.1+cu121\n  Attempting uninstall: xformers\n    Found existing installation: xformers 0.0.29.post1\n    Uninstalling xformers-0.0.29.post1:\n      Successfully uninstalled xformers-0.0.29.post1\n  Attempting uninstall: torchvision\n    Found existing installation: torchvision 0.20.1+cu121\n    Uninstalling torchvision-0.20.1+cu121:\n      Successfully uninstalled torchvision-0.20.1+cu121\n  Attempting uninstall: numba\n    Found existing installation: numba 0.60.0\n    Uninstalling numba-0.60.0:\n      Successfully uninstalled numba-0.60.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ndask-cuda 25.2.0 requires numba<0.61.0a0,>=0.59.1, but you have numba 0.61.2 which is incompatible.\ncuml-cu12 25.2.1 requires numba<0.61.0a0,>=0.59.1, but you have numba 0.61.2 which is incompatible.\ncudf-cu12 25.2.2 requires numba<0.61.0a0,>=0.59.1, but you have numba 0.61.2 which is incompatible.\ndistributed-ucxx-cu12 0.42.0 requires numba<0.61.0a0,>=0.59.1, but you have numba 0.61.2 which is incompatible.\nydata-profiling 4.16.1 requires numba<=0.61,>=0.56.0, but you have numba 0.61.2 which is incompatible.\ngcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.12.0 which is incompatible.\nbigframes 1.36.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\nfastai 2.7.18 requires torch<2.6,>=1.10, but you have torch 2.6.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires pylibraft-cu12==24.12.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires rmm-cu12==24.12.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed airportsdata-20250224 astor-0.8.1 bitsandbytes-0.45.5 blake3-1.0.4 compressed-tensors-0.9.3 cut_cross_entropy-25.1.1 depyf-0.18.0 diskcache-5.6.3 fastapi-0.115.12 fastapi-cli-0.0.7 fsspec-2024.12.0 gguf-0.16.2 hf-xet-1.0.5 httptools-0.6.4 importlib_metadata-8.0.0 interegular-0.3.3 lark-1.2.2 llguidance-0.7.19 llvmlite-0.44.0 lm-format-enforcer-0.10.11 mistral_common-1.5.4 msgspec-0.19.0 numba-0.61.2 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-cusparselt-cu12-0.6.2 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.4.127 opentelemetry-api-1.26.0 opentelemetry-exporter-otlp-1.26.0 opentelemetry-exporter-otlp-proto-common-1.26.0 opentelemetry-exporter-otlp-proto-grpc-1.26.0 opentelemetry-exporter-otlp-proto-http-1.26.0 opentelemetry-proto-1.26.0 opentelemetry-sdk-1.26.0 opentelemetry-semantic-conventions-0.47b0 opentelemetry-semantic-conventions-ai-0.4.3 outlines-0.1.11 outlines_core-0.1.26 partial-json-parser-0.2.1.1.post5 prometheus-fastapi-instrumentator-7.1.0 pycountry-24.6.1 python-dotenv-1.1.0 python-multipart-0.0.20 ray-2.43.0 rich-toolkit-0.14.3 shtab-1.7.2 starlette-0.46.2 torch-2.6.0 torchaudio-2.6.0 torchvision-0.21.0 triton-3.2.0 trl-0.15.2 tyro-0.9.19 unsloth-2025.4.1 unsloth_zoo-2025.4.1 uvicorn-0.34.2 uvloop-0.21.0 vllm-0.8.4 watchfiles-1.0.5 xformers-0.0.29.post2 xgrammar-0.1.18\nCollecting triton==3.1.0\n  Downloading triton-3.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.3 kB)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from triton==3.1.0) (3.18.0)\nDownloading triton-3.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.5/209.5 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: triton\n  Attempting uninstall: triton\n    Found existing installation: triton 3.2.0\n    Uninstalling triton-3.2.0:\n      Successfully uninstalled triton-3.2.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntorch 2.6.0 requires triton==3.2.0; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have triton 3.1.0 which is incompatible.\nfastai 2.7.18 requires torch<2.6,>=1.10, but you have torch 2.6.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed triton-3.1.0\nRequirement already satisfied: pynvml in /usr/local/lib/python3.11/dist-packages (12.0.0)\nRequirement already satisfied: nvidia-ml-py<13.0.0a0,>=12.0.0 in /usr/local/lib/python3.11/dist-packages (from pynvml) (12.570.86)\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_value_0 = user_secrets.get_secret(\"HF_Token\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T19:17:09.238268Z","iopub.execute_input":"2025-04-26T19:17:09.238548Z","iopub.status.idle":"2025-04-26T19:17:09.414198Z","shell.execute_reply.started":"2025-04-26T19:17:09.238526Z","shell.execute_reply":"2025-04-26T19:17:09.413303Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"!rm state.db","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#from unsloth import FastLanguageModel, is_bfloat16_supported\n#import torch\n#max_seq_length = 512 # Can increase for longer reasoning traces\n#lora_rank = 16 # Larger rank = smarter, but slower\n\n#model, tokenizer = FastLanguageModel.from_pretrained(\n    #model_name = \"google/txgemma-9b-predict\",\n    #token = secret_value_0,\n    #max_seq_length = max_seq_length,\n    #load_in_4bit = True, # False for LoRA 16bit\n    #fast_inference = True, # Enable vLLM fast inference\n    #trust_remote_code = False,    # ← allow the repo’s own tokenizer code\n    #fix_tokenizer = True,    # ← apply Unsloth’s Gemma tokenizer patches\n    #max_lora_rank = lora_rank,\n    #gpu_memory_utilization = 0.8, # Reduce if out of memory\n#)\n\n#model = FastLanguageModel.get_peft_model(\n    #model,\n    #r = lora_rank, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n    #target_modules = [\"gate_proj\", \"up_proj\", \"down_proj\",],\n    #lora_alpha = lora_rank,\n    #use_gradient_checkpointing = \"unsloth\", # Enable long context finetuning\n    #random_state = 3407,\n#)\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n\nmodel_id = \"google/txgemma-9b-predict\"\n\nquant_config = BitsAndBytesConfig(\n    load_in_4bit            = True,\n    bnb_4bit_quant_type     = \"nf4\",\n    bnb_4bit_compute_dtype  = torch.float16,\n    llm_int8_enable_fp32_cpu_offload = True,\n)\n\ntokenizer = AutoTokenizer.from_pretrained(model_id,token=secret_value_0)  # loads tokenizer.json, tokenizer.model, etc. :contentReference[oaicite:0]{index=0}\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_id,\n    token = secret_value_0,\n    quantization_config = quant_config,\n    device_map          = \"balanced\",      # shards weights over both GPUs if available\n    torch_dtype         = torch.float16,\n    attn_implementation = \"eager\",     # match Google’s example\n)\n\n#import torch\n#from transformers import (\n    #AutoTokenizer,\n    #AutoModelForCausalLM,\n    #BitsAndBytesConfig,\n#)\n\n#model_id = \"google/txgemma-9b-predict\"\n\n# 1) Set up 4-bit NF4 + FP16 compute + CPU offload\n#quant_config = BitsAndBytesConfig(\n    #load_in_4bit                     = True,\n    #bnb_4bit_quant_type              = \"nf4\",\n    #bnb_4bit_compute_dtype           = torch.float16,\n    #llm_int8_enable_fp32_cpu_offload = True,\n#)\n\n# 2) Load tokenizer & model, forcing \"local only\" so we never hit the network\n#tokenizer = AutoTokenizer.from_pretrained(\n    #model_id,\n    #use_fast=True,\n    #local_files_only=True,            # ↪ only read from ~/.cache/huggingface\n#)\n\n#model = AutoModelForCausalLM.from_pretrained(\n    #model_id,\n    #quantization_config = quant_config,\n    #device_map          = \"balanced\",   # ↪ spread layers across GPU0 & GPU1\n    #torch_dtype         = torch.float16,\n    #local_files_only    = True,         # ↪ only read from cache\n    #attn_implementation   = \"eager\",\n#)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T19:19:18.254699Z","iopub.execute_input":"2025-04-26T19:19:18.255280Z","iopub.status.idle":"2025-04-26T19:24:29.045988Z","shell.execute_reply.started":"2025-04-26T19:19:18.255255Z","shell.execute_reply":"2025-04-26T19:24:29.045170Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/46.4k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2aa69543ffa44f2caabb0d44ea10447f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c3a5ed6c8de34f099ac3180e52274fe0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c9b6b57dd07d4b5eb3f282002ba5d3ea"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e0a9b92e75dc4d1a95293118d5ff3635"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/851 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1871ccdf275046a6a441f3e4a7ea49a0"}},"metadata":{}},{"name":"stderr","text":"2025-04-26 19:19:25.473041: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1745695165.770382      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1745695165.853343      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/39.1k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"98cb2a3ec2d8422ca442904f49fedd19"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 8 files:   0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"098a8850c79b4f11b7621668cff37bc5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00007-of-00008.safetensors:   0%|          | 0.00/4.96G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4b4122324d164662a4fb1dd4a56ab9c2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00005-of-00008.safetensors:   0%|          | 0.00/4.96G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"79b5855407074b2a9fb62dc12bf1401c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00008.safetensors:   0%|          | 0.00/4.96G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a75a4206514c481f8d0ba1d3a801dc02"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00006-of-00008.safetensors:   0%|          | 0.00/4.96G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9d7df1c162d540148945dfa230c84a05"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00008-of-00008.safetensors:   0%|          | 0.00/2.38G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f52b038292c94502b52c224f7e98a00b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00008.safetensors:   0%|          | 0.00/4.96G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1bce7bdeaf80452cb833ae8abb5e2137"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00008.safetensors:   0%|          | 0.00/4.84G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2ad0e5e79875455db0d2f7d68197d29b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00004-of-00008.safetensors:   0%|          | 0.00/4.93G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1b9a33bc5749464bb95cce0ea226704a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f7fd2268d70341d7af989b4ff04d5b72"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/168 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5417699232fb4ccfb4a2dac5fe97f1a5"}},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"!df -h\n!du -sh ~/.cache/huggingface/hub","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import re\nfrom datasets import load_dataset, Dataset\n\n# Load and prep dataset\nSYSTEM_PROMPT = \"\"\"\nRespond in the following format:\n<reasoning>\n...\n</reasoning>\n<answer>\n...\n</answer>\n\"\"\"\n\ndef get_trialbench(split=\"train\"):\n    raw = load_dataset(\n        \"json\",\n        data_files=\"/kaggle/input/txgemma-datasets-trialbench-adverse-event/txgemma_datasets_trialbench_adverse-event-rate-prediction_train.jsonl\",\n        split=split,\n    )\n    # Concatenate the system prompt and the user text into one string\n    def fmt(x):\n        return {\n            \"prompt\": SYSTEM_PROMPT.strip() + \"\\n\\n\" + x[\"input_text\"].strip(),\n            \"answer\": x[\"output_text\"].strip(),\n        }\n    return raw.map(fmt)\n\ndataset = get_trialbench()\n\ndef extract_xml_answer(text: str) -> str:\n    # exactly as before\n    answer = text.split(\"<answer>\")[-1].split(\"</answer>\")[0]\n    return answer.strip()\n\ndef count_xml(text: str) -> float:\n    count = 0.0\n    if text.count(\"<reasoning>\\n\") == 1:        count += 0.125\n    if text.count(\"\\n</reasoning>\\n\") == 1:     count += 0.125\n    if text.count(\"\\n<answer>\\n\") == 1:\n        count += 0.125\n        count -= len(text.split(\"\\n</answer>\\n\")[-1]) * 0.001\n    if text.count(\"\\n</answer>\") == 1:\n        count += 0.125\n        count -= (len(text.split(\"\\n</answer>\")[-1]) - 1) * 0.001\n    return count\n\ndef xmlcount_reward_func(completions, **kwargs) -> list[float]:\n    # completions is List[str]\n    return [count_xml(c) for c in completions]\n\ndef strict_format_reward_func(completions, **kwargs) -> list[float]:\n    pattern = r\"^<reasoning>\\n.*?\\n</reasoning>\\n<answer>\\n.*?\\n</answer>\\n$\"\n    return [0.5 if re.match(pattern, c, re.DOTALL) else 0.0 for c in completions]\n\ndef soft_format_reward_func(completions, **kwargs) -> list[float]:\n    pattern = r\"<reasoning>.*?</reasoning>\\s*<answer>.*?</answer>\"\n    return [0.5 if re.search(pattern, c, re.DOTALL) else 0.0 for c in completions]\n\ndef int_reward_func(completions, **kwargs) -> list[float]:\n    extracted = [extract_xml_answer(c) for c in completions]\n    return [0.5 if r.isdigit() else 0.0 for r in extracted]\n\ndef correctness_reward_func(prompts, completions, answer, **kwargs) -> list[float]:\n    # `answer` is List[str] of ground-truth\n    extracted = [extract_xml_answer(c) for c in completions]\n    return [2.0 if r == a else 0.0 for r, a in zip(extracted, answer)]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T19:24:37.597909Z","iopub.execute_input":"2025-04-26T19:24:37.599018Z","iopub.status.idle":"2025-04-26T19:24:40.162342Z","shell.execute_reply.started":"2025-04-26T19:24:37.598992Z","shell.execute_reply":"2025-04-26T19:24:40.161695Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0c1ae17f9a2f40ebb612aaaab9e89226"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/14368 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"350a029a473e4d2a99de1bbe26f59865"}},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"from peft import LoraConfig, get_peft_model\n# 2) *Attach* LoRA adapters with PEFT\nlora_cfg = LoraConfig(\n    r             = 16,\n    lora_alpha    = 16,\n    target_modules= [\"gate_proj\", \"up_proj\", \"down_proj\"],\n    bias          = \"none\",\n    task_type     = \"CAUSAL_LM\",\n)\npeft_model = get_peft_model(model, lora_cfg)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T19:24:53.559048Z","iopub.execute_input":"2025-04-26T19:24:53.559758Z","iopub.status.idle":"2025-04-26T19:24:54.411757Z","shell.execute_reply.started":"2025-04-26T19:24:53.559716Z","shell.execute_reply":"2025-04-26T19:24:54.411182Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"from trl import GRPOConfig\n\ntraining_args = GRPOConfig(\n    use_vllm                    = False,      # HF path, not Unsloth/vLLM\n    learning_rate               = 5e-6,\n    per_device_train_batch_size = 2,          # MUST match num_generations\n    gradient_accumulation_steps = 3,\n    num_generations             = 2,\n    max_prompt_length           = 256,\n    max_completion_length       = 200,\n    max_steps                   = 100,\n    save_steps                  = 250,\n    output_dir                  = \"outputs\",\n\n    # precision flags for a T4\n    bf16                        = False,\n    fp16                        = True,\n\n    # (other args you already had:)\n    weight_decay                = 0.1,\n    warmup_ratio                = 0.1,\n    lr_scheduler_type           = \"cosine\",\n    optim                       = \"paged_adamw_8bit\",\n    logging_steps               = 1,\n    max_grad_norm               = 0.1,\n    report_to                   = \"none\",\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T19:25:08.113380Z","iopub.execute_input":"2025-04-26T19:25:08.113665Z","iopub.status.idle":"2025-04-26T19:25:08.210847Z","shell.execute_reply.started":"2025-04-26T19:25:08.113639Z","shell.execute_reply":"2025-04-26T19:25:08.210080Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"from trl import GRPOTrainer\ntrainer = GRPOTrainer(\n    model = peft_model,\n    processing_class = tokenizer,\n    reward_funcs = [\n        xmlcount_reward_func,\n        soft_format_reward_func,\n        strict_format_reward_func,\n        int_reward_func,\n        correctness_reward_func,\n    ],\n    args = training_args,\n    train_dataset = dataset,\n)\ntrainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T19:25:19.817549Z","iopub.execute_input":"2025-04-26T19:25:19.817891Z","iopub.status.idle":"2025-04-26T20:06:57.422840Z","shell.execute_reply.started":"2025-04-26T19:25:19.817866Z","shell.execute_reply":"2025-04-26T20:06:57.422187Z"}},"outputs":[{"name":"stdout","text":"INFO 04-26 19:25:25 [__init__.py:239] Automatically detected platform cuda.\n","output_type":"stream"},{"name":"stderr","text":"No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [100/100 41:19, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>11</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>12</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>13</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>14</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>15</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>16</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>17</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>18</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>19</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>21</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>22</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>23</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>24</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>25</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>26</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>27</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>28</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>29</td>\n      <td>0.000300</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>0.000200</td>\n    </tr>\n    <tr>\n      <td>31</td>\n      <td>0.000200</td>\n    </tr>\n    <tr>\n      <td>32</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>33</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>34</td>\n      <td>0.000300</td>\n    </tr>\n    <tr>\n      <td>35</td>\n      <td>0.000400</td>\n    </tr>\n    <tr>\n      <td>36</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>37</td>\n      <td>0.000600</td>\n    </tr>\n    <tr>\n      <td>38</td>\n      <td>0.000400</td>\n    </tr>\n    <tr>\n      <td>39</td>\n      <td>0.000800</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>0.002000</td>\n    </tr>\n    <tr>\n      <td>41</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>42</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>43</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>44</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>45</td>\n      <td>0.000400</td>\n    </tr>\n    <tr>\n      <td>46</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>47</td>\n      <td>0.001400</td>\n    </tr>\n    <tr>\n      <td>48</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>49</td>\n      <td>0.000900</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>51</td>\n      <td>0.001200</td>\n    </tr>\n    <tr>\n      <td>52</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>53</td>\n      <td>0.001900</td>\n    </tr>\n    <tr>\n      <td>54</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>55</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>56</td>\n      <td>0.000600</td>\n    </tr>\n    <tr>\n      <td>57</td>\n      <td>0.002500</td>\n    </tr>\n    <tr>\n      <td>58</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>59</td>\n      <td>0.002600</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>61</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>62</td>\n      <td>0.001900</td>\n    </tr>\n    <tr>\n      <td>63</td>\n      <td>0.005000</td>\n    </tr>\n    <tr>\n      <td>64</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>65</td>\n      <td>0.000700</td>\n    </tr>\n    <tr>\n      <td>66</td>\n      <td>0.001600</td>\n    </tr>\n    <tr>\n      <td>67</td>\n      <td>0.000500</td>\n    </tr>\n    <tr>\n      <td>68</td>\n      <td>0.002200</td>\n    </tr>\n    <tr>\n      <td>69</td>\n      <td>0.004700</td>\n    </tr>\n    <tr>\n      <td>70</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>71</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>72</td>\n      <td>0.000900</td>\n    </tr>\n    <tr>\n      <td>73</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>74</td>\n      <td>0.001100</td>\n    </tr>\n    <tr>\n      <td>75</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>76</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>77</td>\n      <td>0.003100</td>\n    </tr>\n    <tr>\n      <td>78</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>79</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>81</td>\n      <td>0.002000</td>\n    </tr>\n    <tr>\n      <td>82</td>\n      <td>0.001900</td>\n    </tr>\n    <tr>\n      <td>83</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>84</td>\n      <td>0.000200</td>\n    </tr>\n    <tr>\n      <td>85</td>\n      <td>0.000200</td>\n    </tr>\n    <tr>\n      <td>86</td>\n      <td>0.007600</td>\n    </tr>\n    <tr>\n      <td>87</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>88</td>\n      <td>0.000600</td>\n    </tr>\n    <tr>\n      <td>89</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>90</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>91</td>\n      <td>0.005200</td>\n    </tr>\n    <tr>\n      <td>92</td>\n      <td>0.000500</td>\n    </tr>\n    <tr>\n      <td>93</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>94</td>\n      <td>0.011000</td>\n    </tr>\n    <tr>\n      <td>95</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>96</td>\n      <td>0.000200</td>\n    </tr>\n    <tr>\n      <td>97</td>\n      <td>0.006800</td>\n    </tr>\n    <tr>\n      <td>98</td>\n      <td>0.004600</td>\n    </tr>\n    <tr>\n      <td>99</td>\n      <td>0.004800</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>0.002300</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/peft/utils/other.py:716: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-680d3ce0-2406b2d5029659537fadadb7;19c152ff-1c2a-4ac5-93a4-0a02158c3fa7)\n\nCannot access gated repo for url https://huggingface.co/google/txgemma-9b-predict/resolve/main/config.json.\nAccess to model google/txgemma-9b-predict is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in google/txgemma-9b-predict.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:246: UserWarning: Could not find a config file in google/txgemma-9b-predict - will assume that the vocabulary was not modified.\n  warnings.warn(\n","output_type":"stream"},{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=100, training_loss=0.0008768152229007153, metrics={'train_runtime': 2490.1785, 'train_samples_per_second': 0.241, 'train_steps_per_second': 0.04, 'total_flos': 0.0, 'train_loss': 0.0008768152229007153})"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"import torch\nfrom transformers import AutoModelForCausalLM, BitsAndBytesConfig\nfrom peft import PeftModel\n\n# 1) Quant config exactly as you trained\nquant_config = BitsAndBytesConfig(\n    load_in_4bit                     = True,\n    bnb_4bit_quant_type              = \"nf4\",\n    bnb_4bit_compute_dtype           = torch.float16,\n    llm_int8_enable_fp32_cpu_offload = False,    # for pure GPU inference\n)\n\nmodel_id = \"google/txgemma-9b-predict\"\n\n# 2) Reload the base quantized model from your cache onto GPU0\nbase = AutoModelForCausalLM.from_pretrained(\n    model_id,\n    quantization_config   = quant_config,\n    device_map            = {\"\": 0},       # everything on cuda:0\n    torch_dtype           = torch.float16,\n    local_files_only      = True,          # read from ~/.cache only\n    attn_implementation   = \"eager\",\n)\n\n# 3) Re-attach the adapters from checkpoint-100\npeft_model = PeftModel.from_pretrained(\n    base,\n    \"outputs/checkpoint-100\",  # <-- this folder holds adapter_model.safetensors etc.\n    local_files_only=True,\n)\n\n# 4) Move to GPU0 & eval\ndevice = torch.device(\"cuda:0\")\npeft_model.to(device).eval()\n\n# 5) Prepare your prompt & tokenize\nprompt = SYSTEM_PROMPT.strip() + \"\\n\\nWill there be an adverse event for this trial?\"\ninputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n\n# 6) Generate\nout_ids = peft_model.generate(\n    **inputs,\n    do_sample       = True,\n    temperature     = 0.8,\n    top_p           = 0.95,\n    max_new_tokens  = 200,\n    pad_token_id    = tokenizer.eos_token_id,\n)\n\nprint(tokenizer.decode(out_ids[0], skip_special_tokens=True))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T20:22:55.791636Z","iopub.execute_input":"2025-04-26T20:22:55.792559Z","iopub.status.idle":"2025-04-26T20:26:24.689947Z","shell.execute_reply.started":"2025-04-26T20:22:55.792523Z","shell.execute_reply":"2025-04-26T20:26:24.689233Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c4f162aa35394b74b346abc8d1ddb859"}},"metadata":{}},{"name":"stdout","text":"Respond in the following format:\n<reasoning>\n...\n</reasoning>\n<answer>\n...\n</answer>\n\nWill there be an adverse event for this trial?746\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"from tqdm import tqdm\n\nfor example in dataset.select(range(10)):\n    out = peft_model.generate(\n        **tokenizer(example[\"prompt\"], return_tensors=\"pt\").to(device),\n        do_sample=False,  # greedy for clarity\n        max_new_tokens=200,\n        pad_token_id=tokenizer.eos_token_id,\n    )\n    print(tokenizer.decode(out[0], skip_special_tokens=True))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T20:29:04.722382Z","iopub.execute_input":"2025-04-26T20:29:04.722952Z","iopub.status.idle":"2025-04-26T20:29:18.836768Z","shell.execute_reply.started":"2025-04-26T20:29:04.722920Z","shell.execute_reply":"2025-04-26T20:29:18.836096Z"}},"outputs":[{"name":"stdout","text":"Respond in the following format:\n<reasoning>\n...\n</reasoning>\n<answer>\n...\n</answer>\n\nFrom the following information about a clinical trial, predict whether it would have an adverse event.\n\nTitle: Safety, Tolerability and Pharmacokinetics of Single and Repeat Doses of GSK2292767 in Healthy Participants Who Smoke Cigarettes\nSummary: This study is the first administration of GSK2292767 to humans. The study will evaluate the safety, tolerability, pharmacokinetics (PK) and pharmacodynamics (PD) of single and repeat inhaled doses of GSK2292767 in healthy smokers. This study is intended to provide sufficient confidence in the safety of the molecule and preliminary information on target engagement to allow progression to further repeat dose and proof of mechanism studies. This is a two part, single site, randomized, double-blind (sponsor open), placebo controlled study. Part A will consist of two 3-period interlocking cohorts to evaluate the safety, tolerability and pharmacokinetics of ascending single doses of GSK2292767 administered as a dry powder inhalation. Part B is planned to follow Part A and progression will be based on an acceptable safety, tolerability and pharmacokinetic profiles. Subjects will receive repeat doses of GSK2292767 once daily for 14 days during Part B.     \nPhase: 1\nDisease: Asthma\nMinimum age: 18 Years\nMaximum age: 50 Years\nHealthy volunteers: Accepts Healthy Volunteers\nInterventions: GSK2292767 50 μg blended with lactose and magnesium stearate per blister as powder for inhalation; GSK2292767 500 μg blended with lactose and magnesium stearate per blister as powder for inhalation; Lactose as powder for inhalation\nDrug: Not available\n\nAnswer:535\nRespond in the following format:\n<reasoning>\n...\n</reasoning>\n<answer>\n...\n</answer>\n\nFrom the following information about a clinical trial, predict whether it would have an adverse event.\n\nTitle: Bioequivalence Study of Favipiravir 200 mg Film Tablet (ATABAY, Turkey) Under Fasting Conditions\nSummary: A single dose of Reference product containing 200 mg favipiravir and a single dose of Test product containing 200 mg favipiravir or vice versa; administered with 240 mL of water at room temperature, in each period under fasting conditions with current pandemic precautions.     \nPhase: 1\nDisease: Bioequivalence\nMinimum age: 20 Years\nMaximum age: 40 Years\nHealthy volunteers: Accepts Healthy Volunteers\nInterventions: FAVICOVIR 200 MG FT is containing 200 mg favipiravir manufactured by Atabay, Turkey.; AVIGAN 200 mg FT is containing 200 mg favipiravir manufactured by Toyama, Japan.\nDrug: Not available\n\nAnswer:256\nRespond in the following format:\n<reasoning>\n...\n</reasoning>\n<answer>\n...\n</answer>\n\nFrom the following information about a clinical trial, predict whether it would have an adverse event.\n\nTitle: A Study To Estimate The Effect of PF-06650833 On The Pharmacokinetics (PK) of Oral Contraceptive (OC)\nSummary: This is a Phase 1, open label, fixed sequence study of the effect of multiple dose PF-06650833 on single dose OC PK in healthy female subjects.     \nPhase: 1\nDisease: Healthy\nMinimum age: 18 Years\nMaximum age: 60 Years\nHealthy volunteers: Accepts Healthy Volunteers\nInterventions: 400 mg by mouth (PO) Once daily (QD) for 11 days; Single dose of Oral tablet containing 30 ug EE and 150 ug of LN\nDrug: CC[C@H]1[C@@H](COC2=C3C=C(OC)C(=CC3=CC=N2)C(N)=O)NC(=O)[C@H]1F.[H][C@@]12CC[C@H](O)[C@@]1(C)CC[C@]1([H])C3=C(CC[C@@]21[H])C=C(O)C=C3\n\nAnswer:606\nRespond in the following format:\n<reasoning>\n...\n</reasoning>\n<answer>\n...\n</answer>\n\nFrom the following information about a clinical trial, predict whether it would have an adverse event.\n\nTitle: The Maraviroc Darunavir/Ritonavir Once Daily Pharmacokinetic Study\nSummary: This is a phase I, open label, prospective, two phase pharmacokinetic study. Subjects currently attending for HIV care at St. Mary's Hospital, London will be eligible.     \nPhase: 1\nDisease: HIV\nMinimum age: 18 Years\nMaximum age: 65 Years\nHealthy volunteers: No\nInterventions: Maraviroc 150 mg daily; daily until 10. day then stop; daily until 10. day then stop\nDrug: CC(C)C1=NN=C(C)N1[C@H]1C[C@@H]2CC[C@H](C1)N2CC[C@H](NC(=O)C1CCC(F)(F)CC1)C1=CC=CC=C1.[H][C@@]12CCO[C@]1([H])OC[C@@H]2OC(=O)N[C@@H](CC1=CC=CC=C1)[C@H](O)CN(CC(C)C)S(=O)(=O)C1=CC=C(N)C=C1\n\nAnswer:630\nRespond in the following format:\n<reasoning>\n...\n</reasoning>\n<answer>\n...\n</answer>\n\nFrom the following information about a clinical trial, predict whether it would have an adverse event.\n\nTitle: A Study of LY3009120 in Participants With Advanced Cancer or Cancer That Has Spread to Other Parts of Their Body\nSummary: The main purpose of this study is to see how safe the investigational drug known as LY3009120 is and whether it will work to help people with advanced cancer or cancer that has spread to other parts of the body.     \nPhase: 1\nDisease: Neoplasms; Neoplasm Metastasis; Melanoma; Carcinoma, Non-Small-Cell Lung; Colorectal Neoplasms\nMinimum age: 18 Years\nMaximum age: Not available\nHealthy volunteers: No\nInterventions: Administered orally.\nDrug: Not available\n\nAnswer:1\nRespond in the following format:\n<reasoning>\n...\n</reasoning>\n<answer>\n...\n</answer>\n\nFrom the following information about a clinical trial, predict whether it would have an adverse event.\n\nTitle: Safety, Tolerability and PK of Multiple-ascending Doses of Emodepside\nSummary: The study evaluates safety, tolerability, pharmacokinetics (PK) and pharmacodynamics (PD) of emodepside, after administration as a Liquid Service Formulation (LSF), over 10 days, in healthy male caucasian subjects.     \nPhase: 1\nDisease: Filariasis\nMinimum age: 18 Years\nMaximum age: 45 Years\nHealthy volunteers: Accepts Healthy Volunteers\nInterventions: Emodepside administered as an LSF oral solution (1mg/mL)\nDrug: [H][C@]1(C)OC(=O)[C@]([H])(CC(C)C)N(C)C(=O)[C@@]([H])(CC2=CC=C(C=C2)N2CCOCC2)OC(=O)[C@]([H])(CC(C)C)N(C)C(=O)[C@@]([H])(C)OC(=O)[C@]([H])(CC(C)C)N(C)C(=O)[C@@]([H])(CC2=CC=C(C=C2)N2CCOCC2)OC(=O)[C@]([H])(CC(C)C)N(C)C1=O\n\nAnswer:606\nRespond in the following format:\n<reasoning>\n...\n</reasoning>\n<answer>\n...\n</answer>\n\nFrom the following information about a clinical trial, predict whether it would have an adverse event.\n\nTitle: Drug-drug Interaction Study of Gepotidacin\nSummary: This study is a drug-drug interaction (DDI), pharmacokinetics (PK), safety and tolerability study in adult healthy participants, including Japanese cohort. This study is designed to assess co-administration of probe substrates with gepotidacin in study cohorts 1 to 3 and establishing PK and safety in Japanese participants in cohort 4. Food effect will also be evaluated in cohort 4.     \nPhase: 1\nDisease: Infections, Bacterial\nMinimum age: 18 Years\nMaximum age: 50 Years\nHealthy volunteers: Accepts Healthy Volunteers\nInterventions: Gepotidacin tablets will be available as unit dose strength 750 mg and will be administered orally.; Cimetidine tablets will be available as unit dose strength 400 mg and will be administered orally.; Rifampicin Capsules will be available as unit dose strength 300 mg and will be administered orally.; Midazolam oral syrup 2 milligrams per milliliter (mg/mL) will be available to be administered orally.; Digoxin tablets will be available as unit dose strength 0.25 mg and will be administered orally.; Placebo matching to gepotidacin tablets will be administered orally.\nDrug: O=C1C=CC2=C3N1C[C@@H](CN1CCC(CC1)NCC1=CC4=C(OCCC4)C=N1)N3C(=O)C=N2.CN\\C(NCCSCC1=C(C)NC=N1)=N\\C#N.CO[C@H]1\\C=C\\O[C@@]2(C)OC3=C(C2=O)C2=C(O)C(\\C=N\\N4CCN(C)CC4)=C(NC(=O)\\C(C)=C/C=C/[C@H](C)[C@H](O)[C@@H](C)[C@@H](O)[C@@H](C)[C@H](OC(C)=O)[C@@H]1C)C(O)=C2C(O)=C3C.CC1=NC=C2CN=C(C3=CC=CC=C3F)C3=C(C=CC(Cl)=C3)N12.[H][C@]12CC[C@]3([H])[C@]([H])(C[C@@H](O)[C@]4(C)[C@H](CC[C@]34O)C3=CC(=O)OC3)[C@@]1(C)CC[C@@H](C2)O[C@H]1C[C@H](O)[C@H](O[C@H]2C[C@H](O)[C@H](O[C@H]3C[C@H](O)[C@H](O)[C@@H](C)O3)[C@@H](C)O2)[C@@H](C)O1.O=C1C=CC2=C3N1C[C@@H](CN1CCC(CC1)NCC1=CC4=C(OCCC4)C=N1)N3C(=O)C=N2\n\nAnswer:509\nRespond in the following format:\n<reasoning>\n...\n</reasoning>\n<answer>\n...\n</answer>\n\nFrom the following information about a clinical trial, predict whether it would have an adverse event.\n\nTitle: A Study of LY3819253 (LY-CoV555) in Healthy Participants\nSummary: The purpose of this study is to test the safety and tolerability of LY3819253 when it is given by injection just under the skin to healthy participants. Blood tests will be done to check how much LY3819253 is in the bloodstream and how long the body takes to eliminate it. Participation could last up to 16 weeks and may include up to six visits to the study center, with a one-week overnight stay.     \nPhase: 1\nDisease: Healthy\nMinimum age: 18 Years\nMaximum age: 60 Years\nHealthy volunteers: Accepts Healthy Volunteers\nInterventions: Administered SC.; Administered SC.\nDrug: Not available\n\nAnswer:162\nRespond in the following format:\n<reasoning>\n...\n</reasoning>\n<answer>\n...\n</answer>\n\nFrom the following information about a clinical trial, predict whether it would have an adverse event.\n\nTitle: Vincristine, Doxorubicin, And Dexamethasone + Ixazomib in Acute Lymphoblastic Leukemia (ALL), Lymphoblastic Lymphoma Or Mixed Phenotype Acute Leukemia\nSummary: This is a phase I study of vincristine, doxorubicin and dexamethasone (modified VXD) plus MLN9708 in adults with relapsed or refractory acute lymphoblastic leukemia/lymphoma, lymphoblastic lymphoma or mixed phenotype acute leukemia.     \nPhase: 1\nDisease: Relapsed or Refractory Acute Lymphoblastic Leukemia; Relapsed or Refractory Lymphoblastic Lymphoma; Mixed Phenotype Acute Leukemia\nMinimum age: 18 Years\nMaximum age: Not available\nHealthy volunteers: No\nInterventions: Not available\nDrug: [H][C@@]12N3CC[C@@]11C4=C(C=C(OC)C(=C4)[C@]4(C[C@H]5C[N@](C[C@](O)(CC)C5)CCC5=C4NC4=CC=CC=C54)C(=O)OC)N(C=O)[C@@]1([H])[C@](O)([C@H](OC(C)=O)[C@]2(CC)C=CC3)C(=O)OC.COC1=CC=CC2=C1C(=O)C1=C(O)C3=C(C[C@](O)(C[C@@H]3O[C@H]3C[C@H](N)[C@H](O)[C@H](C)O3)C(=O)CO)C(O)=C1C2=O.[H][C@@]12C[C@@H](C)[C@](O)(C(=O)CO)[C@@]1(C)C[C@H](O)[C@@]1(F)[C@@]2([H])CCC2=CC(=O)C=C[C@]12C\n\nAnswer:535\nRespond in the following format:\n<reasoning>\n...\n</reasoning>\n<answer>\n...\n</answer>\n\nFrom the following information about a clinical trial, predict whether it would have an adverse event.\n\nTitle: Effectiveness of Ivermectin as add-on Therapy in COVID-19 Management\nSummary: Comparing the effectiveness of Ivermectin( IVM) +Hydroxychloroquin + azithromycin (AZT) group to Hydroxychloroquin (HCQ) + azithromycin (AZT)     \nPhase: 1\nDisease: COVID 19\nMinimum age: 18 Years\nMaximum age: Not available\nHealthy volunteers: No\nInterventions: Ivermectin 0.2 mg /kg (single dose at once =2 tablets of 6mg/weekly\nDrug: [H][C@@]12OC\\C3=C/C=C/[C@H](C)[C@H](O[C@H]4C[C@H](OC)[C@@H](O[C@H]5C[C@H](OC)[C@@H](O)[C@H](C)O5)[C@H](C)O4)\\C(C)=C\\C[C@]4([H])C[C@@]([H])(C[C@]5(CC[C@H](C)[C@]([H])(O5)C(C)C)O4)OC(=O)[C@]([H])(C=C(C)[C@H]1O)[C@@]23O.[H][C@@]12OC\\C3=C/C=C/[C@H](C)[C@H](O[C@H]4C[C@H](OC)[C@@H](O[C@H]5C[C@H](OC)[C@@H](O)[C@H](C)O5)[C@H](C)O4)\\C(C)=C\\C[C@]4([H])C[C@@]([H])(C[C@]5(CC[C@H](C)[C@]([H])(O5)[C@@H](C)CC)O4)OC(=O)[C@]([H])(C=C(C)[C@H]1O)[C@@]23O\n\nAnswer:535\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"model.save_lora(\"txgemma_grpo_lora\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Merge to 16bit\nif False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_16bit\",)\nif False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_16bit\", token = \"\")\n\n# Merge to 4bit\nif False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_4bit\",)\nif False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_4bit\", token = \"\")\n\n# Just LoRA adapters\nif False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"lora\",)\nif False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"lora\", token = \"\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Save to 8bit Q8_0\nif False: model.save_pretrained_gguf(\"model\", tokenizer,)\n# Remember to go to https://huggingface.co/settings/tokens for a token!\n# And change hf to your username!\nif False: model.push_to_hub_gguf(\"hf/model\", tokenizer, token = \"\")\n\n# Save to 16bit GGUF\nif False: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"f16\")\nif False: model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"f16\", token = \"\")\n\n# Save to q4_k_m GGUF\nif False: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"q4_k_m\")\nif False: model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"q4_k_m\", token = \"\")\n\n# Save to multiple GGUF options - much faster if you want multiple!\nif False:\n    model.push_to_hub_gguf(\n        \"hf/model\", # Change hf to your username!\n        tokenizer,\n        quantization_method = [\"q4_k_m\", \"q8_0\", \"q5_k_m\",],\n        token = \"\",\n    )","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}