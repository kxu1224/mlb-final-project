{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11569757,"sourceType":"datasetVersion","datasetId":7253662}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install torch torchvision torchaudio xformers --index-url https://download.pytorch.org/whl/cu121\n!pip install unsloth vllm\n!pip install triton==3.1.0\n!pip install -U pynvml","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T22:17:30.488179Z","iopub.execute_input":"2025-04-28T22:17:30.488498Z","iopub.status.idle":"2025-04-28T22:23:56.745726Z","shell.execute_reply.started":"2025-04-28T22:17:30.488474Z","shell.execute_reply":"2025-04-28T22:23:56.744996Z"}},"outputs":[{"name":"stdout","text":"Looking in indexes: https://download.pytorch.org/whl/cu121\nRequirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.20.1+cu124)\nRequirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\nCollecting xformers\n  Downloading https://download.pytorch.org/whl/cu121/xformers-0.0.29.post1-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (1.0 kB)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n  Downloading https://download.pytorch.org/whl/cu121/nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hINFO: pip is looking at multiple versions of torch to determine which version is compatible with other requirements. This could take a while.\nCollecting torch\n  Downloading https://download.pytorch.org/whl/cu121/torch-2.5.1%2Bcu121-cp311-cp311-linux_x86_64.whl (780.5 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m780.5/780.5 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m34.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m91.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n  Downloading https://download.pytorch.org/whl/cu121/nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n  Downloading https://download.pytorch.org/whl/cu121/nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch)\n  Downloading https://download.pytorch.org/whl/cu121/nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n  Downloading https://download.pytorch.org/whl/cu121/nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n  Downloading https://download.pytorch.org/whl/cu121/nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\nCollecting nvidia-nvtx-cu12==12.1.105 (from torch)\n  Downloading https://download.pytorch.org/whl/cu121/nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.8.93)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (1.26.4)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.1.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (2.4.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->torchvision) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->torchvision) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->torchvision) (2024.2.0)\nDownloading https://download.pytorch.org/whl/cu121/xformers-0.0.29.post1-cp311-cp311-manylinux_2_28_x86_64.whl (15.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.3/15.3 MB\u001b[0m \u001b[31m90.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: nvidia-nvtx-cu12, nvidia-cusparse-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusolver-cu12, nvidia-cudnn-cu12, torch, xformers\n  Attempting uninstall: nvidia-nvtx-cu12\n    Found existing installation: nvidia-nvtx-cu12 12.4.127\n    Uninstalling nvidia-nvtx-cu12-12.4.127:\n      Successfully uninstalled nvidia-nvtx-cu12-12.4.127\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.8.93\n    Uninstalling nvidia-cusparse-cu12-12.5.8.93:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.8.93\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.9.90\n    Uninstalling nvidia-curand-cu12-10.3.9.90:\n      Successfully uninstalled nvidia-curand-cu12-10.3.9.90\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.3.3.83\n    Uninstalling nvidia-cufft-cu12-11.3.3.83:\n      Successfully uninstalled nvidia-cufft-cu12-11.3.3.83\n  Attempting uninstall: nvidia-cuda-runtime-cu12\n    Found existing installation: nvidia-cuda-runtime-cu12 12.4.127\n    Uninstalling nvidia-cuda-runtime-cu12-12.4.127:\n      Successfully uninstalled nvidia-cuda-runtime-cu12-12.4.127\n  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n    Found existing installation: nvidia-cuda-nvrtc-cu12 12.4.127\n    Uninstalling nvidia-cuda-nvrtc-cu12-12.4.127:\n      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.4.127\n  Attempting uninstall: nvidia-cuda-cupti-cu12\n    Found existing installation: nvidia-cuda-cupti-cu12 12.4.127\n    Uninstalling nvidia-cuda-cupti-cu12-12.4.127:\n      Successfully uninstalled nvidia-cuda-cupti-cu12-12.4.127\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.8.4.1\n    Uninstalling nvidia-cublas-cu12-12.8.4.1:\n      Successfully uninstalled nvidia-cublas-cu12-12.8.4.1\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.7.3.90\n    Uninstalling nvidia-cusolver-cu12-11.7.3.90:\n      Successfully uninstalled nvidia-cusolver-cu12-11.7.3.90\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: torch\n    Found existing installation: torch 2.5.1+cu124\n    Uninstalling torch-2.5.1+cu124:\n      Successfully uninstalled torch-2.5.1+cu124\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npylibcugraph-cu12 24.12.0 requires pylibraft-cu12==24.12.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires rmm-cu12==24.12.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nvtx-cu12-12.1.105 torch-2.5.1+cu121 xformers-0.0.29.post1\nCollecting unsloth\n  Downloading unsloth-2025.4.1-py3-none-any.whl.metadata (46 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.4/46.4 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting vllm\n  Downloading vllm-0.8.4-cp38-abi3-manylinux1_x86_64.whl.metadata (27 kB)\nCollecting unsloth_zoo>=2025.4.1 (from unsloth)\n  Downloading unsloth_zoo-2025.4.1-py3-none-any.whl.metadata (8.0 kB)\nRequirement already satisfied: torch>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from unsloth) (2.5.1+cu121)\nRequirement already satisfied: xformers>=0.0.27.post2 in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.0.29.post1)\nCollecting bitsandbytes (from unsloth)\n  Downloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl.metadata (5.0 kB)\nRequirement already satisfied: triton>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from unsloth) (3.1.0)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from unsloth) (24.2)\nCollecting tyro (from unsloth)\n  Downloading tyro-0.9.19-py3-none-any.whl.metadata (9.9 kB)\nRequirement already satisfied: transformers!=4.47.0,>=4.46.1 in /usr/local/lib/python3.11/dist-packages (from unsloth) (4.51.1)\nRequirement already satisfied: datasets>=2.16.0 in /usr/local/lib/python3.11/dist-packages (from unsloth) (3.5.0)\nRequirement already satisfied: sentencepiece>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.2.0)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from unsloth) (4.67.1)\nRequirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from unsloth) (7.0.0)\nRequirement already satisfied: wheel>=0.42.0 in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.45.1)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from unsloth) (1.26.4)\nRequirement already satisfied: accelerate>=0.34.1 in /usr/local/lib/python3.11/dist-packages (from unsloth) (1.3.0)\nCollecting trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<=0.15.2,>=0.7.9 (from unsloth)\n  Downloading trl-0.15.2-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: peft!=0.11.0,>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.14.0)\nRequirement already satisfied: protobuf<4.0.0 in /usr/local/lib/python3.11/dist-packages (from unsloth) (3.20.3)\nRequirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.30.2)\nRequirement already satisfied: hf_transfer in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.1.9)\nRequirement already satisfied: diffusers in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.32.2)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.20.1+cu124)\nRequirement already satisfied: cachetools in /usr/local/lib/python3.11/dist-packages (from vllm) (5.5.2)\nRequirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from vllm) (2.32.3)\nCollecting blake3 (from vllm)\n  Downloading blake3-1.0.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\nRequirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from vllm) (9.0.0)\nRequirement already satisfied: tokenizers>=0.19.1 in /usr/local/lib/python3.11/dist-packages (from vllm) (0.21.0)\nCollecting fastapi>=0.115.0 (from fastapi[standard]>=0.115.0->vllm)\n  Downloading fastapi-0.115.12-py3-none-any.whl.metadata (27 kB)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from vllm) (3.11.16)\nRequirement already satisfied: openai>=1.52.0 in /usr/local/lib/python3.11/dist-packages (from vllm) (1.61.1)\nRequirement already satisfied: pydantic>=2.9 in /usr/local/lib/python3.11/dist-packages (from vllm) (2.11.3)\nRequirement already satisfied: prometheus_client>=0.18.0 in /usr/local/lib/python3.11/dist-packages (from vllm) (0.21.1)\nRequirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from vllm) (11.1.0)\nCollecting prometheus-fastapi-instrumentator>=7.0.0 (from vllm)\n  Downloading prometheus_fastapi_instrumentator-7.1.0-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: tiktoken>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from vllm) (0.9.0)\nCollecting lm-format-enforcer<0.11,>=0.10.11 (from vllm)\n  Downloading lm_format_enforcer-0.10.11-py3-none-any.whl.metadata (17 kB)\nCollecting llguidance<0.8.0,>=0.7.9 (from vllm)\n  Downloading llguidance-0.7.19-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\nCollecting outlines==0.1.11 (from vllm)\n  Downloading outlines-0.1.11-py3-none-any.whl.metadata (17 kB)\nCollecting lark==1.2.2 (from vllm)\n  Downloading lark-1.2.2-py3-none-any.whl.metadata (1.8 kB)\nCollecting xgrammar==0.1.18 (from vllm)\n  Downloading xgrammar-0.1.18-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\nRequirement already satisfied: typing_extensions>=4.10 in /usr/local/lib/python3.11/dist-packages (from vllm) (4.13.1)\nRequirement already satisfied: filelock>=3.16.1 in /usr/local/lib/python3.11/dist-packages (from vllm) (3.18.0)\nCollecting partial-json-parser (from vllm)\n  Downloading partial_json_parser-0.2.1.1.post5-py3-none-any.whl.metadata (6.1 kB)\nRequirement already satisfied: pyzmq in /usr/local/lib/python3.11/dist-packages (from vllm) (24.0.1)\nCollecting msgspec (from vllm)\n  Downloading msgspec-0.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)\nCollecting gguf>=0.13.0 (from vllm)\n  Downloading gguf-0.16.2-py3-none-any.whl.metadata (4.4 kB)\nRequirement already satisfied: importlib_metadata in /usr/local/lib/python3.11/dist-packages (from vllm) (8.6.1)\nCollecting mistral_common>=1.5.4 (from mistral_common[opencv]>=1.5.4->vllm)\n  Downloading mistral_common-1.5.4-py3-none-any.whl.metadata (4.5 kB)\nRequirement already satisfied: opencv-python-headless>=4.11.0 in /usr/local/lib/python3.11/dist-packages (from vllm) (4.11.0.86)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from vllm) (6.0.2)\nRequirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from vllm) (0.8.1)\nCollecting compressed-tensors==0.9.3 (from vllm)\n  Downloading compressed_tensors-0.9.3-py3-none-any.whl.metadata (7.0 kB)\nCollecting depyf==0.18.0 (from vllm)\n  Downloading depyf-0.18.0-py3-none-any.whl.metadata (7.1 kB)\nRequirement already satisfied: cloudpickle in /usr/local/lib/python3.11/dist-packages (from vllm) (3.1.1)\nCollecting watchfiles (from vllm)\n  Downloading watchfiles-1.0.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\nRequirement already satisfied: python-json-logger in /usr/local/lib/python3.11/dist-packages (from vllm) (3.3.0)\nRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from vllm) (1.15.2)\nRequirement already satisfied: ninja in /usr/local/lib/python3.11/dist-packages (from vllm) (1.11.1.4)\nCollecting opentelemetry-sdk<1.27.0,>=1.26.0 (from vllm)\n  Downloading opentelemetry_sdk-1.26.0-py3-none-any.whl.metadata (1.5 kB)\nCollecting opentelemetry-api<1.27.0,>=1.26.0 (from vllm)\n  Downloading opentelemetry_api-1.26.0-py3-none-any.whl.metadata (1.4 kB)\nCollecting opentelemetry-exporter-otlp<1.27.0,>=1.26.0 (from vllm)\n  Downloading opentelemetry_exporter_otlp-1.26.0-py3-none-any.whl.metadata (2.3 kB)\nCollecting opentelemetry-semantic-conventions-ai<0.5.0,>=0.4.1 (from vllm)\n  Downloading opentelemetry_semantic_conventions_ai-0.4.3-py3-none-any.whl.metadata (1.2 kB)\nCollecting numba==0.61.2 (from vllm)\n  Downloading numba-0.61.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.8 kB)\nCollecting ray!=2.44.*,>=2.43.0 (from ray[cgraph]!=2.44.*,>=2.43.0->vllm)\n  Downloading ray-2.43.0-cp311-cp311-manylinux2014_x86_64.whl.metadata (19 kB)\nCollecting torch>=2.4.0 (from unsloth)\n  Downloading torch-2.6.0-cp311-cp311-manylinux1_x86_64.whl.metadata (28 kB)\nCollecting torchaudio==2.6.0 (from vllm)\n  Downloading torchaudio-2.6.0-cp311-cp311-manylinux1_x86_64.whl.metadata (6.6 kB)\nCollecting torchvision (from unsloth)\n  Downloading torchvision-0.21.0-cp311-cp311-manylinux1_x86_64.whl.metadata (6.1 kB)\nCollecting xformers>=0.0.27.post2 (from unsloth)\n  Downloading xformers-0.0.29.post2-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (1.0 kB)\nCollecting astor (from depyf==0.18.0->vllm)\n  Downloading astor-0.8.1-py2.py3-none-any.whl.metadata (4.2 kB)\nRequirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from depyf==0.18.0->vllm) (0.3.8)\nCollecting llvmlite<0.45,>=0.44.0dev0 (from numba==0.61.2->vllm)\n  Downloading llvmlite-0.44.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.8 kB)\nCollecting interegular (from outlines==0.1.11->vllm)\n  Downloading interegular-0.3.3-py37-none-any.whl.metadata (3.0 kB)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from outlines==0.1.11->vllm) (3.1.6)\nRequirement already satisfied: nest_asyncio in /usr/local/lib/python3.11/dist-packages (from outlines==0.1.11->vllm) (1.6.0)\nCollecting diskcache (from outlines==0.1.11->vllm)\n  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\nRequirement already satisfied: referencing in /usr/local/lib/python3.11/dist-packages (from outlines==0.1.11->vllm) (0.36.2)\nRequirement already satisfied: jsonschema in /usr/local/lib/python3.11/dist-packages (from outlines==0.1.11->vllm) (4.23.0)\nCollecting pycountry (from outlines==0.1.11->vllm)\n  Downloading pycountry-24.6.1-py3-none-any.whl.metadata (12 kB)\nCollecting airportsdata (from outlines==0.1.11->vllm)\n  Downloading airportsdata-20250224-py3-none-any.whl.metadata (9.0 kB)\nCollecting outlines_core==0.1.26 (from outlines==0.1.11->vllm)\n  Downloading outlines_core-0.1.26-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (3.4.2)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (2025.3.2)\nCollecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (9.1.0.70)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparselt-cu12==0.6.2 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (2.21.5)\nCollecting nvidia-nvtx-cu12==12.4.127 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.7 kB)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting triton>=3.0.0 (from unsloth)\n  Downloading triton-3.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.4.0->unsloth) (1.3.0)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.34.1->unsloth) (0.5.2)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0->unsloth) (19.0.1)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0->unsloth) (2.2.3)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0->unsloth) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0->unsloth) (0.70.16)\nCollecting fsspec (from torch>=2.4.0->unsloth)\n  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\nCollecting starlette<0.47.0,>=0.40.0 (from fastapi>=0.115.0->fastapi[standard]>=0.115.0->vllm)\n  Downloading starlette-0.46.2-py3-none-any.whl.metadata (6.2 kB)\nCollecting fastapi-cli>=0.0.5 (from fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm)\n  Downloading fastapi_cli-0.0.7-py3-none-any.whl.metadata (6.2 kB)\nRequirement already satisfied: httpx>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from fastapi[standard]>=0.115.0->vllm) (0.28.1)\nCollecting python-multipart>=0.0.18 (from fastapi[standard]>=0.115.0->vllm)\n  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\nRequirement already satisfied: email-validator>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from fastapi[standard]>=0.115.0->vllm) (2.2.0)\nCollecting uvicorn>=0.12.0 (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm)\n  Downloading uvicorn-0.34.2-py3-none-any.whl.metadata (6.5 kB)\nCollecting hf-xet>=0.1.4 (from huggingface-hub[hf_xet]>=0.30.0->vllm)\n  Downloading hf_xet-1.0.5-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (494 bytes)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->unsloth) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->unsloth) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->unsloth) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->unsloth) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->unsloth) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->unsloth) (2.4.1)\nRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.52.0->vllm) (3.7.1)\nRequirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.52.0->vllm) (1.9.0)\nRequirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.52.0->vllm) (0.8.2)\nRequirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai>=1.52.0->vllm) (1.3.1)\nRequirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api<1.27.0,>=1.26.0->vllm) (1.2.18)\nCollecting importlib_metadata (from vllm)\n  Downloading importlib_metadata-8.0.0-py3-none-any.whl.metadata (4.6 kB)\nRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.11/dist-packages (from importlib_metadata->vllm) (3.21.0)\nCollecting opentelemetry-exporter-otlp-proto-grpc==1.26.0 (from opentelemetry-exporter-otlp<1.27.0,>=1.26.0->vllm)\n  Downloading opentelemetry_exporter_otlp_proto_grpc-1.26.0-py3-none-any.whl.metadata (2.3 kB)\nCollecting opentelemetry-exporter-otlp-proto-http==1.26.0 (from opentelemetry-exporter-otlp<1.27.0,>=1.26.0->vllm)\n  Downloading opentelemetry_exporter_otlp_proto_http-1.26.0-py3-none-any.whl.metadata (2.3 kB)\nRequirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc==1.26.0->opentelemetry-exporter-otlp<1.27.0,>=1.26.0->vllm) (1.67.0)\nRequirement already satisfied: grpcio<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc==1.26.0->opentelemetry-exporter-otlp<1.27.0,>=1.26.0->vllm) (1.70.0)\nCollecting opentelemetry-exporter-otlp-proto-common==1.26.0 (from opentelemetry-exporter-otlp-proto-grpc==1.26.0->opentelemetry-exporter-otlp<1.27.0,>=1.26.0->vllm)\n  Downloading opentelemetry_exporter_otlp_proto_common-1.26.0-py3-none-any.whl.metadata (1.8 kB)\nCollecting opentelemetry-proto==1.26.0 (from opentelemetry-exporter-otlp-proto-grpc==1.26.0->opentelemetry-exporter-otlp<1.27.0,>=1.26.0->vllm)\n  Downloading opentelemetry_proto-1.26.0-py3-none-any.whl.metadata (2.3 kB)\nCollecting opentelemetry-semantic-conventions==0.47b0 (from opentelemetry-sdk<1.27.0,>=1.26.0->vllm)\n  Downloading opentelemetry_semantic_conventions-0.47b0-py3-none-any.whl.metadata (2.4 kB)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9->vllm) (0.7.0)\nRequirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9->vllm) (2.33.1)\nRequirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9->vllm) (0.4.0)\nRequirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from ray!=2.44.*,>=2.43.0->ray[cgraph]!=2.44.*,>=2.43.0->vllm) (8.1.8)\nRequirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from ray!=2.44.*,>=2.43.0->ray[cgraph]!=2.44.*,>=2.43.0->vllm) (1.1.0)\nRequirement already satisfied: aiosignal in /usr/local/lib/python3.11/dist-packages (from ray!=2.44.*,>=2.43.0->ray[cgraph]!=2.44.*,>=2.43.0->vllm) (1.3.2)\nRequirement already satisfied: frozenlist in /usr/local/lib/python3.11/dist-packages (from ray!=2.44.*,>=2.43.0->ray[cgraph]!=2.44.*,>=2.43.0->vllm) (1.5.0)\nRequirement already satisfied: cupy-cuda12x in /usr/local/lib/python3.11/dist-packages (from ray[cgraph]!=2.44.*,>=2.43.0->vllm) (13.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->vllm) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->vllm) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->vllm) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->vllm) (2025.1.31)\nRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken>=0.6.0->vllm) (2024.11.6)\nRequirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<=0.15.2,>=0.7.9->unsloth) (14.0.0)\nCollecting cut_cross_entropy (from unsloth_zoo>=2025.4.1->unsloth)\n  Downloading cut_cross_entropy-25.1.1-py3-none-any.whl.metadata (9.3 kB)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->vllm) (2.6.1)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->vllm) (25.3.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->vllm) (6.2.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->vllm) (0.3.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->vllm) (1.19.0)\nRequirement already satisfied: docstring-parser>=0.15 in /usr/local/lib/python3.11/dist-packages (from tyro->unsloth) (0.16)\nCollecting shtab>=1.5.6 (from tyro->unsloth)\n  Downloading shtab-1.7.2-py3-none-any.whl.metadata (7.4 kB)\nRequirement already satisfied: typeguard>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from tyro->unsloth) (4.4.1)\nRequirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.11/dist-packages (from deprecated>=1.2.6->opentelemetry-api<1.27.0,>=1.26.0->vllm) (1.17.2)\nRequirement already satisfied: dnspython>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from email-validator>=2.0.0->fastapi[standard]>=0.115.0->vllm) (2.7.0)\nRequirement already satisfied: typer>=0.12.3 in /usr/local/lib/python3.11/dist-packages (from fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.15.1)\nCollecting rich-toolkit>=0.11.1 (from fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm)\n  Downloading rich_toolkit-0.14.3-py3-none-any.whl.metadata (999 bytes)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.23.0->fastapi[standard]>=0.115.0->vllm) (1.0.7)\nRequirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.23.0->fastapi[standard]>=0.115.0->vllm) (0.14.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->outlines==0.1.11->vllm) (3.0.2)\nRequirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema->outlines==0.1.11->vllm) (2024.10.1)\nRequirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema->outlines==0.1.11->vllm) (0.22.3)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<=0.15.2,>=0.7.9->unsloth) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<=0.15.2,>=0.7.9->unsloth) (2.19.1)\nCollecting httptools>=0.6.3 (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm)\n  Downloading httptools-0.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\nCollecting python-dotenv>=0.13 (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm)\n  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\nCollecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm)\n  Downloading uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\nRequirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (14.2)\nRequirement already satisfied: fastrlock>=0.5 in /usr/local/lib/python3.11/dist-packages (from cupy-cuda12x->ray[cgraph]!=2.44.*,>=2.43.0->vllm) (0.8.3)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->unsloth) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->unsloth) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->unsloth) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->unsloth) (2024.2.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=2.16.0->unsloth) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=2.16.0->unsloth) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=2.16.0->unsloth) (2025.2)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->unsloth) (2024.2.0)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<=0.15.2,>=0.7.9->unsloth) (0.1.2)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.16.0->unsloth) (1.17.0)\nRequirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.12.3->fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (1.5.4)\nDownloading unsloth-2025.4.1-py3-none-any.whl (193 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.2/193.2 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading vllm-0.8.4-cp38-abi3-manylinux1_x86_64.whl (294.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.1/294.1 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading compressed_tensors-0.9.3-py3-none-any.whl (98 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.4/98.4 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading depyf-0.18.0-py3-none-any.whl (38 kB)\nDownloading lark-1.2.2-py3-none-any.whl (111 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.0/111.0 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading numba-0.61.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m81.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading outlines-0.1.11-py3-none-any.whl (87 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.6/87.6 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading torch-2.6.0-cp311-cp311-manylinux1_x86_64.whl (766.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m766.7/766.7 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading torchaudio-2.6.0-cp311-cp311-manylinux1_x86_64.whl (3.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m\n\u001b[?25hDownloading torchvision-0.21.0-cp311-cp311-manylinux1_x86_64.whl (7.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading triton-3.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (253.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.2/253.2 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0mm\n\u001b[?25hDownloading xformers-0.0.29.post2-cp311-cp311-manylinux_2_28_x86_64.whl (44.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 MB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading xgrammar-0.1.18-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl (150.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.1/150.1 MB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m35.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (99 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading outlines_core-0.1.26-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (343 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m343.3/343.3 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading fastapi-0.115.12-py3-none-any.whl (95 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading gguf-0.16.2-py3-none-any.whl (92 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.2/92.2 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading llguidance-0.7.19-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.0/14.0 MB\u001b[0m \u001b[31m36.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading lm_format_enforcer-0.10.11-py3-none-any.whl (44 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.2/44.2 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading mistral_common-1.5.4-py3-none-any.whl (6.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m36.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading opentelemetry_api-1.26.0-py3-none-any.whl (61 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.5/61.5 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading importlib_metadata-8.0.0-py3-none-any.whl (24 kB)\nDownloading opentelemetry_exporter_otlp-1.26.0-py3-none-any.whl (7.0 kB)\nDownloading opentelemetry_exporter_otlp_proto_grpc-1.26.0-py3-none-any.whl (18 kB)\nDownloading opentelemetry_exporter_otlp_proto_http-1.26.0-py3-none-any.whl (16 kB)\nDownloading opentelemetry_exporter_otlp_proto_common-1.26.0-py3-none-any.whl (17 kB)\nDownloading opentelemetry_proto-1.26.0-py3-none-any.whl (52 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.5/52.5 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading opentelemetry_sdk-1.26.0-py3-none-any.whl (109 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.5/109.5 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading opentelemetry_semantic_conventions-0.47b0-py3-none-any.whl (138 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.0/138.0 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading opentelemetry_semantic_conventions_ai-0.4.3-py3-none-any.whl (5.4 kB)\nDownloading prometheus_fastapi_instrumentator-7.1.0-py3-none-any.whl (19 kB)\nDownloading ray-2.43.0-cp311-cp311-manylinux2014_x86_64.whl (67.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.7/67.7 MB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading trl-0.15.2-py3-none-any.whl (318 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.9/318.9 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading unsloth_zoo-2025.4.1-py3-none-any.whl (128 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.9/128.9 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl (76.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading blake3-1.0.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (376 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m376.2/376.2 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading msgspec-0.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (210 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.7/210.7 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading partial_json_parser-0.2.1.1.post5-py3-none-any.whl (10 kB)\nDownloading tyro-0.9.19-py3-none-any.whl (124 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.3/124.3 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading watchfiles-1.0.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (454 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m454.8/454.8 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading fastapi_cli-0.0.7-py3-none-any.whl (10 kB)\nDownloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading hf_xet-1.0.5-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (54.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.0/54.0 MB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading interegular-0.3.3-py37-none-any.whl (23 kB)\nDownloading llvmlite-0.44.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (42.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.4/42.4 MB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\nDownloading shtab-1.7.2-py3-none-any.whl (14 kB)\nDownloading starlette-0.46.2-py3-none-any.whl (72 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading uvicorn-0.34.2-py3-none-any.whl (62 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading airportsdata-20250224-py3-none-any.whl (913 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m913.7/913.7 kB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading astor-0.8.1-py2.py3-none-any.whl (27 kB)\nDownloading cut_cross_entropy-25.1.1-py3-none-any.whl (22 kB)\nDownloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pycountry-24.6.1-py3-none-any.whl (6.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m32.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading httptools-0.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (459 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\nDownloading rich_toolkit-0.14.3-py3-none-any.whl (24 kB)\nDownloading uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m30.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: triton, nvidia-cusparselt-cu12, blake3, uvloop, uvicorn, shtab, python-multipart, python-dotenv, pycountry, partial-json-parser, opentelemetry-semantic-conventions-ai, opentelemetry-proto, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, msgspec, llvmlite, llguidance, lark, interegular, importlib_metadata, httptools, hf-xet, fsspec, diskcache, astor, airportsdata, watchfiles, starlette, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, nvidia-cusparse-cu12, depyf, tyro, rich-toolkit, prometheus-fastapi-instrumentator, opentelemetry-semantic-conventions, nvidia-cusolver-cu12, lm-format-enforcer, fastapi, torch, ray, outlines_core, opentelemetry-sdk, fastapi-cli, torchaudio, opentelemetry-exporter-otlp-proto-http, opentelemetry-exporter-otlp-proto-grpc, cut_cross_entropy, opentelemetry-exporter-otlp, trl, mistral_common, xgrammar, xformers, unsloth_zoo, torchvision, outlines, numba, gguf, compressed-tensors, bitsandbytes, vllm, unsloth\n  Attempting uninstall: triton\n    Found existing installation: triton 3.1.0\n    Uninstalling triton-3.1.0:\n      Successfully uninstalled triton-3.1.0\n  Attempting uninstall: nvidia-nvtx-cu12\n    Found existing installation: nvidia-nvtx-cu12 12.1.105\n    Uninstalling nvidia-nvtx-cu12-12.1.105:\n      Successfully uninstalled nvidia-nvtx-cu12-12.1.105\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.8.93\n    Uninstalling nvidia-nvjitlink-cu12-12.8.93:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.8.93\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.2.106\n    Uninstalling nvidia-curand-cu12-10.3.2.106:\n      Successfully uninstalled nvidia-curand-cu12-10.3.2.106\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.0.2.54\n    Uninstalling nvidia-cufft-cu12-11.0.2.54:\n      Successfully uninstalled nvidia-cufft-cu12-11.0.2.54\n  Attempting uninstall: nvidia-cuda-runtime-cu12\n    Found existing installation: nvidia-cuda-runtime-cu12 12.1.105\n    Uninstalling nvidia-cuda-runtime-cu12-12.1.105:\n      Successfully uninstalled nvidia-cuda-runtime-cu12-12.1.105\n  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n    Found existing installation: nvidia-cuda-nvrtc-cu12 12.1.105\n    Uninstalling nvidia-cuda-nvrtc-cu12-12.1.105:\n      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.1.105\n  Attempting uninstall: nvidia-cuda-cupti-cu12\n    Found existing installation: nvidia-cuda-cupti-cu12 12.1.105\n    Uninstalling nvidia-cuda-cupti-cu12-12.1.105:\n      Successfully uninstalled nvidia-cuda-cupti-cu12-12.1.105\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.1.3.1\n    Uninstalling nvidia-cublas-cu12-12.1.3.1:\n      Successfully uninstalled nvidia-cublas-cu12-12.1.3.1\n  Attempting uninstall: llvmlite\n    Found existing installation: llvmlite 0.43.0\n    Uninstalling llvmlite-0.43.0:\n      Successfully uninstalled llvmlite-0.43.0\n  Attempting uninstall: importlib_metadata\n    Found existing installation: importlib_metadata 8.6.1\n    Uninstalling importlib_metadata-8.6.1:\n      Successfully uninstalled importlib_metadata-8.6.1\n  Attempting uninstall: fsspec\n    Found existing installation: fsspec 2025.3.2\n    Uninstalling fsspec-2025.3.2:\n      Successfully uninstalled fsspec-2025.3.2\n  Attempting uninstall: opentelemetry-api\n    Found existing installation: opentelemetry-api 1.16.0\n    Uninstalling opentelemetry-api-1.16.0:\n      Successfully uninstalled opentelemetry-api-1.16.0\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.1.0.106\n    Uninstalling nvidia-cusparse-cu12-12.1.0.106:\n      Successfully uninstalled nvidia-cusparse-cu12-12.1.0.106\n  Attempting uninstall: opentelemetry-semantic-conventions\n    Found existing installation: opentelemetry-semantic-conventions 0.37b0\n    Uninstalling opentelemetry-semantic-conventions-0.37b0:\n      Successfully uninstalled opentelemetry-semantic-conventions-0.37b0\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.4.5.107\n    Uninstalling nvidia-cusolver-cu12-11.4.5.107:\n      Successfully uninstalled nvidia-cusolver-cu12-11.4.5.107\n  Attempting uninstall: torch\n    Found existing installation: torch 2.5.1+cu121\n    Uninstalling torch-2.5.1+cu121:\n      Successfully uninstalled torch-2.5.1+cu121\n  Attempting uninstall: ray\n    Found existing installation: ray 2.44.1\n    Uninstalling ray-2.44.1:\n      Successfully uninstalled ray-2.44.1\n  Attempting uninstall: opentelemetry-sdk\n    Found existing installation: opentelemetry-sdk 1.16.0\n    Uninstalling opentelemetry-sdk-1.16.0:\n      Successfully uninstalled opentelemetry-sdk-1.16.0\n  Attempting uninstall: torchaudio\n    Found existing installation: torchaudio 2.5.1+cu124\n    Uninstalling torchaudio-2.5.1+cu124:\n      Successfully uninstalled torchaudio-2.5.1+cu124\n  Attempting uninstall: xformers\n    Found existing installation: xformers 0.0.29.post1\n    Uninstalling xformers-0.0.29.post1:\n      Successfully uninstalled xformers-0.0.29.post1\n  Attempting uninstall: torchvision\n    Found existing installation: torchvision 0.20.1+cu124\n    Uninstalling torchvision-0.20.1+cu124:\n      Successfully uninstalled torchvision-0.20.1+cu124\n  Attempting uninstall: numba\n    Found existing installation: numba 0.60.0\n    Uninstalling numba-0.60.0:\n      Successfully uninstalled numba-0.60.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ndask-cuda 25.2.0 requires numba<0.61.0a0,>=0.59.1, but you have numba 0.61.2 which is incompatible.\ncuml-cu12 25.2.1 requires numba<0.61.0a0,>=0.59.1, but you have numba 0.61.2 which is incompatible.\ncudf-cu12 25.2.2 requires numba<0.61.0a0,>=0.59.1, but you have numba 0.61.2 which is incompatible.\ndistributed-ucxx-cu12 0.42.0 requires numba<0.61.0a0,>=0.59.1, but you have numba 0.61.2 which is incompatible.\nydata-profiling 4.16.1 requires numba<=0.61,>=0.56.0, but you have numba 0.61.2 which is incompatible.\ngcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.12.0 which is incompatible.\nbigframes 1.36.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\nfastai 2.7.18 requires torch<2.6,>=1.10, but you have torch 2.6.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires pylibraft-cu12==24.12.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires rmm-cu12==24.12.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed airportsdata-20250224 astor-0.8.1 bitsandbytes-0.45.5 blake3-1.0.4 compressed-tensors-0.9.3 cut_cross_entropy-25.1.1 depyf-0.18.0 diskcache-5.6.3 fastapi-0.115.12 fastapi-cli-0.0.7 fsspec-2024.12.0 gguf-0.16.2 hf-xet-1.0.5 httptools-0.6.4 importlib_metadata-8.0.0 interegular-0.3.3 lark-1.2.2 llguidance-0.7.19 llvmlite-0.44.0 lm-format-enforcer-0.10.11 mistral_common-1.5.4 msgspec-0.19.0 numba-0.61.2 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-cusparselt-cu12-0.6.2 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.4.127 opentelemetry-api-1.26.0 opentelemetry-exporter-otlp-1.26.0 opentelemetry-exporter-otlp-proto-common-1.26.0 opentelemetry-exporter-otlp-proto-grpc-1.26.0 opentelemetry-exporter-otlp-proto-http-1.26.0 opentelemetry-proto-1.26.0 opentelemetry-sdk-1.26.0 opentelemetry-semantic-conventions-0.47b0 opentelemetry-semantic-conventions-ai-0.4.3 outlines-0.1.11 outlines_core-0.1.26 partial-json-parser-0.2.1.1.post5 prometheus-fastapi-instrumentator-7.1.0 pycountry-24.6.1 python-dotenv-1.1.0 python-multipart-0.0.20 ray-2.43.0 rich-toolkit-0.14.3 shtab-1.7.2 starlette-0.46.2 torch-2.6.0 torchaudio-2.6.0 torchvision-0.21.0 triton-3.2.0 trl-0.15.2 tyro-0.9.19 unsloth-2025.4.1 unsloth_zoo-2025.4.1 uvicorn-0.34.2 uvloop-0.21.0 vllm-0.8.4 watchfiles-1.0.5 xformers-0.0.29.post2 xgrammar-0.1.18\nCollecting triton==3.1.0\n  Downloading triton-3.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.3 kB)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from triton==3.1.0) (3.18.0)\nDownloading triton-3.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.5/209.5 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: triton\n  Attempting uninstall: triton\n    Found existing installation: triton 3.2.0\n    Uninstalling triton-3.2.0:\n      Successfully uninstalled triton-3.2.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntorch 2.6.0 requires triton==3.2.0; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have triton 3.1.0 which is incompatible.\nfastai 2.7.18 requires torch<2.6,>=1.10, but you have torch 2.6.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed triton-3.1.0\nRequirement already satisfied: pynvml in /usr/local/lib/python3.11/dist-packages (12.0.0)\nRequirement already satisfied: nvidia-ml-py<13.0.0a0,>=12.0.0 in /usr/local/lib/python3.11/dist-packages (from pynvml) (12.570.86)\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_value_0 = user_secrets.get_secret(\"HF_Token\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T22:23:56.747320Z","iopub.execute_input":"2025-04-28T22:23:56.747631Z","iopub.status.idle":"2025-04-28T22:23:56.893135Z","shell.execute_reply.started":"2025-04-28T22:23:56.747601Z","shell.execute_reply":"2025-04-28T22:23:56.892432Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# CELL 1: Force Single GPU\nimport os\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T22:23:56.893918Z","iopub.execute_input":"2025-04-28T22:23:56.894123Z","iopub.status.idle":"2025-04-28T22:23:56.897893Z","shell.execute_reply.started":"2025-04-28T22:23:56.894106Z","shell.execute_reply":"2025-04-28T22:23:56.897222Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n\nmodel_id = \"google/txgemma-2b-predict\"\n\nquant_config = BitsAndBytesConfig(\n    load_in_4bit            = True,\n    bnb_4bit_quant_type     = \"nf4\",\n    bnb_4bit_compute_dtype  = torch.float16,\n    llm_int8_enable_fp32_cpu_offload = False,\n)\n\ntokenizer = AutoTokenizer.from_pretrained(model_id,token=secret_value_0)  # loads tokenizer.json, tokenizer.model,\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_id,\n    token = secret_value_0,\n    quantization_config = quant_config,\n    device_map          = {'':0},      #Use one GPU\n    torch_dtype         = torch.float16,\n    attn_implementation = \"eager\",     #Google’s preferred attention implementation for this model\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T21:44:47.109816Z","iopub.execute_input":"2025-04-27T21:44:47.110048Z","iopub.status.idle":"2025-04-27T21:45:58.641145Z","shell.execute_reply.started":"2025-04-27T21:44:47.110031Z","shell.execute_reply":"2025-04-27T21:45:58.640460Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/46.4k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bf40d1c3265d461795b4822ec7333b1d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"671edb4880ac40738f75506669c41ae2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d1cfe17eab0941dca89a52aa4e6aac1e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1f9700a9c9304c0a8fff1eab2a75cea2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/818 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4b42b27ef05b443e8fc1c4fb7ab46516"}},"metadata":{}},{"name":"stderr","text":"2025-04-27 21:44:59.594995: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1745790299.789477      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1745790299.849730      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/24.2k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ab25a45f38084075a501e7b30fee5af9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"14ca7c6af9a7446aa113b40b08f62637"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00003.safetensors:   0%|          | 0.00/4.99G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0b6c8f7203e743ef87833f9da60c856a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00003.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5d0410205e6d4aeb986a598311512111"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00003.safetensors:   0%|          | 0.00/481M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"437eaefbbc8f4d7aa45b529d70f7a6d1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7d6a320f091f46a48309207d40daade4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/168 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"95ab6c003c0046fdb3b9c984043a8f65"}},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"import re\nfrom datasets import load_dataset, Dataset\n\n# Load and prep dataset\nSYSTEM_PROMPT = \"\"\"\nRespond in the following format:\n<reasoning>\n...\n</reasoning>\n<answer>\n...\n</answer>\n\"\"\"\n\ndef get_trialbench(split=\"train\"):\n    raw = load_dataset(\n        \"json\",\n        data_files=\"/kaggle/input/txgemma-datasets-trialbench-adverse-event/txgemma_datasets_trialbench_adverse-event-rate-prediction_train.jsonl\",\n        split=split,\n    )\n    # Concatenate the system prompt and the user text into one string\n    def fmt(x):\n        return {\n            \"prompt\": SYSTEM_PROMPT.strip() + \"\\n\\n\" + x[\"input_text\"].strip(),\n            \"answer\": x[\"output_text\"].strip(),\n        }\n    return raw.map(fmt)\n\ndataset = get_trialbench()\n\ndef extract_xml_answer(text: str) -> str:\n    # exactly as before\n    answer = text.split(\"<answer>\")[-1].split(\"</answer>\")[0]\n    return answer.strip()\n\ndef count_xml(text: str) -> float:\n    count = 0.0\n    if text.count(\"<reasoning>\\n\") == 1:        count += 0.125\n    if text.count(\"\\n</reasoning>\\n\") == 1:     count += 0.125\n    if text.count(\"\\n<answer>\\n\") == 1:\n        count += 0.125\n        count -= len(text.split(\"\\n</answer>\\n\")[-1]) * 0.001\n    if text.count(\"\\n</answer>\") == 1:\n        count += 0.125\n        count -= (len(text.split(\"\\n</answer>\")[-1]) - 1) * 0.001\n    return count\n\ndef xmlcount_reward_func(completions, **kwargs) -> list[float]:\n    # completions is List[str]\n    return [count_xml(c) for c in completions]\n\ndef strict_format_reward_func(completions, **kwargs) -> list[float]:\n    pattern = r\"^<reasoning>\\n.*?\\n</reasoning>\\n<answer>\\n.*?\\n</answer>\\n$\"\n    return [0.5 if re.match(pattern, c, re.DOTALL) else 0.0 for c in completions]\n\ndef soft_format_reward_func(completions, **kwargs) -> list[float]:\n    pattern = r\"<reasoning>.*?</reasoning>\\s*<answer>.*?</answer>\"\n    return [0.5 if re.search(pattern, c, re.DOTALL) else 0.0 for c in completions]\n\ndef int_reward_func(completions, **kwargs) -> list[float]:\n    extracted = [extract_xml_answer(c) for c in completions]\n    return [0.5 if r.isdigit() else 0.0 for r in extracted]\n\ndef correctness_reward_func(prompts, completions, answer, **kwargs) -> list[float]:\n    # `answer` is List[str] of ground-truth\n    extracted = [extract_xml_answer(c) for c in completions]\n    return [2.0 if r == a else 0.0 for r, a in zip(extracted, answer)]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T21:45:58.642069Z","iopub.execute_input":"2025-04-27T21:45:58.642746Z","iopub.status.idle":"2025-04-27T21:46:00.820500Z","shell.execute_reply.started":"2025-04-27T21:45:58.642717Z","shell.execute_reply":"2025-04-27T21:46:00.819755Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2718c70278d741908cd2bbb9626c6720"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/14368 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d25b6980801e4897976acc0782254c3c"}},"metadata":{}}],"execution_count":5},{"cell_type":"markdown","source":"**Modified Cell with new Training Settings**","metadata":{}},{"cell_type":"code","source":"from peft import LoraConfig, get_peft_model\nfrom trl  import GRPOConfig, GRPOTrainer\nimport torch\n\n# 1) Attach LoRA adapters as before\nlora_cfg = LoraConfig(\n    r              = 16,\n    lora_alpha     = 16,\n    target_modules = [\"gate_proj\", \"up_proj\", \"down_proj\"],\n    bias           = \"none\",\n    task_type      = \"CAUSAL_LM\",\n)\npeft_model = get_peft_model(model, lora_cfg)\n\n# 2) Build GRPOConfig tuned for a 2B model\ntraining_args = GRPOConfig(\n    use_vllm                    = False,\n    learning_rate               = 5e-6,\n    per_device_train_batch_size = 4,        # smaller = faster\n    gradient_accumulation_steps = 4,        # effective batch of 2\n    num_generations             = 2,        # one rollout per step\n    max_prompt_length           = 128,\n    max_completion_length       = 100,\n    max_steps                   = 5000,\n    save_steps                  = 250,\n    output_dir                  = \"outputs\",\n\n    # precision\n    bf16                        = False,\n    fp16                        = True,\n\n    # optimizer / schedule\n    weight_decay                = 0.1,\n    warmup_ratio                = 0.1,\n    lr_scheduler_type           = \"cosine\",\n    optim                       = \"paged_adamw_8bit\",\n    max_grad_norm               = 0.1,\n\n    # richer logging\n    logging_strategy            = \"steps\",        # log every N steps\n    logging_steps               = 20,\n    log_completions             = True,           # show sample outputs\n    report_to                   = \"tensorboard\",   # or \"wandb\"\n)\n\n# 3) Launch trainer (no eval split here)\ntrainer = GRPOTrainer(\n    model            = peft_model,\n    processing_class = tokenizer,\n    train_dataset    = dataset,\n    reward_funcs     = [\n        xmlcount_reward_func,\n        soft_format_reward_func,\n        strict_format_reward_func,\n        int_reward_func,\n        correctness_reward_func,\n    ],\n    args             = training_args,\n)\n\n# 4) Start training\ntrainer.train()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T20:34:44.954377Z","iopub.execute_input":"2025-04-27T20:34:44.955109Z","iopub.status.idle":"2025-04-27T20:34:45.947633Z","shell.execute_reply.started":"2025-04-27T20:34:44.955080Z","shell.execute_reply":"2025-04-27T20:34:45.946621Z"}},"outputs":[{"name":"stderr","text":"No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_31/3793210434.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;31m# 4) Start training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2243\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2244\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2245\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2246\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2247\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2372\u001b[0m                     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2373\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2374\u001b[0;31m                     \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2375\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2376\u001b[0m                 \u001b[0;31m# to handle cases wherein we pass \"DummyScheduler\" such as when it is specified in DeepSpeed config.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/accelerator.py\u001b[0m in \u001b[0;36mprepare\u001b[0;34m(self, device_placement, *args)\u001b[0m\n\u001b[1;32m   1337\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp8_backend\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"MSAMP\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1338\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_placement\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_msamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_placement\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice_placement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1339\u001b[0;31m             result = tuple(\n\u001b[0m\u001b[1;32m   1340\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_one\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfirst_pass\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_placement\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_placement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1341\u001b[0m             )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/accelerator.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1338\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_placement\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_msamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_placement\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice_placement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1339\u001b[0m             result = tuple(\n\u001b[0;32m-> 1340\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_one\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfirst_pass\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_placement\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_placement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1341\u001b[0m             )\n\u001b[1;32m   1342\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_one\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_placement\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_placement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/accelerator.py\u001b[0m in \u001b[0;36m_prepare_one\u001b[0;34m(self, obj, first_pass, device_placement)\u001b[0m\n\u001b[1;32m   1213\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_data_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_placement\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice_placement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1214\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1215\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_placement\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice_placement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1216\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1217\u001b[0m                 \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_optimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_placement\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice_placement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/accelerator.py\u001b[0m in \u001b[0;36mprepare_model\u001b[0;34m(self, model, device_placement, evaluation_mode)\u001b[0m\n\u001b[1;32m   1439\u001b[0m                     \u001b[0;31m# if on the first device (GPU 0) we don't care\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1440\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcurrent_device_index\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1441\u001b[0;31m                         raise ValueError(\n\u001b[0m\u001b[1;32m   1442\u001b[0m                             \u001b[0;34m\"You can't train a model that has been loaded in 8-bit or 4-bit precision on a different device than the one \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1443\u001b[0m                             \u001b[0;34m\"you're training on. Make sure you loaded the model on the correct device using for example `device_map={'':torch.cuda.current_device()}` or `device_map={'':torch.xpu.current_device()}`\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: You can't train a model that has been loaded in 8-bit or 4-bit precision on a different device than the one you're training on. Make sure you loaded the model on the correct device using for example `device_map={'':torch.cuda.current_device()}` or `device_map={'':torch.xpu.current_device()}`"],"ename":"ValueError","evalue":"You can't train a model that has been loaded in 8-bit or 4-bit precision on a different device than the one you're training on. Make sure you loaded the model on the correct device using for example `device_map={'':torch.cuda.current_device()}` or `device_map={'':torch.xpu.current_device()}`","output_type":"error"}],"execution_count":14},{"cell_type":"code","source":"from peft import LoraConfig, get_peft_model\n# 2) *Attach* LoRA adapters with PEFT\nlora_cfg = LoraConfig(\n    r             = 16,\n    lora_alpha    = 16,\n    target_modules= [\"gate_proj\", \"up_proj\", \"down_proj\"],\n    bias          = \"none\",\n    task_type     = \"CAUSAL_LM\",\n)\npeft_model = get_peft_model(model, lora_cfg)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T21:46:00.821312Z","iopub.execute_input":"2025-04-27T21:46:00.821682Z","iopub.status.idle":"2025-04-27T21:46:05.252930Z","shell.execute_reply.started":"2025-04-27T21:46:00.821654Z","shell.execute_reply":"2025-04-27T21:46:05.252119Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"from trl import GRPOConfig\n\ntraining_args = GRPOConfig(\n    use_vllm                    = False,      # HF path, not Unsloth/vLLM\n    learning_rate               = 5e-6,\n    per_device_train_batch_size = 4,          # MUST match num_generations\n    gradient_accumulation_steps = 4,\n    num_generations             = 2,\n    max_prompt_length           = 256,\n    max_completion_length       = 200,\n    max_steps                   = 5000,\n    save_steps                  = 250,\n    output_dir                  = \"outputs\",\n\n    # precision flags for a T4\n    bf16                        = False,\n    fp16                        = True,\n\n    # (other args you already had:)\n    weight_decay                = 0.1,\n    warmup_ratio                = 0.1,\n    lr_scheduler_type           = \"cosine\",\n    optim                       = \"paged_adamw_8bit\",\n    logging_steps               = 1,\n    max_grad_norm               = 0.1,\n    report_to                   = \"none\",\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T21:46:05.253695Z","iopub.execute_input":"2025-04-27T21:46:05.253946Z","iopub.status.idle":"2025-04-27T21:46:05.342284Z","shell.execute_reply.started":"2025-04-27T21:46:05.253917Z","shell.execute_reply":"2025-04-27T21:46:05.341769Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"from trl import GRPOTrainer\ntrainer = GRPOTrainer(\n    model = peft_model,\n    processing_class = tokenizer,\n    reward_funcs = [\n        xmlcount_reward_func,\n        soft_format_reward_func,\n        strict_format_reward_func,\n        int_reward_func,\n        correctness_reward_func,\n    ],\n    args = training_args,\n    train_dataset = dataset,\n)\ntrainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T21:46:05.342921Z","iopub.execute_input":"2025-04-27T21:46:05.343102Z","iopub.status.idle":"2025-04-28T04:16:43.274479Z","shell.execute_reply.started":"2025-04-27T21:46:05.343081Z","shell.execute_reply":"2025-04-28T04:16:43.273132Z"}},"outputs":[{"name":"stdout","text":"INFO 04-27 21:46:10 [__init__.py:239] Automatically detected platform cuda.\n","output_type":"stream"},{"name":"stderr","text":"No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2010' max='5000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2010/5000 6:30:16 < 9:41:08, 0.09 it/s, Epoch 1.12/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>-0.000000</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>11</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>12</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>13</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>14</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>15</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>16</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>17</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>18</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>19</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>21</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>22</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>23</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>24</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>25</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>26</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>27</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>28</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>29</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>31</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>32</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>33</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>34</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>35</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>36</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>37</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>38</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>39</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>41</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>42</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>43</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>44</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>45</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>46</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>47</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>48</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>49</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>51</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>52</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>53</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>54</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>55</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>56</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>57</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>58</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>59</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>61</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>62</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>63</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>64</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>65</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>66</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>67</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>68</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>69</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>70</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>71</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>72</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>73</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>74</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>75</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>76</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>77</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>78</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>79</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>81</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>82</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>83</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>84</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>85</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>86</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>87</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>88</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>89</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>90</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>91</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>92</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>93</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>94</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>95</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>96</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>97</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>98</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>99</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>101</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>102</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>103</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>104</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>105</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>106</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>107</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>108</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>109</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>110</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>111</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>112</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>113</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>114</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>115</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>116</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>117</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>118</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>119</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>120</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>121</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>122</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>123</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>124</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>125</td>\n      <td>0.000200</td>\n    </tr>\n    <tr>\n      <td>126</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>127</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>128</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>129</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>130</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>131</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>132</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>133</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>134</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>135</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>136</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>137</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>138</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>139</td>\n      <td>0.000200</td>\n    </tr>\n    <tr>\n      <td>140</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>141</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>142</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>143</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>144</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>145</td>\n      <td>0.000500</td>\n    </tr>\n    <tr>\n      <td>146</td>\n      <td>0.000200</td>\n    </tr>\n    <tr>\n      <td>147</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>148</td>\n      <td>0.000400</td>\n    </tr>\n    <tr>\n      <td>149</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>151</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>152</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>153</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>154</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>155</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>156</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>157</td>\n      <td>0.000200</td>\n    </tr>\n    <tr>\n      <td>158</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>159</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>160</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>161</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>162</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>163</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>164</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>165</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>166</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>167</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>168</td>\n      <td>0.000200</td>\n    </tr>\n    <tr>\n      <td>169</td>\n      <td>0.000400</td>\n    </tr>\n    <tr>\n      <td>170</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>171</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>172</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>173</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>174</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>175</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>176</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>177</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>178</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>179</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>180</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>181</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>182</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>183</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>184</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>185</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>186</td>\n      <td>0.000300</td>\n    </tr>\n    <tr>\n      <td>187</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>188</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>189</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>190</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>191</td>\n      <td>0.000400</td>\n    </tr>\n    <tr>\n      <td>192</td>\n      <td>0.000200</td>\n    </tr>\n    <tr>\n      <td>193</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>194</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>195</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>196</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>197</td>\n      <td>0.002800</td>\n    </tr>\n    <tr>\n      <td>198</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>199</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.000400</td>\n    </tr>\n    <tr>\n      <td>201</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>202</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>203</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>204</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>205</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>206</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>207</td>\n      <td>0.001000</td>\n    </tr>\n    <tr>\n      <td>208</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>209</td>\n      <td>0.000300</td>\n    </tr>\n    <tr>\n      <td>210</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>211</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>212</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>213</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>214</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>215</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>216</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>217</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>218</td>\n      <td>0.000200</td>\n    </tr>\n    <tr>\n      <td>219</td>\n      <td>0.001200</td>\n    </tr>\n    <tr>\n      <td>220</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>221</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>222</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>223</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>224</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>225</td>\n      <td>0.000300</td>\n    </tr>\n    <tr>\n      <td>226</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>227</td>\n      <td>0.000200</td>\n    </tr>\n    <tr>\n      <td>228</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>229</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>230</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>231</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>232</td>\n      <td>0.001100</td>\n    </tr>\n    <tr>\n      <td>233</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>234</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>235</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>236</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>237</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>238</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>239</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>240</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>241</td>\n      <td>0.000700</td>\n    </tr>\n    <tr>\n      <td>242</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>243</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>244</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>245</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>246</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>247</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>248</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>249</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>251</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>252</td>\n      <td>0.000200</td>\n    </tr>\n    <tr>\n      <td>253</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>254</td>\n      <td>0.000200</td>\n    </tr>\n    <tr>\n      <td>255</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>256</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>257</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>258</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>259</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>260</td>\n      <td>0.000400</td>\n    </tr>\n    <tr>\n      <td>261</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>262</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>263</td>\n      <td>0.000400</td>\n    </tr>\n    <tr>\n      <td>264</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>265</td>\n      <td>0.000500</td>\n    </tr>\n    <tr>\n      <td>266</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>267</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>268</td>\n      <td>0.000900</td>\n    </tr>\n    <tr>\n      <td>269</td>\n      <td>0.000600</td>\n    </tr>\n    <tr>\n      <td>270</td>\n      <td>0.000200</td>\n    </tr>\n    <tr>\n      <td>271</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>272</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>273</td>\n      <td>0.001100</td>\n    </tr>\n    <tr>\n      <td>274</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>275</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>276</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>277</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>278</td>\n      <td>0.000300</td>\n    </tr>\n    <tr>\n      <td>279</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>280</td>\n      <td>0.000800</td>\n    </tr>\n    <tr>\n      <td>281</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>282</td>\n      <td>0.000700</td>\n    </tr>\n    <tr>\n      <td>283</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>284</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>285</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>286</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>287</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>288</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>289</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>290</td>\n      <td>0.000600</td>\n    </tr>\n    <tr>\n      <td>291</td>\n      <td>0.000500</td>\n    </tr>\n    <tr>\n      <td>292</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>293</td>\n      <td>0.001200</td>\n    </tr>\n    <tr>\n      <td>294</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>295</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>296</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>297</td>\n      <td>0.001300</td>\n    </tr>\n    <tr>\n      <td>298</td>\n      <td>0.001500</td>\n    </tr>\n    <tr>\n      <td>299</td>\n      <td>0.002200</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>301</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>302</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>303</td>\n      <td>0.000800</td>\n    </tr>\n    <tr>\n      <td>304</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>305</td>\n      <td>0.003600</td>\n    </tr>\n    <tr>\n      <td>306</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>307</td>\n      <td>0.001000</td>\n    </tr>\n    <tr>\n      <td>308</td>\n      <td>0.000300</td>\n    </tr>\n    <tr>\n      <td>309</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>310</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>311</td>\n      <td>0.004900</td>\n    </tr>\n    <tr>\n      <td>312</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>313</td>\n      <td>0.002300</td>\n    </tr>\n    <tr>\n      <td>314</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>315</td>\n      <td>0.000300</td>\n    </tr>\n    <tr>\n      <td>316</td>\n      <td>0.001900</td>\n    </tr>\n    <tr>\n      <td>317</td>\n      <td>0.000700</td>\n    </tr>\n    <tr>\n      <td>318</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>319</td>\n      <td>0.000200</td>\n    </tr>\n    <tr>\n      <td>320</td>\n      <td>0.000800</td>\n    </tr>\n    <tr>\n      <td>321</td>\n      <td>0.001800</td>\n    </tr>\n    <tr>\n      <td>322</td>\n      <td>0.000300</td>\n    </tr>\n    <tr>\n      <td>323</td>\n      <td>0.000400</td>\n    </tr>\n    <tr>\n      <td>324</td>\n      <td>0.035500</td>\n    </tr>\n    <tr>\n      <td>325</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>326</td>\n      <td>0.000800</td>\n    </tr>\n    <tr>\n      <td>327</td>\n      <td>0.000700</td>\n    </tr>\n    <tr>\n      <td>328</td>\n      <td>0.001500</td>\n    </tr>\n    <tr>\n      <td>329</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>330</td>\n      <td>0.001000</td>\n    </tr>\n    <tr>\n      <td>331</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>332</td>\n      <td>0.000300</td>\n    </tr>\n    <tr>\n      <td>333</td>\n      <td>0.002900</td>\n    </tr>\n    <tr>\n      <td>334</td>\n      <td>0.001700</td>\n    </tr>\n    <tr>\n      <td>335</td>\n      <td>0.002200</td>\n    </tr>\n    <tr>\n      <td>336</td>\n      <td>0.000600</td>\n    </tr>\n    <tr>\n      <td>337</td>\n      <td>0.000500</td>\n    </tr>\n    <tr>\n      <td>338</td>\n      <td>0.001400</td>\n    </tr>\n    <tr>\n      <td>339</td>\n      <td>0.000500</td>\n    </tr>\n    <tr>\n      <td>340</td>\n      <td>0.001000</td>\n    </tr>\n    <tr>\n      <td>341</td>\n      <td>0.000700</td>\n    </tr>\n    <tr>\n      <td>342</td>\n      <td>0.000200</td>\n    </tr>\n    <tr>\n      <td>343</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>344</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>345</td>\n      <td>0.003400</td>\n    </tr>\n    <tr>\n      <td>346</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>347</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>348</td>\n      <td>0.003000</td>\n    </tr>\n    <tr>\n      <td>349</td>\n      <td>0.004300</td>\n    </tr>\n    <tr>\n      <td>350</td>\n      <td>0.001700</td>\n    </tr>\n    <tr>\n      <td>351</td>\n      <td>0.000300</td>\n    </tr>\n    <tr>\n      <td>352</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>353</td>\n      <td>0.001900</td>\n    </tr>\n    <tr>\n      <td>354</td>\n      <td>0.000200</td>\n    </tr>\n    <tr>\n      <td>355</td>\n      <td>0.001900</td>\n    </tr>\n    <tr>\n      <td>356</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>357</td>\n      <td>0.000200</td>\n    </tr>\n    <tr>\n      <td>358</td>\n      <td>0.002500</td>\n    </tr>\n    <tr>\n      <td>359</td>\n      <td>0.000300</td>\n    </tr>\n    <tr>\n      <td>360</td>\n      <td>0.001400</td>\n    </tr>\n    <tr>\n      <td>361</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>362</td>\n      <td>0.000500</td>\n    </tr>\n    <tr>\n      <td>363</td>\n      <td>0.000300</td>\n    </tr>\n    <tr>\n      <td>364</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>365</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>366</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>367</td>\n      <td>0.001000</td>\n    </tr>\n    <tr>\n      <td>368</td>\n      <td>0.000800</td>\n    </tr>\n    <tr>\n      <td>369</td>\n      <td>0.006700</td>\n    </tr>\n    <tr>\n      <td>370</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>371</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>372</td>\n      <td>0.002500</td>\n    </tr>\n    <tr>\n      <td>373</td>\n      <td>0.001000</td>\n    </tr>\n    <tr>\n      <td>374</td>\n      <td>0.001200</td>\n    </tr>\n    <tr>\n      <td>375</td>\n      <td>0.000200</td>\n    </tr>\n    <tr>\n      <td>376</td>\n      <td>0.002700</td>\n    </tr>\n    <tr>\n      <td>377</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>378</td>\n      <td>0.005800</td>\n    </tr>\n    <tr>\n      <td>379</td>\n      <td>0.000300</td>\n    </tr>\n    <tr>\n      <td>380</td>\n      <td>0.000300</td>\n    </tr>\n    <tr>\n      <td>381</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>382</td>\n      <td>0.001400</td>\n    </tr>\n    <tr>\n      <td>383</td>\n      <td>0.003800</td>\n    </tr>\n    <tr>\n      <td>384</td>\n      <td>0.002500</td>\n    </tr>\n    <tr>\n      <td>385</td>\n      <td>0.002800</td>\n    </tr>\n    <tr>\n      <td>386</td>\n      <td>0.003000</td>\n    </tr>\n    <tr>\n      <td>387</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>388</td>\n      <td>0.003500</td>\n    </tr>\n    <tr>\n      <td>389</td>\n      <td>0.000500</td>\n    </tr>\n    <tr>\n      <td>390</td>\n      <td>0.001500</td>\n    </tr>\n    <tr>\n      <td>391</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>392</td>\n      <td>0.000300</td>\n    </tr>\n    <tr>\n      <td>393</td>\n      <td>0.002500</td>\n    </tr>\n    <tr>\n      <td>394</td>\n      <td>0.000200</td>\n    </tr>\n    <tr>\n      <td>395</td>\n      <td>0.001500</td>\n    </tr>\n    <tr>\n      <td>396</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>397</td>\n      <td>0.000900</td>\n    </tr>\n    <tr>\n      <td>398</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>399</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.000200</td>\n    </tr>\n    <tr>\n      <td>401</td>\n      <td>0.000200</td>\n    </tr>\n    <tr>\n      <td>402</td>\n      <td>0.002200</td>\n    </tr>\n    <tr>\n      <td>403</td>\n      <td>0.002600</td>\n    </tr>\n    <tr>\n      <td>404</td>\n      <td>0.001000</td>\n    </tr>\n    <tr>\n      <td>405</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>406</td>\n      <td>0.000400</td>\n    </tr>\n    <tr>\n      <td>407</td>\n      <td>0.001100</td>\n    </tr>\n    <tr>\n      <td>408</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>409</td>\n      <td>0.002900</td>\n    </tr>\n    <tr>\n      <td>410</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>411</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>412</td>\n      <td>0.002200</td>\n    </tr>\n    <tr>\n      <td>413</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>414</td>\n      <td>0.000200</td>\n    </tr>\n    <tr>\n      <td>415</td>\n      <td>0.004900</td>\n    </tr>\n    <tr>\n      <td>416</td>\n      <td>0.001800</td>\n    </tr>\n    <tr>\n      <td>417</td>\n      <td>0.000600</td>\n    </tr>\n    <tr>\n      <td>418</td>\n      <td>0.000200</td>\n    </tr>\n    <tr>\n      <td>419</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>420</td>\n      <td>0.000300</td>\n    </tr>\n    <tr>\n      <td>421</td>\n      <td>0.000200</td>\n    </tr>\n    <tr>\n      <td>422</td>\n      <td>0.002500</td>\n    </tr>\n    <tr>\n      <td>423</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>424</td>\n      <td>0.003400</td>\n    </tr>\n    <tr>\n      <td>425</td>\n      <td>0.000900</td>\n    </tr>\n    <tr>\n      <td>426</td>\n      <td>0.000800</td>\n    </tr>\n    <tr>\n      <td>427</td>\n      <td>0.005400</td>\n    </tr>\n    <tr>\n      <td>428</td>\n      <td>0.004000</td>\n    </tr>\n    <tr>\n      <td>429</td>\n      <td>0.003700</td>\n    </tr>\n    <tr>\n      <td>430</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>431</td>\n      <td>0.004200</td>\n    </tr>\n    <tr>\n      <td>432</td>\n      <td>0.010400</td>\n    </tr>\n    <tr>\n      <td>433</td>\n      <td>0.005100</td>\n    </tr>\n    <tr>\n      <td>434</td>\n      <td>0.001100</td>\n    </tr>\n    <tr>\n      <td>435</td>\n      <td>0.000300</td>\n    </tr>\n    <tr>\n      <td>436</td>\n      <td>0.005700</td>\n    </tr>\n    <tr>\n      <td>437</td>\n      <td>0.006200</td>\n    </tr>\n    <tr>\n      <td>438</td>\n      <td>0.000200</td>\n    </tr>\n    <tr>\n      <td>439</td>\n      <td>0.000400</td>\n    </tr>\n    <tr>\n      <td>440</td>\n      <td>0.002500</td>\n    </tr>\n    <tr>\n      <td>441</td>\n      <td>0.001700</td>\n    </tr>\n    <tr>\n      <td>442</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>443</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>444</td>\n      <td>0.000200</td>\n    </tr>\n    <tr>\n      <td>445</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>446</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>447</td>\n      <td>0.000300</td>\n    </tr>\n    <tr>\n      <td>448</td>\n      <td>0.002100</td>\n    </tr>\n    <tr>\n      <td>449</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>450</td>\n      <td>0.000400</td>\n    </tr>\n    <tr>\n      <td>451</td>\n      <td>0.003500</td>\n    </tr>\n    <tr>\n      <td>452</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>453</td>\n      <td>0.000200</td>\n    </tr>\n    <tr>\n      <td>454</td>\n      <td>0.002700</td>\n    </tr>\n    <tr>\n      <td>455</td>\n      <td>0.004000</td>\n    </tr>\n    <tr>\n      <td>456</td>\n      <td>0.000400</td>\n    </tr>\n    <tr>\n      <td>457</td>\n      <td>0.000200</td>\n    </tr>\n    <tr>\n      <td>458</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>459</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>460</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>461</td>\n      <td>0.004500</td>\n    </tr>\n    <tr>\n      <td>462</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>463</td>\n      <td>0.002900</td>\n    </tr>\n    <tr>\n      <td>464</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>465</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>466</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>467</td>\n      <td>0.003000</td>\n    </tr>\n    <tr>\n      <td>468</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>469</td>\n      <td>0.003600</td>\n    </tr>\n    <tr>\n      <td>470</td>\n      <td>0.000400</td>\n    </tr>\n    <tr>\n      <td>471</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>472</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>473</td>\n      <td>0.001100</td>\n    </tr>\n    <tr>\n      <td>474</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>475</td>\n      <td>0.002100</td>\n    </tr>\n    <tr>\n      <td>476</td>\n      <td>0.003800</td>\n    </tr>\n    <tr>\n      <td>477</td>\n      <td>0.002600</td>\n    </tr>\n    <tr>\n      <td>478</td>\n      <td>0.003500</td>\n    </tr>\n    <tr>\n      <td>479</td>\n      <td>0.000200</td>\n    </tr>\n    <tr>\n      <td>480</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>481</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>482</td>\n      <td>0.000500</td>\n    </tr>\n    <tr>\n      <td>483</td>\n      <td>0.005400</td>\n    </tr>\n    <tr>\n      <td>484</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>485</td>\n      <td>0.002600</td>\n    </tr>\n    <tr>\n      <td>486</td>\n      <td>0.000700</td>\n    </tr>\n    <tr>\n      <td>487</td>\n      <td>0.002100</td>\n    </tr>\n    <tr>\n      <td>488</td>\n      <td>0.000800</td>\n    </tr>\n    <tr>\n      <td>489</td>\n      <td>0.000200</td>\n    </tr>\n    <tr>\n      <td>490</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>491</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>492</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>493</td>\n      <td>0.001300</td>\n    </tr>\n    <tr>\n      <td>494</td>\n      <td>0.000200</td>\n    </tr>\n    <tr>\n      <td>495</td>\n      <td>0.000200</td>\n    </tr>\n    <tr>\n      <td>496</td>\n      <td>0.000600</td>\n    </tr>\n    <tr>\n      <td>497</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>498</td>\n      <td>0.001600</td>\n    </tr>\n    <tr>\n      <td>499</td>\n      <td>0.000300</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>0.004300</td>\n    </tr>\n    <tr>\n      <td>501</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>502</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>503</td>\n      <td>0.000700</td>\n    </tr>\n    <tr>\n      <td>504</td>\n      <td>0.010700</td>\n    </tr>\n    <tr>\n      <td>505</td>\n      <td>0.000900</td>\n    </tr>\n    <tr>\n      <td>506</td>\n      <td>0.004200</td>\n    </tr>\n    <tr>\n      <td>507</td>\n      <td>0.001500</td>\n    </tr>\n    <tr>\n      <td>508</td>\n      <td>0.000500</td>\n    </tr>\n    <tr>\n      <td>509</td>\n      <td>0.002200</td>\n    </tr>\n    <tr>\n      <td>510</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>511</td>\n      <td>0.002300</td>\n    </tr>\n    <tr>\n      <td>512</td>\n      <td>0.000200</td>\n    </tr>\n    <tr>\n      <td>513</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>514</td>\n      <td>0.006500</td>\n    </tr>\n    <tr>\n      <td>515</td>\n      <td>0.013400</td>\n    </tr>\n    <tr>\n      <td>516</td>\n      <td>0.002100</td>\n    </tr>\n    <tr>\n      <td>517</td>\n      <td>0.006200</td>\n    </tr>\n    <tr>\n      <td>518</td>\n      <td>0.002200</td>\n    </tr>\n    <tr>\n      <td>519</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>520</td>\n      <td>0.000300</td>\n    </tr>\n    <tr>\n      <td>521</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>522</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>523</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>524</td>\n      <td>0.002200</td>\n    </tr>\n    <tr>\n      <td>525</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>526</td>\n      <td>0.006500</td>\n    </tr>\n    <tr>\n      <td>527</td>\n      <td>0.002300</td>\n    </tr>\n    <tr>\n      <td>528</td>\n      <td>0.001300</td>\n    </tr>\n    <tr>\n      <td>529</td>\n      <td>0.000400</td>\n    </tr>\n    <tr>\n      <td>530</td>\n      <td>0.003400</td>\n    </tr>\n    <tr>\n      <td>531</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>532</td>\n      <td>0.004500</td>\n    </tr>\n    <tr>\n      <td>533</td>\n      <td>0.000300</td>\n    </tr>\n    <tr>\n      <td>534</td>\n      <td>0.002000</td>\n    </tr>\n    <tr>\n      <td>535</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>536</td>\n      <td>0.000300</td>\n    </tr>\n    <tr>\n      <td>537</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>538</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>539</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>540</td>\n      <td>0.000500</td>\n    </tr>\n    <tr>\n      <td>541</td>\n      <td>0.002600</td>\n    </tr>\n    <tr>\n      <td>542</td>\n      <td>0.000800</td>\n    </tr>\n    <tr>\n      <td>543</td>\n      <td>0.002400</td>\n    </tr>\n    <tr>\n      <td>544</td>\n      <td>0.006600</td>\n    </tr>\n    <tr>\n      <td>545</td>\n      <td>0.005600</td>\n    </tr>\n    <tr>\n      <td>546</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>547</td>\n      <td>0.001500</td>\n    </tr>\n    <tr>\n      <td>548</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>549</td>\n      <td>0.005000</td>\n    </tr>\n    <tr>\n      <td>550</td>\n      <td>0.000200</td>\n    </tr>\n    <tr>\n      <td>551</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>552</td>\n      <td>0.008000</td>\n    </tr>\n    <tr>\n      <td>553</td>\n      <td>0.002500</td>\n    </tr>\n    <tr>\n      <td>554</td>\n      <td>0.001600</td>\n    </tr>\n    <tr>\n      <td>555</td>\n      <td>0.005000</td>\n    </tr>\n    <tr>\n      <td>556</td>\n      <td>0.000200</td>\n    </tr>\n    <tr>\n      <td>557</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>558</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>559</td>\n      <td>0.003900</td>\n    </tr>\n    <tr>\n      <td>560</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>561</td>\n      <td>0.004200</td>\n    </tr>\n    <tr>\n      <td>562</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>563</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>564</td>\n      <td>0.111500</td>\n    </tr>\n    <tr>\n      <td>565</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>566</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>567</td>\n      <td>0.001300</td>\n    </tr>\n    <tr>\n      <td>568</td>\n      <td>0.001500</td>\n    </tr>\n    <tr>\n      <td>569</td>\n      <td>0.002400</td>\n    </tr>\n    <tr>\n      <td>570</td>\n      <td>0.002500</td>\n    </tr>\n    <tr>\n      <td>571</td>\n      <td>0.001000</td>\n    </tr>\n    <tr>\n      <td>572</td>\n      <td>0.000900</td>\n    </tr>\n    <tr>\n      <td>573</td>\n      <td>0.000200</td>\n    </tr>\n    <tr>\n      <td>574</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>575</td>\n      <td>0.003500</td>\n    </tr>\n    <tr>\n      <td>576</td>\n      <td>0.000200</td>\n    </tr>\n    <tr>\n      <td>577</td>\n      <td>0.001500</td>\n    </tr>\n    <tr>\n      <td>578</td>\n      <td>0.001100</td>\n    </tr>\n    <tr>\n      <td>579</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>580</td>\n      <td>0.002200</td>\n    </tr>\n    <tr>\n      <td>581</td>\n      <td>0.005600</td>\n    </tr>\n    <tr>\n      <td>582</td>\n      <td>0.002400</td>\n    </tr>\n    <tr>\n      <td>583</td>\n      <td>0.002100</td>\n    </tr>\n    <tr>\n      <td>584</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>585</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>586</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>587</td>\n      <td>0.002000</td>\n    </tr>\n    <tr>\n      <td>588</td>\n      <td>0.003300</td>\n    </tr>\n    <tr>\n      <td>589</td>\n      <td>0.003000</td>\n    </tr>\n    <tr>\n      <td>590</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>591</td>\n      <td>0.000400</td>\n    </tr>\n    <tr>\n      <td>592</td>\n      <td>0.000200</td>\n    </tr>\n    <tr>\n      <td>593</td>\n      <td>0.000700</td>\n    </tr>\n    <tr>\n      <td>594</td>\n      <td>0.001300</td>\n    </tr>\n    <tr>\n      <td>595</td>\n      <td>0.000300</td>\n    </tr>\n    <tr>\n      <td>596</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>597</td>\n      <td>0.001400</td>\n    </tr>\n    <tr>\n      <td>598</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>599</td>\n      <td>0.001300</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.002000</td>\n    </tr>\n    <tr>\n      <td>601</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>602</td>\n      <td>0.000500</td>\n    </tr>\n    <tr>\n      <td>603</td>\n      <td>0.000200</td>\n    </tr>\n    <tr>\n      <td>604</td>\n      <td>0.000200</td>\n    </tr>\n    <tr>\n      <td>605</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>606</td>\n      <td>0.000700</td>\n    </tr>\n    <tr>\n      <td>607</td>\n      <td>0.000300</td>\n    </tr>\n    <tr>\n      <td>608</td>\n      <td>0.005800</td>\n    </tr>\n    <tr>\n      <td>609</td>\n      <td>0.000500</td>\n    </tr>\n    <tr>\n      <td>610</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>611</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>612</td>\n      <td>0.003200</td>\n    </tr>\n    <tr>\n      <td>613</td>\n      <td>0.000400</td>\n    </tr>\n    <tr>\n      <td>614</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>615</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>616</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>617</td>\n      <td>0.001600</td>\n    </tr>\n    <tr>\n      <td>618</td>\n      <td>0.000400</td>\n    </tr>\n    <tr>\n      <td>619</td>\n      <td>0.000200</td>\n    </tr>\n    <tr>\n      <td>620</td>\n      <td>0.000600</td>\n    </tr>\n    <tr>\n      <td>621</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>622</td>\n      <td>0.001300</td>\n    </tr>\n    <tr>\n      <td>623</td>\n      <td>0.000800</td>\n    </tr>\n    <tr>\n      <td>624</td>\n      <td>0.004200</td>\n    </tr>\n    <tr>\n      <td>625</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>626</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>627</td>\n      <td>0.001500</td>\n    </tr>\n    <tr>\n      <td>628</td>\n      <td>0.000200</td>\n    </tr>\n    <tr>\n      <td>629</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>630</td>\n      <td>0.002100</td>\n    </tr>\n    <tr>\n      <td>631</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>632</td>\n      <td>0.004700</td>\n    </tr>\n    <tr>\n      <td>633</td>\n      <td>0.001500</td>\n    </tr>\n    <tr>\n      <td>634</td>\n      <td>0.000400</td>\n    </tr>\n    <tr>\n      <td>635</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>636</td>\n      <td>0.000700</td>\n    </tr>\n    <tr>\n      <td>637</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>638</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>639</td>\n      <td>0.003200</td>\n    </tr>\n    <tr>\n      <td>640</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>641</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>642</td>\n      <td>0.000400</td>\n    </tr>\n    <tr>\n      <td>643</td>\n      <td>0.002100</td>\n    </tr>\n    <tr>\n      <td>644</td>\n      <td>0.003700</td>\n    </tr>\n    <tr>\n      <td>645</td>\n      <td>0.000900</td>\n    </tr>\n    <tr>\n      <td>646</td>\n      <td>0.002400</td>\n    </tr>\n    <tr>\n      <td>647</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>648</td>\n      <td>0.001600</td>\n    </tr>\n    <tr>\n      <td>649</td>\n      <td>0.004300</td>\n    </tr>\n    <tr>\n      <td>650</td>\n      <td>0.001500</td>\n    </tr>\n    <tr>\n      <td>651</td>\n      <td>0.002700</td>\n    </tr>\n    <tr>\n      <td>652</td>\n      <td>0.004500</td>\n    </tr>\n    <tr>\n      <td>653</td>\n      <td>0.000200</td>\n    </tr>\n    <tr>\n      <td>654</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>655</td>\n      <td>0.001600</td>\n    </tr>\n    <tr>\n      <td>656</td>\n      <td>0.000200</td>\n    </tr>\n    <tr>\n      <td>657</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>658</td>\n      <td>0.000300</td>\n    </tr>\n    <tr>\n      <td>659</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>660</td>\n      <td>0.001800</td>\n    </tr>\n    <tr>\n      <td>661</td>\n      <td>0.001600</td>\n    </tr>\n    <tr>\n      <td>662</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>663</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>664</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>665</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>666</td>\n      <td>0.007800</td>\n    </tr>\n    <tr>\n      <td>667</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>668</td>\n      <td>0.000800</td>\n    </tr>\n    <tr>\n      <td>669</td>\n      <td>0.002800</td>\n    </tr>\n    <tr>\n      <td>670</td>\n      <td>0.002200</td>\n    </tr>\n    <tr>\n      <td>671</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>672</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>673</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>674</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>675</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>676</td>\n      <td>0.000800</td>\n    </tr>\n    <tr>\n      <td>677</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>678</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>679</td>\n      <td>0.001900</td>\n    </tr>\n    <tr>\n      <td>680</td>\n      <td>0.002500</td>\n    </tr>\n    <tr>\n      <td>681</td>\n      <td>0.000300</td>\n    </tr>\n    <tr>\n      <td>682</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>683</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>684</td>\n      <td>0.001600</td>\n    </tr>\n    <tr>\n      <td>685</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>686</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>687</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>688</td>\n      <td>0.001100</td>\n    </tr>\n    <tr>\n      <td>689</td>\n      <td>0.000800</td>\n    </tr>\n    <tr>\n      <td>690</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>691</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>692</td>\n      <td>0.002200</td>\n    </tr>\n    <tr>\n      <td>693</td>\n      <td>0.000200</td>\n    </tr>\n    <tr>\n      <td>694</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>695</td>\n      <td>0.004100</td>\n    </tr>\n    <tr>\n      <td>696</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>697</td>\n      <td>0.001700</td>\n    </tr>\n    <tr>\n      <td>698</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>699</td>\n      <td>0.000300</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>701</td>\n      <td>0.000200</td>\n    </tr>\n    <tr>\n      <td>702</td>\n      <td>0.000200</td>\n    </tr>\n    <tr>\n      <td>703</td>\n      <td>0.007900</td>\n    </tr>\n    <tr>\n      <td>704</td>\n      <td>0.001600</td>\n    </tr>\n    <tr>\n      <td>705</td>\n      <td>0.000200</td>\n    </tr>\n    <tr>\n      <td>706</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>707</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>708</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>709</td>\n      <td>0.000500</td>\n    </tr>\n    <tr>\n      <td>710</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>711</td>\n      <td>0.002100</td>\n    </tr>\n    <tr>\n      <td>712</td>\n      <td>0.005600</td>\n    </tr>\n    <tr>\n      <td>713</td>\n      <td>0.000600</td>\n    </tr>\n    <tr>\n      <td>714</td>\n      <td>0.000300</td>\n    </tr>\n    <tr>\n      <td>715</td>\n      <td>0.001800</td>\n    </tr>\n    <tr>\n      <td>716</td>\n      <td>0.001600</td>\n    </tr>\n    <tr>\n      <td>717</td>\n      <td>0.001400</td>\n    </tr>\n    <tr>\n      <td>718</td>\n      <td>0.003700</td>\n    </tr>\n    <tr>\n      <td>719</td>\n      <td>0.000200</td>\n    </tr>\n    <tr>\n      <td>720</td>\n      <td>0.003400</td>\n    </tr>\n    <tr>\n      <td>721</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>722</td>\n      <td>0.001100</td>\n    </tr>\n    <tr>\n      <td>723</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>724</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>725</td>\n      <td>0.001000</td>\n    </tr>\n    <tr>\n      <td>726</td>\n      <td>0.000900</td>\n    </tr>\n    <tr>\n      <td>727</td>\n      <td>0.004300</td>\n    </tr>\n    <tr>\n      <td>728</td>\n      <td>0.000400</td>\n    </tr>\n    <tr>\n      <td>729</td>\n      <td>0.002800</td>\n    </tr>\n    <tr>\n      <td>730</td>\n      <td>0.000200</td>\n    </tr>\n    <tr>\n      <td>731</td>\n      <td>0.001000</td>\n    </tr>\n    <tr>\n      <td>732</td>\n      <td>0.002300</td>\n    </tr>\n    <tr>\n      <td>733</td>\n      <td>0.001600</td>\n    </tr>\n    <tr>\n      <td>734</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>735</td>\n      <td>0.000200</td>\n    </tr>\n    <tr>\n      <td>736</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>737</td>\n      <td>0.009800</td>\n    </tr>\n    <tr>\n      <td>738</td>\n      <td>0.000200</td>\n    </tr>\n    <tr>\n      <td>739</td>\n      <td>0.001500</td>\n    </tr>\n    <tr>\n      <td>740</td>\n      <td>0.000500</td>\n    </tr>\n    <tr>\n      <td>741</td>\n      <td>0.002300</td>\n    </tr>\n    <tr>\n      <td>742</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>743</td>\n      <td>0.000200</td>\n    </tr>\n    <tr>\n      <td>744</td>\n      <td>0.001600</td>\n    </tr>\n    <tr>\n      <td>745</td>\n      <td>0.000300</td>\n    </tr>\n    <tr>\n      <td>746</td>\n      <td>0.006400</td>\n    </tr>\n    <tr>\n      <td>747</td>\n      <td>0.001600</td>\n    </tr>\n    <tr>\n      <td>748</td>\n      <td>0.000400</td>\n    </tr>\n    <tr>\n      <td>749</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>750</td>\n      <td>0.000400</td>\n    </tr>\n    <tr>\n      <td>751</td>\n      <td>0.003800</td>\n    </tr>\n    <tr>\n      <td>752</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>753</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>754</td>\n      <td>0.000200</td>\n    </tr>\n    <tr>\n      <td>755</td>\n      <td>0.001100</td>\n    </tr>\n    <tr>\n      <td>756</td>\n      <td>0.008600</td>\n    </tr>\n    <tr>\n      <td>757</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>758</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>759</td>\n      <td>0.002100</td>\n    </tr>\n    <tr>\n      <td>760</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>761</td>\n      <td>0.000800</td>\n    </tr>\n    <tr>\n      <td>762</td>\n      <td>0.005100</td>\n    </tr>\n    <tr>\n      <td>763</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>764</td>\n      <td>0.003100</td>\n    </tr>\n    <tr>\n      <td>765</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>766</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>767</td>\n      <td>0.003300</td>\n    </tr>\n    <tr>\n      <td>768</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>769</td>\n      <td>0.002800</td>\n    </tr>\n    <tr>\n      <td>770</td>\n      <td>0.000200</td>\n    </tr>\n    <tr>\n      <td>771</td>\n      <td>0.000500</td>\n    </tr>\n    <tr>\n      <td>772</td>\n      <td>0.001900</td>\n    </tr>\n    <tr>\n      <td>773</td>\n      <td>0.003300</td>\n    </tr>\n    <tr>\n      <td>774</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>775</td>\n      <td>0.013400</td>\n    </tr>\n    <tr>\n      <td>776</td>\n      <td>0.001000</td>\n    </tr>\n    <tr>\n      <td>777</td>\n      <td>0.003800</td>\n    </tr>\n    <tr>\n      <td>778</td>\n      <td>0.005400</td>\n    </tr>\n    <tr>\n      <td>779</td>\n      <td>0.000200</td>\n    </tr>\n    <tr>\n      <td>780</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>781</td>\n      <td>0.002000</td>\n    </tr>\n    <tr>\n      <td>782</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>783</td>\n      <td>0.001900</td>\n    </tr>\n    <tr>\n      <td>784</td>\n      <td>0.000200</td>\n    </tr>\n    <tr>\n      <td>785</td>\n      <td>0.002600</td>\n    </tr>\n    <tr>\n      <td>786</td>\n      <td>0.003300</td>\n    </tr>\n    <tr>\n      <td>787</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>788</td>\n      <td>0.000200</td>\n    </tr>\n    <tr>\n      <td>789</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>790</td>\n      <td>0.004600</td>\n    </tr>\n    <tr>\n      <td>791</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>792</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>793</td>\n      <td>0.004300</td>\n    </tr>\n    <tr>\n      <td>794</td>\n      <td>0.003600</td>\n    </tr>\n    <tr>\n      <td>795</td>\n      <td>0.004300</td>\n    </tr>\n    <tr>\n      <td>796</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>797</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>798</td>\n      <td>0.003000</td>\n    </tr>\n    <tr>\n      <td>799</td>\n      <td>0.001800</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>801</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>802</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>803</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>804</td>\n      <td>0.002500</td>\n    </tr>\n    <tr>\n      <td>805</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>806</td>\n      <td>0.000400</td>\n    </tr>\n    <tr>\n      <td>807</td>\n      <td>0.000300</td>\n    </tr>\n    <tr>\n      <td>808</td>\n      <td>0.004000</td>\n    </tr>\n    <tr>\n      <td>809</td>\n      <td>0.001000</td>\n    </tr>\n    <tr>\n      <td>810</td>\n      <td>0.003700</td>\n    </tr>\n    <tr>\n      <td>811</td>\n      <td>0.001700</td>\n    </tr>\n    <tr>\n      <td>812</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>813</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>814</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>815</td>\n      <td>0.033000</td>\n    </tr>\n    <tr>\n      <td>816</td>\n      <td>0.002400</td>\n    </tr>\n    <tr>\n      <td>817</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>818</td>\n      <td>0.000600</td>\n    </tr>\n    <tr>\n      <td>819</td>\n      <td>0.004400</td>\n    </tr>\n    <tr>\n      <td>820</td>\n      <td>0.001800</td>\n    </tr>\n    <tr>\n      <td>821</td>\n      <td>0.004300</td>\n    </tr>\n    <tr>\n      <td>822</td>\n      <td>0.010300</td>\n    </tr>\n    <tr>\n      <td>823</td>\n      <td>0.000700</td>\n    </tr>\n    <tr>\n      <td>824</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>825</td>\n      <td>0.006200</td>\n    </tr>\n    <tr>\n      <td>826</td>\n      <td>0.004000</td>\n    </tr>\n    <tr>\n      <td>827</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>828</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>829</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>830</td>\n      <td>0.007200</td>\n    </tr>\n    <tr>\n      <td>831</td>\n      <td>0.000500</td>\n    </tr>\n    <tr>\n      <td>832</td>\n      <td>0.000800</td>\n    </tr>\n    <tr>\n      <td>833</td>\n      <td>0.000300</td>\n    </tr>\n    <tr>\n      <td>834</td>\n      <td>0.001200</td>\n    </tr>\n    <tr>\n      <td>835</td>\n      <td>0.002000</td>\n    </tr>\n    <tr>\n      <td>836</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>837</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>838</td>\n      <td>0.004500</td>\n    </tr>\n    <tr>\n      <td>839</td>\n      <td>0.002500</td>\n    </tr>\n    <tr>\n      <td>840</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>841</td>\n      <td>0.002600</td>\n    </tr>\n    <tr>\n      <td>842</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>843</td>\n      <td>0.002300</td>\n    </tr>\n    <tr>\n      <td>844</td>\n      <td>0.004400</td>\n    </tr>\n    <tr>\n      <td>845</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>846</td>\n      <td>0.003100</td>\n    </tr>\n    <tr>\n      <td>847</td>\n      <td>0.002600</td>\n    </tr>\n    <tr>\n      <td>848</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>849</td>\n      <td>0.002100</td>\n    </tr>\n    <tr>\n      <td>850</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>851</td>\n      <td>0.003300</td>\n    </tr>\n    <tr>\n      <td>852</td>\n      <td>0.006800</td>\n    </tr>\n    <tr>\n      <td>853</td>\n      <td>0.000600</td>\n    </tr>\n    <tr>\n      <td>854</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>855</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>856</td>\n      <td>0.000800</td>\n    </tr>\n    <tr>\n      <td>857</td>\n      <td>0.001800</td>\n    </tr>\n    <tr>\n      <td>858</td>\n      <td>0.018800</td>\n    </tr>\n    <tr>\n      <td>859</td>\n      <td>0.001500</td>\n    </tr>\n    <tr>\n      <td>860</td>\n      <td>0.004100</td>\n    </tr>\n    <tr>\n      <td>861</td>\n      <td>0.001400</td>\n    </tr>\n    <tr>\n      <td>862</td>\n      <td>0.003400</td>\n    </tr>\n    <tr>\n      <td>863</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>864</td>\n      <td>0.007300</td>\n    </tr>\n    <tr>\n      <td>865</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>866</td>\n      <td>0.000700</td>\n    </tr>\n    <tr>\n      <td>867</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>868</td>\n      <td>0.000400</td>\n    </tr>\n    <tr>\n      <td>869</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>870</td>\n      <td>0.003500</td>\n    </tr>\n    <tr>\n      <td>871</td>\n      <td>0.003500</td>\n    </tr>\n    <tr>\n      <td>872</td>\n      <td>0.000500</td>\n    </tr>\n    <tr>\n      <td>873</td>\n      <td>0.001600</td>\n    </tr>\n    <tr>\n      <td>874</td>\n      <td>0.000300</td>\n    </tr>\n    <tr>\n      <td>875</td>\n      <td>0.003100</td>\n    </tr>\n    <tr>\n      <td>876</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>877</td>\n      <td>0.004700</td>\n    </tr>\n    <tr>\n      <td>878</td>\n      <td>0.001600</td>\n    </tr>\n    <tr>\n      <td>879</td>\n      <td>0.007400</td>\n    </tr>\n    <tr>\n      <td>880</td>\n      <td>0.003100</td>\n    </tr>\n    <tr>\n      <td>881</td>\n      <td>0.001400</td>\n    </tr>\n    <tr>\n      <td>882</td>\n      <td>0.001200</td>\n    </tr>\n    <tr>\n      <td>883</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>884</td>\n      <td>0.000500</td>\n    </tr>\n    <tr>\n      <td>885</td>\n      <td>0.000800</td>\n    </tr>\n    <tr>\n      <td>886</td>\n      <td>0.000500</td>\n    </tr>\n    <tr>\n      <td>887</td>\n      <td>0.001900</td>\n    </tr>\n    <tr>\n      <td>888</td>\n      <td>0.001900</td>\n    </tr>\n    <tr>\n      <td>889</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>890</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>891</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>892</td>\n      <td>0.001000</td>\n    </tr>\n    <tr>\n      <td>893</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>894</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>895</td>\n      <td>0.002700</td>\n    </tr>\n    <tr>\n      <td>896</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>897</td>\n      <td>0.010800</td>\n    </tr>\n    <tr>\n      <td>898</td>\n      <td>0.008200</td>\n    </tr>\n    <tr>\n      <td>899</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>0.002100</td>\n    </tr>\n    <tr>\n      <td>901</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>902</td>\n      <td>0.000200</td>\n    </tr>\n    <tr>\n      <td>903</td>\n      <td>0.002300</td>\n    </tr>\n    <tr>\n      <td>904</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>905</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>906</td>\n      <td>0.002900</td>\n    </tr>\n    <tr>\n      <td>907</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>908</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>909</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>910</td>\n      <td>0.000700</td>\n    </tr>\n    <tr>\n      <td>911</td>\n      <td>0.001600</td>\n    </tr>\n    <tr>\n      <td>912</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>913</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>914</td>\n      <td>0.001900</td>\n    </tr>\n    <tr>\n      <td>915</td>\n      <td>0.004900</td>\n    </tr>\n    <tr>\n      <td>916</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>917</td>\n      <td>0.000200</td>\n    </tr>\n    <tr>\n      <td>918</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>919</td>\n      <td>0.010700</td>\n    </tr>\n    <tr>\n      <td>920</td>\n      <td>0.004300</td>\n    </tr>\n    <tr>\n      <td>921</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>922</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>923</td>\n      <td>0.001500</td>\n    </tr>\n    <tr>\n      <td>924</td>\n      <td>0.001700</td>\n    </tr>\n    <tr>\n      <td>925</td>\n      <td>0.004300</td>\n    </tr>\n    <tr>\n      <td>926</td>\n      <td>0.000200</td>\n    </tr>\n    <tr>\n      <td>927</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>928</td>\n      <td>0.000600</td>\n    </tr>\n    <tr>\n      <td>929</td>\n      <td>0.000700</td>\n    </tr>\n    <tr>\n      <td>930</td>\n      <td>0.003900</td>\n    </tr>\n    <tr>\n      <td>931</td>\n      <td>0.001200</td>\n    </tr>\n    <tr>\n      <td>932</td>\n      <td>0.003600</td>\n    </tr>\n    <tr>\n      <td>933</td>\n      <td>0.001600</td>\n    </tr>\n    <tr>\n      <td>934</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>935</td>\n      <td>0.010500</td>\n    </tr>\n    <tr>\n      <td>936</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>937</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>938</td>\n      <td>0.000700</td>\n    </tr>\n    <tr>\n      <td>939</td>\n      <td>0.001500</td>\n    </tr>\n    <tr>\n      <td>940</td>\n      <td>0.002300</td>\n    </tr>\n    <tr>\n      <td>941</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>942</td>\n      <td>0.002500</td>\n    </tr>\n    <tr>\n      <td>943</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>944</td>\n      <td>0.001000</td>\n    </tr>\n    <tr>\n      <td>945</td>\n      <td>0.006400</td>\n    </tr>\n    <tr>\n      <td>946</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>947</td>\n      <td>0.008500</td>\n    </tr>\n    <tr>\n      <td>948</td>\n      <td>0.001400</td>\n    </tr>\n    <tr>\n      <td>949</td>\n      <td>0.001800</td>\n    </tr>\n    <tr>\n      <td>950</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>951</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>952</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>953</td>\n      <td>0.001800</td>\n    </tr>\n    <tr>\n      <td>954</td>\n      <td>0.004600</td>\n    </tr>\n    <tr>\n      <td>955</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>956</td>\n      <td>0.001500</td>\n    </tr>\n    <tr>\n      <td>957</td>\n      <td>0.003800</td>\n    </tr>\n    <tr>\n      <td>958</td>\n      <td>0.002600</td>\n    </tr>\n    <tr>\n      <td>959</td>\n      <td>0.000500</td>\n    </tr>\n    <tr>\n      <td>960</td>\n      <td>0.000400</td>\n    </tr>\n    <tr>\n      <td>961</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>962</td>\n      <td>0.000300</td>\n    </tr>\n    <tr>\n      <td>963</td>\n      <td>0.000700</td>\n    </tr>\n    <tr>\n      <td>964</td>\n      <td>0.006800</td>\n    </tr>\n    <tr>\n      <td>965</td>\n      <td>0.001300</td>\n    </tr>\n    <tr>\n      <td>966</td>\n      <td>0.003000</td>\n    </tr>\n    <tr>\n      <td>967</td>\n      <td>0.002700</td>\n    </tr>\n    <tr>\n      <td>968</td>\n      <td>0.005800</td>\n    </tr>\n    <tr>\n      <td>969</td>\n      <td>0.003900</td>\n    </tr>\n    <tr>\n      <td>970</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>971</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>972</td>\n      <td>0.005700</td>\n    </tr>\n    <tr>\n      <td>973</td>\n      <td>0.000200</td>\n    </tr>\n    <tr>\n      <td>974</td>\n      <td>0.005300</td>\n    </tr>\n    <tr>\n      <td>975</td>\n      <td>0.001900</td>\n    </tr>\n    <tr>\n      <td>976</td>\n      <td>0.005700</td>\n    </tr>\n    <tr>\n      <td>977</td>\n      <td>0.004000</td>\n    </tr>\n    <tr>\n      <td>978</td>\n      <td>0.008600</td>\n    </tr>\n    <tr>\n      <td>979</td>\n      <td>0.000200</td>\n    </tr>\n    <tr>\n      <td>980</td>\n      <td>0.004600</td>\n    </tr>\n    <tr>\n      <td>981</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>982</td>\n      <td>0.000200</td>\n    </tr>\n    <tr>\n      <td>983</td>\n      <td>0.003700</td>\n    </tr>\n    <tr>\n      <td>984</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>985</td>\n      <td>0.000500</td>\n    </tr>\n    <tr>\n      <td>986</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>987</td>\n      <td>0.007700</td>\n    </tr>\n    <tr>\n      <td>988</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>989</td>\n      <td>0.000600</td>\n    </tr>\n    <tr>\n      <td>990</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>991</td>\n      <td>0.000600</td>\n    </tr>\n    <tr>\n      <td>992</td>\n      <td>0.000200</td>\n    </tr>\n    <tr>\n      <td>993</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>994</td>\n      <td>0.001600</td>\n    </tr>\n    <tr>\n      <td>995</td>\n      <td>0.004100</td>\n    </tr>\n    <tr>\n      <td>996</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>997</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>998</td>\n      <td>0.000900</td>\n    </tr>\n    <tr>\n      <td>999</td>\n      <td>0.002100</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.000200</td>\n    </tr>\n    <tr>\n      <td>1001</td>\n      <td>0.000700</td>\n    </tr>\n    <tr>\n      <td>1002</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1003</td>\n      <td>0.001300</td>\n    </tr>\n    <tr>\n      <td>1004</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>1005</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1006</td>\n      <td>0.000300</td>\n    </tr>\n    <tr>\n      <td>1007</td>\n      <td>0.000700</td>\n    </tr>\n    <tr>\n      <td>1008</td>\n      <td>0.003300</td>\n    </tr>\n    <tr>\n      <td>1009</td>\n      <td>0.002900</td>\n    </tr>\n    <tr>\n      <td>1010</td>\n      <td>0.003000</td>\n    </tr>\n    <tr>\n      <td>1011</td>\n      <td>0.000800</td>\n    </tr>\n    <tr>\n      <td>1012</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>1013</td>\n      <td>0.000500</td>\n    </tr>\n    <tr>\n      <td>1014</td>\n      <td>0.002000</td>\n    </tr>\n    <tr>\n      <td>1015</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>1016</td>\n      <td>0.000500</td>\n    </tr>\n    <tr>\n      <td>1017</td>\n      <td>0.000400</td>\n    </tr>\n    <tr>\n      <td>1018</td>\n      <td>0.000700</td>\n    </tr>\n    <tr>\n      <td>1019</td>\n      <td>0.005000</td>\n    </tr>\n    <tr>\n      <td>1020</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1021</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1022</td>\n      <td>0.008000</td>\n    </tr>\n    <tr>\n      <td>1023</td>\n      <td>0.001100</td>\n    </tr>\n    <tr>\n      <td>1024</td>\n      <td>0.006300</td>\n    </tr>\n    <tr>\n      <td>1025</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>1026</td>\n      <td>0.002200</td>\n    </tr>\n    <tr>\n      <td>1027</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1028</td>\n      <td>0.005600</td>\n    </tr>\n    <tr>\n      <td>1029</td>\n      <td>0.002600</td>\n    </tr>\n    <tr>\n      <td>1030</td>\n      <td>0.002600</td>\n    </tr>\n    <tr>\n      <td>1031</td>\n      <td>0.002900</td>\n    </tr>\n    <tr>\n      <td>1032</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1033</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1034</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1035</td>\n      <td>0.000200</td>\n    </tr>\n    <tr>\n      <td>1036</td>\n      <td>0.002000</td>\n    </tr>\n    <tr>\n      <td>1037</td>\n      <td>0.000700</td>\n    </tr>\n    <tr>\n      <td>1038</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1039</td>\n      <td>0.000200</td>\n    </tr>\n    <tr>\n      <td>1040</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1041</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1042</td>\n      <td>0.001400</td>\n    </tr>\n    <tr>\n      <td>1043</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>1044</td>\n      <td>0.003400</td>\n    </tr>\n    <tr>\n      <td>1045</td>\n      <td>0.008500</td>\n    </tr>\n    <tr>\n      <td>1046</td>\n      <td>0.004200</td>\n    </tr>\n    <tr>\n      <td>1047</td>\n      <td>0.002900</td>\n    </tr>\n    <tr>\n      <td>1048</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>1049</td>\n      <td>0.000400</td>\n    </tr>\n    <tr>\n      <td>1050</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1051</td>\n      <td>0.000300</td>\n    </tr>\n    <tr>\n      <td>1052</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1053</td>\n      <td>0.003000</td>\n    </tr>\n    <tr>\n      <td>1054</td>\n      <td>0.006600</td>\n    </tr>\n    <tr>\n      <td>1055</td>\n      <td>0.002400</td>\n    </tr>\n    <tr>\n      <td>1056</td>\n      <td>0.002900</td>\n    </tr>\n    <tr>\n      <td>1057</td>\n      <td>0.002800</td>\n    </tr>\n    <tr>\n      <td>1058</td>\n      <td>0.000200</td>\n    </tr>\n    <tr>\n      <td>1059</td>\n      <td>0.000600</td>\n    </tr>\n    <tr>\n      <td>1060</td>\n      <td>0.000200</td>\n    </tr>\n    <tr>\n      <td>1061</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>1062</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>1063</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>1064</td>\n      <td>0.001000</td>\n    </tr>\n    <tr>\n      <td>1065</td>\n      <td>0.006400</td>\n    </tr>\n    <tr>\n      <td>1066</td>\n      <td>0.002300</td>\n    </tr>\n    <tr>\n      <td>1067</td>\n      <td>0.001500</td>\n    </tr>\n    <tr>\n      <td>1068</td>\n      <td>0.007500</td>\n    </tr>\n    <tr>\n      <td>1069</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>1070</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>1071</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1072</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>1073</td>\n      <td>0.000200</td>\n    </tr>\n    <tr>\n      <td>1074</td>\n      <td>0.011400</td>\n    </tr>\n    <tr>\n      <td>1075</td>\n      <td>0.002800</td>\n    </tr>\n    <tr>\n      <td>1076</td>\n      <td>0.001400</td>\n    </tr>\n    <tr>\n      <td>1077</td>\n      <td>0.000400</td>\n    </tr>\n    <tr>\n      <td>1078</td>\n      <td>0.001800</td>\n    </tr>\n    <tr>\n      <td>1079</td>\n      <td>0.000700</td>\n    </tr>\n    <tr>\n      <td>1080</td>\n      <td>0.002400</td>\n    </tr>\n    <tr>\n      <td>1081</td>\n      <td>0.002400</td>\n    </tr>\n    <tr>\n      <td>1082</td>\n      <td>0.000500</td>\n    </tr>\n    <tr>\n      <td>1083</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1084</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1085</td>\n      <td>0.000300</td>\n    </tr>\n    <tr>\n      <td>1086</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>1087</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1088</td>\n      <td>0.001500</td>\n    </tr>\n    <tr>\n      <td>1089</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>1090</td>\n      <td>0.000200</td>\n    </tr>\n    <tr>\n      <td>1091</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>1092</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1093</td>\n      <td>0.001100</td>\n    </tr>\n    <tr>\n      <td>1094</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1095</td>\n      <td>0.002600</td>\n    </tr>\n    <tr>\n      <td>1096</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1097</td>\n      <td>0.002700</td>\n    </tr>\n    <tr>\n      <td>1098</td>\n      <td>0.023600</td>\n    </tr>\n    <tr>\n      <td>1099</td>\n      <td>0.000800</td>\n    </tr>\n    <tr>\n      <td>1100</td>\n      <td>0.001300</td>\n    </tr>\n    <tr>\n      <td>1101</td>\n      <td>0.000300</td>\n    </tr>\n    <tr>\n      <td>1102</td>\n      <td>0.003600</td>\n    </tr>\n    <tr>\n      <td>1103</td>\n      <td>0.000400</td>\n    </tr>\n    <tr>\n      <td>1104</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1105</td>\n      <td>0.003800</td>\n    </tr>\n    <tr>\n      <td>1106</td>\n      <td>0.010800</td>\n    </tr>\n    <tr>\n      <td>1107</td>\n      <td>0.001400</td>\n    </tr>\n    <tr>\n      <td>1108</td>\n      <td>0.001800</td>\n    </tr>\n    <tr>\n      <td>1109</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>1110</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1111</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1112</td>\n      <td>0.004200</td>\n    </tr>\n    <tr>\n      <td>1113</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>1114</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>1115</td>\n      <td>0.008000</td>\n    </tr>\n    <tr>\n      <td>1116</td>\n      <td>0.005600</td>\n    </tr>\n    <tr>\n      <td>1117</td>\n      <td>0.000900</td>\n    </tr>\n    <tr>\n      <td>1118</td>\n      <td>0.003900</td>\n    </tr>\n    <tr>\n      <td>1119</td>\n      <td>0.000200</td>\n    </tr>\n    <tr>\n      <td>1120</td>\n      <td>0.003400</td>\n    </tr>\n    <tr>\n      <td>1121</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1122</td>\n      <td>0.001000</td>\n    </tr>\n    <tr>\n      <td>1123</td>\n      <td>0.002000</td>\n    </tr>\n    <tr>\n      <td>1124</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>1125</td>\n      <td>0.001200</td>\n    </tr>\n    <tr>\n      <td>1126</td>\n      <td>0.001700</td>\n    </tr>\n    <tr>\n      <td>1127</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>1128</td>\n      <td>0.000500</td>\n    </tr>\n    <tr>\n      <td>1129</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1130</td>\n      <td>0.004700</td>\n    </tr>\n    <tr>\n      <td>1131</td>\n      <td>0.000300</td>\n    </tr>\n    <tr>\n      <td>1132</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>1133</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1134</td>\n      <td>0.003700</td>\n    </tr>\n    <tr>\n      <td>1135</td>\n      <td>0.001300</td>\n    </tr>\n    <tr>\n      <td>1136</td>\n      <td>0.002600</td>\n    </tr>\n    <tr>\n      <td>1137</td>\n      <td>0.000500</td>\n    </tr>\n    <tr>\n      <td>1138</td>\n      <td>0.000400</td>\n    </tr>\n    <tr>\n      <td>1139</td>\n      <td>0.006400</td>\n    </tr>\n    <tr>\n      <td>1140</td>\n      <td>0.001000</td>\n    </tr>\n    <tr>\n      <td>1141</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>1142</td>\n      <td>0.000300</td>\n    </tr>\n    <tr>\n      <td>1143</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1144</td>\n      <td>0.003100</td>\n    </tr>\n    <tr>\n      <td>1145</td>\n      <td>0.001100</td>\n    </tr>\n    <tr>\n      <td>1146</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>1147</td>\n      <td>0.000200</td>\n    </tr>\n    <tr>\n      <td>1148</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>1149</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1150</td>\n      <td>0.000200</td>\n    </tr>\n    <tr>\n      <td>1151</td>\n      <td>0.000700</td>\n    </tr>\n    <tr>\n      <td>1152</td>\n      <td>0.000900</td>\n    </tr>\n    <tr>\n      <td>1153</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1154</td>\n      <td>0.002300</td>\n    </tr>\n    <tr>\n      <td>1155</td>\n      <td>0.006800</td>\n    </tr>\n    <tr>\n      <td>1156</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1157</td>\n      <td>0.000300</td>\n    </tr>\n    <tr>\n      <td>1158</td>\n      <td>0.000700</td>\n    </tr>\n    <tr>\n      <td>1159</td>\n      <td>0.000600</td>\n    </tr>\n    <tr>\n      <td>1160</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1161</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>1162</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1163</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1164</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>1165</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1166</td>\n      <td>0.000300</td>\n    </tr>\n    <tr>\n      <td>1167</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>1168</td>\n      <td>0.000700</td>\n    </tr>\n    <tr>\n      <td>1169</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1170</td>\n      <td>0.003700</td>\n    </tr>\n    <tr>\n      <td>1171</td>\n      <td>0.002600</td>\n    </tr>\n    <tr>\n      <td>1172</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>1173</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1174</td>\n      <td>0.004100</td>\n    </tr>\n    <tr>\n      <td>1175</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>1176</td>\n      <td>0.000900</td>\n    </tr>\n    <tr>\n      <td>1177</td>\n      <td>0.000200</td>\n    </tr>\n    <tr>\n      <td>1178</td>\n      <td>0.000400</td>\n    </tr>\n    <tr>\n      <td>1179</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1180</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1181</td>\n      <td>0.001000</td>\n    </tr>\n    <tr>\n      <td>1182</td>\n      <td>0.006100</td>\n    </tr>\n    <tr>\n      <td>1183</td>\n      <td>0.000900</td>\n    </tr>\n    <tr>\n      <td>1184</td>\n      <td>0.002600</td>\n    </tr>\n    <tr>\n      <td>1185</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1186</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>1187</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>1188</td>\n      <td>0.002000</td>\n    </tr>\n    <tr>\n      <td>1189</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>1190</td>\n      <td>0.000600</td>\n    </tr>\n    <tr>\n      <td>1191</td>\n      <td>0.000900</td>\n    </tr>\n    <tr>\n      <td>1192</td>\n      <td>0.000300</td>\n    </tr>\n    <tr>\n      <td>1193</td>\n      <td>0.008400</td>\n    </tr>\n    <tr>\n      <td>1194</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>1195</td>\n      <td>0.001800</td>\n    </tr>\n    <tr>\n      <td>1196</td>\n      <td>0.003600</td>\n    </tr>\n    <tr>\n      <td>1197</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>1198</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>1199</td>\n      <td>0.002800</td>\n    </tr>\n    <tr>\n      <td>1200</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1201</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1202</td>\n      <td>0.000400</td>\n    </tr>\n    <tr>\n      <td>1203</td>\n      <td>0.001200</td>\n    </tr>\n    <tr>\n      <td>1204</td>\n      <td>0.004000</td>\n    </tr>\n    <tr>\n      <td>1205</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1206</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>1207</td>\n      <td>0.007000</td>\n    </tr>\n    <tr>\n      <td>1208</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1209</td>\n      <td>0.006100</td>\n    </tr>\n    <tr>\n      <td>1210</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>1211</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>1212</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>1213</td>\n      <td>0.000300</td>\n    </tr>\n    <tr>\n      <td>1214</td>\n      <td>0.004200</td>\n    </tr>\n    <tr>\n      <td>1215</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>1216</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>1217</td>\n      <td>0.002900</td>\n    </tr>\n    <tr>\n      <td>1218</td>\n      <td>0.004000</td>\n    </tr>\n    <tr>\n      <td>1219</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1220</td>\n      <td>0.002100</td>\n    </tr>\n    <tr>\n      <td>1221</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1222</td>\n      <td>0.002800</td>\n    </tr>\n    <tr>\n      <td>1223</td>\n      <td>0.002300</td>\n    </tr>\n    <tr>\n      <td>1224</td>\n      <td>0.005700</td>\n    </tr>\n    <tr>\n      <td>1225</td>\n      <td>0.000400</td>\n    </tr>\n    <tr>\n      <td>1226</td>\n      <td>0.000600</td>\n    </tr>\n    <tr>\n      <td>1227</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>1228</td>\n      <td>0.000200</td>\n    </tr>\n    <tr>\n      <td>1229</td>\n      <td>0.001100</td>\n    </tr>\n    <tr>\n      <td>1230</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1231</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1232</td>\n      <td>0.004000</td>\n    </tr>\n    <tr>\n      <td>1233</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>1234</td>\n      <td>0.000900</td>\n    </tr>\n    <tr>\n      <td>1235</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>1236</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>1237</td>\n      <td>0.005700</td>\n    </tr>\n    <tr>\n      <td>1238</td>\n      <td>0.004300</td>\n    </tr>\n    <tr>\n      <td>1239</td>\n      <td>0.011800</td>\n    </tr>\n    <tr>\n      <td>1240</td>\n      <td>0.000200</td>\n    </tr>\n    <tr>\n      <td>1241</td>\n      <td>0.000400</td>\n    </tr>\n    <tr>\n      <td>1242</td>\n      <td>0.000400</td>\n    </tr>\n    <tr>\n      <td>1243</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1244</td>\n      <td>0.000300</td>\n    </tr>\n    <tr>\n      <td>1245</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1246</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1247</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>1248</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>1249</td>\n      <td>0.002500</td>\n    </tr>\n    <tr>\n      <td>1250</td>\n      <td>0.004500</td>\n    </tr>\n    <tr>\n      <td>1251</td>\n      <td>0.006200</td>\n    </tr>\n    <tr>\n      <td>1252</td>\n      <td>0.001900</td>\n    </tr>\n    <tr>\n      <td>1253</td>\n      <td>0.002000</td>\n    </tr>\n    <tr>\n      <td>1254</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>1255</td>\n      <td>0.001900</td>\n    </tr>\n    <tr>\n      <td>1256</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1257</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1258</td>\n      <td>0.004900</td>\n    </tr>\n    <tr>\n      <td>1259</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1260</td>\n      <td>0.000600</td>\n    </tr>\n    <tr>\n      <td>1261</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1262</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1263</td>\n      <td>0.004700</td>\n    </tr>\n    <tr>\n      <td>1264</td>\n      <td>0.002700</td>\n    </tr>\n    <tr>\n      <td>1265</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>1266</td>\n      <td>0.000200</td>\n    </tr>\n    <tr>\n      <td>1267</td>\n      <td>0.004400</td>\n    </tr>\n    <tr>\n      <td>1268</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1269</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>1270</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1271</td>\n      <td>0.010100</td>\n    </tr>\n    <tr>\n      <td>1272</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>1273</td>\n      <td>0.001800</td>\n    </tr>\n    <tr>\n      <td>1274</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1275</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1276</td>\n      <td>0.004200</td>\n    </tr>\n    <tr>\n      <td>1277</td>\n      <td>0.000200</td>\n    </tr>\n    <tr>\n      <td>1278</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1279</td>\n      <td>0.002200</td>\n    </tr>\n    <tr>\n      <td>1280</td>\n      <td>0.002900</td>\n    </tr>\n    <tr>\n      <td>1281</td>\n      <td>0.002600</td>\n    </tr>\n    <tr>\n      <td>1282</td>\n      <td>0.001000</td>\n    </tr>\n    <tr>\n      <td>1283</td>\n      <td>0.004100</td>\n    </tr>\n    <tr>\n      <td>1284</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1285</td>\n      <td>0.008400</td>\n    </tr>\n    <tr>\n      <td>1286</td>\n      <td>0.000800</td>\n    </tr>\n    <tr>\n      <td>1287</td>\n      <td>0.001000</td>\n    </tr>\n    <tr>\n      <td>1288</td>\n      <td>0.003000</td>\n    </tr>\n    <tr>\n      <td>1289</td>\n      <td>0.000300</td>\n    </tr>\n    <tr>\n      <td>1290</td>\n      <td>0.002500</td>\n    </tr>\n    <tr>\n      <td>1291</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1292</td>\n      <td>0.000700</td>\n    </tr>\n    <tr>\n      <td>1293</td>\n      <td>0.002000</td>\n    </tr>\n    <tr>\n      <td>1294</td>\n      <td>0.006700</td>\n    </tr>\n    <tr>\n      <td>1295</td>\n      <td>0.001200</td>\n    </tr>\n    <tr>\n      <td>1296</td>\n      <td>0.002500</td>\n    </tr>\n    <tr>\n      <td>1297</td>\n      <td>0.001600</td>\n    </tr>\n    <tr>\n      <td>1298</td>\n      <td>0.000900</td>\n    </tr>\n    <tr>\n      <td>1299</td>\n      <td>0.004700</td>\n    </tr>\n    <tr>\n      <td>1300</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>1301</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1302</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>1303</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>1304</td>\n      <td>0.000800</td>\n    </tr>\n    <tr>\n      <td>1305</td>\n      <td>0.003000</td>\n    </tr>\n    <tr>\n      <td>1306</td>\n      <td>0.006700</td>\n    </tr>\n    <tr>\n      <td>1307</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1308</td>\n      <td>0.005800</td>\n    </tr>\n    <tr>\n      <td>1309</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1310</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1311</td>\n      <td>0.003500</td>\n    </tr>\n    <tr>\n      <td>1312</td>\n      <td>0.000600</td>\n    </tr>\n    <tr>\n      <td>1313</td>\n      <td>0.000400</td>\n    </tr>\n    <tr>\n      <td>1314</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1315</td>\n      <td>0.004500</td>\n    </tr>\n    <tr>\n      <td>1316</td>\n      <td>0.001700</td>\n    </tr>\n    <tr>\n      <td>1317</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1318</td>\n      <td>0.000300</td>\n    </tr>\n    <tr>\n      <td>1319</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>1320</td>\n      <td>0.003900</td>\n    </tr>\n    <tr>\n      <td>1321</td>\n      <td>0.001200</td>\n    </tr>\n    <tr>\n      <td>1322</td>\n      <td>0.003300</td>\n    </tr>\n    <tr>\n      <td>1323</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1324</td>\n      <td>0.000400</td>\n    </tr>\n    <tr>\n      <td>1325</td>\n      <td>0.008900</td>\n    </tr>\n    <tr>\n      <td>1326</td>\n      <td>0.001800</td>\n    </tr>\n    <tr>\n      <td>1327</td>\n      <td>0.003500</td>\n    </tr>\n    <tr>\n      <td>1328</td>\n      <td>0.004800</td>\n    </tr>\n    <tr>\n      <td>1329</td>\n      <td>0.000900</td>\n    </tr>\n    <tr>\n      <td>1330</td>\n      <td>0.003400</td>\n    </tr>\n    <tr>\n      <td>1331</td>\n      <td>0.000200</td>\n    </tr>\n    <tr>\n      <td>1332</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>1333</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>1334</td>\n      <td>0.009500</td>\n    </tr>\n    <tr>\n      <td>1335</td>\n      <td>0.002600</td>\n    </tr>\n    <tr>\n      <td>1336</td>\n      <td>0.002700</td>\n    </tr>\n    <tr>\n      <td>1337</td>\n      <td>0.003000</td>\n    </tr>\n    <tr>\n      <td>1338</td>\n      <td>0.001300</td>\n    </tr>\n    <tr>\n      <td>1339</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>1340</td>\n      <td>0.007900</td>\n    </tr>\n    <tr>\n      <td>1341</td>\n      <td>0.005300</td>\n    </tr>\n    <tr>\n      <td>1342</td>\n      <td>0.004000</td>\n    </tr>\n    <tr>\n      <td>1343</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>1344</td>\n      <td>0.000400</td>\n    </tr>\n    <tr>\n      <td>1345</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>1346</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1347</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>1348</td>\n      <td>0.000800</td>\n    </tr>\n    <tr>\n      <td>1349</td>\n      <td>0.002600</td>\n    </tr>\n    <tr>\n      <td>1350</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1351</td>\n      <td>0.003100</td>\n    </tr>\n    <tr>\n      <td>1352</td>\n      <td>0.000200</td>\n    </tr>\n    <tr>\n      <td>1353</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1354</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1355</td>\n      <td>0.002700</td>\n    </tr>\n    <tr>\n      <td>1356</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>1357</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>1358</td>\n      <td>0.000200</td>\n    </tr>\n    <tr>\n      <td>1359</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1360</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>1361</td>\n      <td>0.001000</td>\n    </tr>\n    <tr>\n      <td>1362</td>\n      <td>0.000400</td>\n    </tr>\n    <tr>\n      <td>1363</td>\n      <td>0.001900</td>\n    </tr>\n    <tr>\n      <td>1364</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1365</td>\n      <td>0.000300</td>\n    </tr>\n    <tr>\n      <td>1366</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>1367</td>\n      <td>0.003800</td>\n    </tr>\n    <tr>\n      <td>1368</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1369</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1370</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>1371</td>\n      <td>0.008500</td>\n    </tr>\n    <tr>\n      <td>1372</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1373</td>\n      <td>0.002900</td>\n    </tr>\n    <tr>\n      <td>1374</td>\n      <td>0.003300</td>\n    </tr>\n    <tr>\n      <td>1375</td>\n      <td>0.000400</td>\n    </tr>\n    <tr>\n      <td>1376</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>1377</td>\n      <td>0.004400</td>\n    </tr>\n    <tr>\n      <td>1378</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1379</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>1380</td>\n      <td>0.000200</td>\n    </tr>\n    <tr>\n      <td>1381</td>\n      <td>0.007300</td>\n    </tr>\n    <tr>\n      <td>1382</td>\n      <td>0.003200</td>\n    </tr>\n    <tr>\n      <td>1383</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>1384</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1385</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1386</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>1387</td>\n      <td>0.001300</td>\n    </tr>\n    <tr>\n      <td>1388</td>\n      <td>0.000500</td>\n    </tr>\n    <tr>\n      <td>1389</td>\n      <td>0.004700</td>\n    </tr>\n    <tr>\n      <td>1390</td>\n      <td>0.005200</td>\n    </tr>\n    <tr>\n      <td>1391</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>1392</td>\n      <td>0.000300</td>\n    </tr>\n    <tr>\n      <td>1393</td>\n      <td>0.001900</td>\n    </tr>\n    <tr>\n      <td>1394</td>\n      <td>0.000900</td>\n    </tr>\n    <tr>\n      <td>1395</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1396</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1397</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>1398</td>\n      <td>0.002000</td>\n    </tr>\n    <tr>\n      <td>1399</td>\n      <td>0.000200</td>\n    </tr>\n    <tr>\n      <td>1400</td>\n      <td>0.007300</td>\n    </tr>\n    <tr>\n      <td>1401</td>\n      <td>0.007200</td>\n    </tr>\n    <tr>\n      <td>1402</td>\n      <td>0.002300</td>\n    </tr>\n    <tr>\n      <td>1403</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>1404</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>1405</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>1406</td>\n      <td>0.006900</td>\n    </tr>\n    <tr>\n      <td>1407</td>\n      <td>0.000300</td>\n    </tr>\n    <tr>\n      <td>1408</td>\n      <td>0.000300</td>\n    </tr>\n    <tr>\n      <td>1409</td>\n      <td>0.000200</td>\n    </tr>\n    <tr>\n      <td>1410</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1411</td>\n      <td>0.000300</td>\n    </tr>\n    <tr>\n      <td>1412</td>\n      <td>0.001100</td>\n    </tr>\n    <tr>\n      <td>1413</td>\n      <td>0.007000</td>\n    </tr>\n    <tr>\n      <td>1414</td>\n      <td>0.002700</td>\n    </tr>\n    <tr>\n      <td>1415</td>\n      <td>0.001200</td>\n    </tr>\n    <tr>\n      <td>1416</td>\n      <td>0.003400</td>\n    </tr>\n    <tr>\n      <td>1417</td>\n      <td>0.001200</td>\n    </tr>\n    <tr>\n      <td>1418</td>\n      <td>0.009600</td>\n    </tr>\n    <tr>\n      <td>1419</td>\n      <td>0.004500</td>\n    </tr>\n    <tr>\n      <td>1420</td>\n      <td>0.000300</td>\n    </tr>\n    <tr>\n      <td>1421</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>1422</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1423</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1424</td>\n      <td>0.001000</td>\n    </tr>\n    <tr>\n      <td>1425</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1426</td>\n      <td>0.003800</td>\n    </tr>\n    <tr>\n      <td>1427</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1428</td>\n      <td>0.003100</td>\n    </tr>\n    <tr>\n      <td>1429</td>\n      <td>0.003700</td>\n    </tr>\n    <tr>\n      <td>1430</td>\n      <td>0.000300</td>\n    </tr>\n    <tr>\n      <td>1431</td>\n      <td>0.000500</td>\n    </tr>\n    <tr>\n      <td>1432</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>1433</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1434</td>\n      <td>0.002200</td>\n    </tr>\n    <tr>\n      <td>1435</td>\n      <td>0.000200</td>\n    </tr>\n    <tr>\n      <td>1436</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1437</td>\n      <td>0.000300</td>\n    </tr>\n    <tr>\n      <td>1438</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>1439</td>\n      <td>0.006800</td>\n    </tr>\n    <tr>\n      <td>1440</td>\n      <td>0.000800</td>\n    </tr>\n    <tr>\n      <td>1441</td>\n      <td>0.001600</td>\n    </tr>\n    <tr>\n      <td>1442</td>\n      <td>0.000300</td>\n    </tr>\n    <tr>\n      <td>1443</td>\n      <td>0.001000</td>\n    </tr>\n    <tr>\n      <td>1444</td>\n      <td>0.006400</td>\n    </tr>\n    <tr>\n      <td>1445</td>\n      <td>0.005500</td>\n    </tr>\n    <tr>\n      <td>1446</td>\n      <td>0.003300</td>\n    </tr>\n    <tr>\n      <td>1447</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1448</td>\n      <td>0.002400</td>\n    </tr>\n    <tr>\n      <td>1449</td>\n      <td>0.000800</td>\n    </tr>\n    <tr>\n      <td>1450</td>\n      <td>0.013500</td>\n    </tr>\n    <tr>\n      <td>1451</td>\n      <td>0.003200</td>\n    </tr>\n    <tr>\n      <td>1452</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>1453</td>\n      <td>0.003400</td>\n    </tr>\n    <tr>\n      <td>1454</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1455</td>\n      <td>0.010800</td>\n    </tr>\n    <tr>\n      <td>1456</td>\n      <td>0.003300</td>\n    </tr>\n    <tr>\n      <td>1457</td>\n      <td>0.003400</td>\n    </tr>\n    <tr>\n      <td>1458</td>\n      <td>0.000300</td>\n    </tr>\n    <tr>\n      <td>1459</td>\n      <td>0.001600</td>\n    </tr>\n    <tr>\n      <td>1460</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>1461</td>\n      <td>0.006200</td>\n    </tr>\n    <tr>\n      <td>1462</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>1463</td>\n      <td>0.000200</td>\n    </tr>\n    <tr>\n      <td>1464</td>\n      <td>0.005100</td>\n    </tr>\n    <tr>\n      <td>1465</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>1466</td>\n      <td>0.001800</td>\n    </tr>\n    <tr>\n      <td>1467</td>\n      <td>0.000400</td>\n    </tr>\n    <tr>\n      <td>1468</td>\n      <td>0.000300</td>\n    </tr>\n    <tr>\n      <td>1469</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1470</td>\n      <td>0.010000</td>\n    </tr>\n    <tr>\n      <td>1471</td>\n      <td>0.003000</td>\n    </tr>\n    <tr>\n      <td>1472</td>\n      <td>0.004000</td>\n    </tr>\n    <tr>\n      <td>1473</td>\n      <td>0.002900</td>\n    </tr>\n    <tr>\n      <td>1474</td>\n      <td>0.005000</td>\n    </tr>\n    <tr>\n      <td>1475</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1476</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1477</td>\n      <td>0.006600</td>\n    </tr>\n    <tr>\n      <td>1478</td>\n      <td>0.004000</td>\n    </tr>\n    <tr>\n      <td>1479</td>\n      <td>0.006600</td>\n    </tr>\n    <tr>\n      <td>1480</td>\n      <td>0.000300</td>\n    </tr>\n    <tr>\n      <td>1481</td>\n      <td>0.006300</td>\n    </tr>\n    <tr>\n      <td>1482</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>1483</td>\n      <td>0.002200</td>\n    </tr>\n    <tr>\n      <td>1484</td>\n      <td>0.003700</td>\n    </tr>\n    <tr>\n      <td>1485</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1486</td>\n      <td>0.000900</td>\n    </tr>\n    <tr>\n      <td>1487</td>\n      <td>0.000200</td>\n    </tr>\n    <tr>\n      <td>1488</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1489</td>\n      <td>0.000200</td>\n    </tr>\n    <tr>\n      <td>1490</td>\n      <td>0.000200</td>\n    </tr>\n    <tr>\n      <td>1491</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1492</td>\n      <td>0.004600</td>\n    </tr>\n    <tr>\n      <td>1493</td>\n      <td>0.002700</td>\n    </tr>\n    <tr>\n      <td>1494</td>\n      <td>0.000200</td>\n    </tr>\n    <tr>\n      <td>1495</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1496</td>\n      <td>0.000200</td>\n    </tr>\n    <tr>\n      <td>1497</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>1498</td>\n      <td>0.003900</td>\n    </tr>\n    <tr>\n      <td>1499</td>\n      <td>0.005100</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1501</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1502</td>\n      <td>0.001300</td>\n    </tr>\n    <tr>\n      <td>1503</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>1504</td>\n      <td>0.002700</td>\n    </tr>\n    <tr>\n      <td>1505</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1506</td>\n      <td>0.001900</td>\n    </tr>\n    <tr>\n      <td>1507</td>\n      <td>0.003900</td>\n    </tr>\n    <tr>\n      <td>1508</td>\n      <td>0.000200</td>\n    </tr>\n    <tr>\n      <td>1509</td>\n      <td>0.005300</td>\n    </tr>\n    <tr>\n      <td>1510</td>\n      <td>0.004900</td>\n    </tr>\n    <tr>\n      <td>1511</td>\n      <td>0.003500</td>\n    </tr>\n    <tr>\n      <td>1512</td>\n      <td>0.003400</td>\n    </tr>\n    <tr>\n      <td>1513</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>1514</td>\n      <td>0.008200</td>\n    </tr>\n    <tr>\n      <td>1515</td>\n      <td>0.001200</td>\n    </tr>\n    <tr>\n      <td>1516</td>\n      <td>0.000200</td>\n    </tr>\n    <tr>\n      <td>1517</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>1518</td>\n      <td>0.002100</td>\n    </tr>\n    <tr>\n      <td>1519</td>\n      <td>0.003900</td>\n    </tr>\n    <tr>\n      <td>1520</td>\n      <td>0.004000</td>\n    </tr>\n    <tr>\n      <td>1521</td>\n      <td>0.004900</td>\n    </tr>\n    <tr>\n      <td>1522</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>1523</td>\n      <td>0.007600</td>\n    </tr>\n    <tr>\n      <td>1524</td>\n      <td>0.007300</td>\n    </tr>\n    <tr>\n      <td>1525</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>1526</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1527</td>\n      <td>0.000300</td>\n    </tr>\n    <tr>\n      <td>1528</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1529</td>\n      <td>0.000400</td>\n    </tr>\n    <tr>\n      <td>1530</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1531</td>\n      <td>0.000200</td>\n    </tr>\n    <tr>\n      <td>1532</td>\n      <td>0.000700</td>\n    </tr>\n    <tr>\n      <td>1533</td>\n      <td>0.013500</td>\n    </tr>\n    <tr>\n      <td>1534</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1535</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1536</td>\n      <td>0.001100</td>\n    </tr>\n    <tr>\n      <td>1537</td>\n      <td>0.000300</td>\n    </tr>\n    <tr>\n      <td>1538</td>\n      <td>0.003500</td>\n    </tr>\n    <tr>\n      <td>1539</td>\n      <td>0.000900</td>\n    </tr>\n    <tr>\n      <td>1540</td>\n      <td>0.000200</td>\n    </tr>\n    <tr>\n      <td>1541</td>\n      <td>0.008300</td>\n    </tr>\n    <tr>\n      <td>1542</td>\n      <td>0.000600</td>\n    </tr>\n    <tr>\n      <td>1543</td>\n      <td>0.004100</td>\n    </tr>\n    <tr>\n      <td>1544</td>\n      <td>0.001900</td>\n    </tr>\n    <tr>\n      <td>1545</td>\n      <td>0.001800</td>\n    </tr>\n    <tr>\n      <td>1546</td>\n      <td>0.002600</td>\n    </tr>\n    <tr>\n      <td>1547</td>\n      <td>0.001300</td>\n    </tr>\n    <tr>\n      <td>1548</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>1549</td>\n      <td>0.000200</td>\n    </tr>\n    <tr>\n      <td>1550</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>1551</td>\n      <td>0.004600</td>\n    </tr>\n    <tr>\n      <td>1552</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>1553</td>\n      <td>0.000200</td>\n    </tr>\n    <tr>\n      <td>1554</td>\n      <td>0.001900</td>\n    </tr>\n    <tr>\n      <td>1555</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>1556</td>\n      <td>0.000600</td>\n    </tr>\n    <tr>\n      <td>1557</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1558</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1559</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>1560</td>\n      <td>0.003000</td>\n    </tr>\n    <tr>\n      <td>1561</td>\n      <td>0.003000</td>\n    </tr>\n    <tr>\n      <td>1562</td>\n      <td>0.000300</td>\n    </tr>\n    <tr>\n      <td>1563</td>\n      <td>0.000300</td>\n    </tr>\n    <tr>\n      <td>1564</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1565</td>\n      <td>0.003700</td>\n    </tr>\n    <tr>\n      <td>1566</td>\n      <td>0.005500</td>\n    </tr>\n    <tr>\n      <td>1567</td>\n      <td>0.003800</td>\n    </tr>\n    <tr>\n      <td>1568</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>1569</td>\n      <td>0.000200</td>\n    </tr>\n    <tr>\n      <td>1570</td>\n      <td>0.001400</td>\n    </tr>\n    <tr>\n      <td>1571</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>1572</td>\n      <td>0.000800</td>\n    </tr>\n    <tr>\n      <td>1573</td>\n      <td>0.000800</td>\n    </tr>\n    <tr>\n      <td>1574</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>1575</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>1576</td>\n      <td>0.000200</td>\n    </tr>\n    <tr>\n      <td>1577</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1578</td>\n      <td>0.004100</td>\n    </tr>\n    <tr>\n      <td>1579</td>\n      <td>0.002800</td>\n    </tr>\n    <tr>\n      <td>1580</td>\n      <td>0.005700</td>\n    </tr>\n    <tr>\n      <td>1581</td>\n      <td>0.002000</td>\n    </tr>\n    <tr>\n      <td>1582</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>1583</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>1584</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1585</td>\n      <td>0.002300</td>\n    </tr>\n    <tr>\n      <td>1586</td>\n      <td>0.002300</td>\n    </tr>\n    <tr>\n      <td>1587</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>1588</td>\n      <td>0.001900</td>\n    </tr>\n    <tr>\n      <td>1589</td>\n      <td>0.000300</td>\n    </tr>\n    <tr>\n      <td>1590</td>\n      <td>0.004600</td>\n    </tr>\n    <tr>\n      <td>1591</td>\n      <td>0.007300</td>\n    </tr>\n    <tr>\n      <td>1592</td>\n      <td>0.001400</td>\n    </tr>\n    <tr>\n      <td>1593</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>1594</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1595</td>\n      <td>0.001700</td>\n    </tr>\n    <tr>\n      <td>1596</td>\n      <td>0.000700</td>\n    </tr>\n    <tr>\n      <td>1597</td>\n      <td>0.003400</td>\n    </tr>\n    <tr>\n      <td>1598</td>\n      <td>0.002400</td>\n    </tr>\n    <tr>\n      <td>1599</td>\n      <td>0.000400</td>\n    </tr>\n    <tr>\n      <td>1600</td>\n      <td>0.005800</td>\n    </tr>\n    <tr>\n      <td>1601</td>\n      <td>0.003100</td>\n    </tr>\n    <tr>\n      <td>1602</td>\n      <td>0.001100</td>\n    </tr>\n    <tr>\n      <td>1603</td>\n      <td>0.012200</td>\n    </tr>\n    <tr>\n      <td>1604</td>\n      <td>0.002300</td>\n    </tr>\n    <tr>\n      <td>1605</td>\n      <td>0.000200</td>\n    </tr>\n    <tr>\n      <td>1606</td>\n      <td>0.003200</td>\n    </tr>\n    <tr>\n      <td>1607</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1608</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>1609</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1610</td>\n      <td>0.001800</td>\n    </tr>\n    <tr>\n      <td>1611</td>\n      <td>0.004600</td>\n    </tr>\n    <tr>\n      <td>1612</td>\n      <td>0.000400</td>\n    </tr>\n    <tr>\n      <td>1613</td>\n      <td>0.000500</td>\n    </tr>\n    <tr>\n      <td>1614</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>1615</td>\n      <td>0.002300</td>\n    </tr>\n    <tr>\n      <td>1616</td>\n      <td>0.003700</td>\n    </tr>\n    <tr>\n      <td>1617</td>\n      <td>0.003500</td>\n    </tr>\n    <tr>\n      <td>1618</td>\n      <td>0.002100</td>\n    </tr>\n    <tr>\n      <td>1619</td>\n      <td>0.005600</td>\n    </tr>\n    <tr>\n      <td>1620</td>\n      <td>0.000300</td>\n    </tr>\n    <tr>\n      <td>1621</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>1622</td>\n      <td>0.007000</td>\n    </tr>\n    <tr>\n      <td>1623</td>\n      <td>0.000200</td>\n    </tr>\n    <tr>\n      <td>1624</td>\n      <td>0.001700</td>\n    </tr>\n    <tr>\n      <td>1625</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>1626</td>\n      <td>0.002200</td>\n    </tr>\n    <tr>\n      <td>1627</td>\n      <td>0.002800</td>\n    </tr>\n    <tr>\n      <td>1628</td>\n      <td>0.000900</td>\n    </tr>\n    <tr>\n      <td>1629</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>1630</td>\n      <td>0.003600</td>\n    </tr>\n    <tr>\n      <td>1631</td>\n      <td>0.004800</td>\n    </tr>\n    <tr>\n      <td>1632</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1633</td>\n      <td>0.000800</td>\n    </tr>\n    <tr>\n      <td>1634</td>\n      <td>0.005700</td>\n    </tr>\n    <tr>\n      <td>1635</td>\n      <td>0.000300</td>\n    </tr>\n    <tr>\n      <td>1636</td>\n      <td>0.003400</td>\n    </tr>\n    <tr>\n      <td>1637</td>\n      <td>0.003200</td>\n    </tr>\n    <tr>\n      <td>1638</td>\n      <td>0.000200</td>\n    </tr>\n    <tr>\n      <td>1639</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1640</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>1641</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1642</td>\n      <td>0.000500</td>\n    </tr>\n    <tr>\n      <td>1643</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1644</td>\n      <td>0.004300</td>\n    </tr>\n    <tr>\n      <td>1645</td>\n      <td>0.004600</td>\n    </tr>\n    <tr>\n      <td>1646</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>1647</td>\n      <td>0.000500</td>\n    </tr>\n    <tr>\n      <td>1648</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>1649</td>\n      <td>0.002300</td>\n    </tr>\n    <tr>\n      <td>1650</td>\n      <td>0.002800</td>\n    </tr>\n    <tr>\n      <td>1651</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>1652</td>\n      <td>0.003300</td>\n    </tr>\n    <tr>\n      <td>1653</td>\n      <td>0.004800</td>\n    </tr>\n    <tr>\n      <td>1654</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1655</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>1656</td>\n      <td>0.001300</td>\n    </tr>\n    <tr>\n      <td>1657</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>1658</td>\n      <td>0.009900</td>\n    </tr>\n    <tr>\n      <td>1659</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1660</td>\n      <td>0.004000</td>\n    </tr>\n    <tr>\n      <td>1661</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>1662</td>\n      <td>0.001300</td>\n    </tr>\n    <tr>\n      <td>1663</td>\n      <td>0.002800</td>\n    </tr>\n    <tr>\n      <td>1664</td>\n      <td>0.000200</td>\n    </tr>\n    <tr>\n      <td>1665</td>\n      <td>0.000900</td>\n    </tr>\n    <tr>\n      <td>1666</td>\n      <td>0.000900</td>\n    </tr>\n    <tr>\n      <td>1667</td>\n      <td>0.003500</td>\n    </tr>\n    <tr>\n      <td>1668</td>\n      <td>0.002000</td>\n    </tr>\n    <tr>\n      <td>1669</td>\n      <td>0.002300</td>\n    </tr>\n    <tr>\n      <td>1670</td>\n      <td>0.000300</td>\n    </tr>\n    <tr>\n      <td>1671</td>\n      <td>0.008600</td>\n    </tr>\n    <tr>\n      <td>1672</td>\n      <td>0.006800</td>\n    </tr>\n    <tr>\n      <td>1673</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1674</td>\n      <td>0.000200</td>\n    </tr>\n    <tr>\n      <td>1675</td>\n      <td>0.000600</td>\n    </tr>\n    <tr>\n      <td>1676</td>\n      <td>0.001200</td>\n    </tr>\n    <tr>\n      <td>1677</td>\n      <td>0.000200</td>\n    </tr>\n    <tr>\n      <td>1678</td>\n      <td>0.000200</td>\n    </tr>\n    <tr>\n      <td>1679</td>\n      <td>0.003200</td>\n    </tr>\n    <tr>\n      <td>1680</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>1681</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>1682</td>\n      <td>0.001800</td>\n    </tr>\n    <tr>\n      <td>1683</td>\n      <td>0.001900</td>\n    </tr>\n    <tr>\n      <td>1684</td>\n      <td>0.000300</td>\n    </tr>\n    <tr>\n      <td>1685</td>\n      <td>0.001500</td>\n    </tr>\n    <tr>\n      <td>1686</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1687</td>\n      <td>0.003000</td>\n    </tr>\n    <tr>\n      <td>1688</td>\n      <td>0.000200</td>\n    </tr>\n    <tr>\n      <td>1689</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1690</td>\n      <td>0.002800</td>\n    </tr>\n    <tr>\n      <td>1691</td>\n      <td>0.004300</td>\n    </tr>\n    <tr>\n      <td>1692</td>\n      <td>0.000600</td>\n    </tr>\n    <tr>\n      <td>1693</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>1694</td>\n      <td>0.002100</td>\n    </tr>\n    <tr>\n      <td>1695</td>\n      <td>0.006000</td>\n    </tr>\n    <tr>\n      <td>1696</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1697</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>1698</td>\n      <td>0.000300</td>\n    </tr>\n    <tr>\n      <td>1699</td>\n      <td>0.000500</td>\n    </tr>\n    <tr>\n      <td>1700</td>\n      <td>0.001900</td>\n    </tr>\n    <tr>\n      <td>1701</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1702</td>\n      <td>0.001100</td>\n    </tr>\n    <tr>\n      <td>1703</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>1704</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1705</td>\n      <td>0.000500</td>\n    </tr>\n    <tr>\n      <td>1706</td>\n      <td>0.003200</td>\n    </tr>\n    <tr>\n      <td>1707</td>\n      <td>0.000200</td>\n    </tr>\n    <tr>\n      <td>1708</td>\n      <td>0.001400</td>\n    </tr>\n    <tr>\n      <td>1709</td>\n      <td>0.001400</td>\n    </tr>\n    <tr>\n      <td>1710</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>1711</td>\n      <td>0.000200</td>\n    </tr>\n    <tr>\n      <td>1712</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>1713</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1714</td>\n      <td>0.000400</td>\n    </tr>\n    <tr>\n      <td>1715</td>\n      <td>0.005100</td>\n    </tr>\n    <tr>\n      <td>1716</td>\n      <td>0.000700</td>\n    </tr>\n    <tr>\n      <td>1717</td>\n      <td>0.000500</td>\n    </tr>\n    <tr>\n      <td>1718</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1719</td>\n      <td>0.001300</td>\n    </tr>\n    <tr>\n      <td>1720</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>1721</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>1722</td>\n      <td>0.000200</td>\n    </tr>\n    <tr>\n      <td>1723</td>\n      <td>0.004200</td>\n    </tr>\n    <tr>\n      <td>1724</td>\n      <td>0.002500</td>\n    </tr>\n    <tr>\n      <td>1725</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>1726</td>\n      <td>0.000200</td>\n    </tr>\n    <tr>\n      <td>1727</td>\n      <td>0.001000</td>\n    </tr>\n    <tr>\n      <td>1728</td>\n      <td>0.000400</td>\n    </tr>\n    <tr>\n      <td>1729</td>\n      <td>0.000400</td>\n    </tr>\n    <tr>\n      <td>1730</td>\n      <td>0.002500</td>\n    </tr>\n    <tr>\n      <td>1731</td>\n      <td>0.002400</td>\n    </tr>\n    <tr>\n      <td>1732</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1733</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>1734</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>1735</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>1736</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>1737</td>\n      <td>0.001100</td>\n    </tr>\n    <tr>\n      <td>1738</td>\n      <td>0.000700</td>\n    </tr>\n    <tr>\n      <td>1739</td>\n      <td>0.010200</td>\n    </tr>\n    <tr>\n      <td>1740</td>\n      <td>0.000900</td>\n    </tr>\n    <tr>\n      <td>1741</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1742</td>\n      <td>0.003500</td>\n    </tr>\n    <tr>\n      <td>1743</td>\n      <td>0.000300</td>\n    </tr>\n    <tr>\n      <td>1744</td>\n      <td>0.003000</td>\n    </tr>\n    <tr>\n      <td>1745</td>\n      <td>0.000200</td>\n    </tr>\n    <tr>\n      <td>1746</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1747</td>\n      <td>0.004400</td>\n    </tr>\n    <tr>\n      <td>1748</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1749</td>\n      <td>0.000200</td>\n    </tr>\n    <tr>\n      <td>1750</td>\n      <td>0.005000</td>\n    </tr>\n    <tr>\n      <td>1751</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>1752</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>1753</td>\n      <td>0.003300</td>\n    </tr>\n    <tr>\n      <td>1754</td>\n      <td>0.001800</td>\n    </tr>\n    <tr>\n      <td>1755</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>1756</td>\n      <td>0.004100</td>\n    </tr>\n    <tr>\n      <td>1757</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>1758</td>\n      <td>0.000300</td>\n    </tr>\n    <tr>\n      <td>1759</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1760</td>\n      <td>0.000800</td>\n    </tr>\n    <tr>\n      <td>1761</td>\n      <td>0.003800</td>\n    </tr>\n    <tr>\n      <td>1762</td>\n      <td>0.001200</td>\n    </tr>\n    <tr>\n      <td>1763</td>\n      <td>0.000400</td>\n    </tr>\n    <tr>\n      <td>1764</td>\n      <td>0.008800</td>\n    </tr>\n    <tr>\n      <td>1765</td>\n      <td>0.000200</td>\n    </tr>\n    <tr>\n      <td>1766</td>\n      <td>0.000600</td>\n    </tr>\n    <tr>\n      <td>1767</td>\n      <td>0.005400</td>\n    </tr>\n    <tr>\n      <td>1768</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>1769</td>\n      <td>0.000900</td>\n    </tr>\n    <tr>\n      <td>1770</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1771</td>\n      <td>0.000500</td>\n    </tr>\n    <tr>\n      <td>1772</td>\n      <td>0.000500</td>\n    </tr>\n    <tr>\n      <td>1773</td>\n      <td>0.003800</td>\n    </tr>\n    <tr>\n      <td>1774</td>\n      <td>0.004200</td>\n    </tr>\n    <tr>\n      <td>1775</td>\n      <td>0.006300</td>\n    </tr>\n    <tr>\n      <td>1776</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>1777</td>\n      <td>0.000200</td>\n    </tr>\n    <tr>\n      <td>1778</td>\n      <td>0.006200</td>\n    </tr>\n    <tr>\n      <td>1779</td>\n      <td>0.003500</td>\n    </tr>\n    <tr>\n      <td>1780</td>\n      <td>0.000200</td>\n    </tr>\n    <tr>\n      <td>1781</td>\n      <td>0.000600</td>\n    </tr>\n    <tr>\n      <td>1782</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>1783</td>\n      <td>0.000400</td>\n    </tr>\n    <tr>\n      <td>1784</td>\n      <td>0.003700</td>\n    </tr>\n    <tr>\n      <td>1785</td>\n      <td>0.003300</td>\n    </tr>\n    <tr>\n      <td>1786</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>1787</td>\n      <td>0.000800</td>\n    </tr>\n    <tr>\n      <td>1788</td>\n      <td>0.000900</td>\n    </tr>\n    <tr>\n      <td>1789</td>\n      <td>0.002400</td>\n    </tr>\n    <tr>\n      <td>1790</td>\n      <td>0.000600</td>\n    </tr>\n    <tr>\n      <td>1791</td>\n      <td>0.006300</td>\n    </tr>\n    <tr>\n      <td>1792</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>1793</td>\n      <td>0.000700</td>\n    </tr>\n    <tr>\n      <td>1794</td>\n      <td>0.000300</td>\n    </tr>\n    <tr>\n      <td>1795</td>\n      <td>0.000600</td>\n    </tr>\n    <tr>\n      <td>1796</td>\n      <td>0.000200</td>\n    </tr>\n    <tr>\n      <td>1797</td>\n      <td>0.000200</td>\n    </tr>\n    <tr>\n      <td>1798</td>\n      <td>0.001300</td>\n    </tr>\n    <tr>\n      <td>1799</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>1800</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>1801</td>\n      <td>0.000300</td>\n    </tr>\n    <tr>\n      <td>1802</td>\n      <td>0.004500</td>\n    </tr>\n    <tr>\n      <td>1803</td>\n      <td>0.000200</td>\n    </tr>\n    <tr>\n      <td>1804</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>1805</td>\n      <td>0.007800</td>\n    </tr>\n    <tr>\n      <td>1806</td>\n      <td>0.000500</td>\n    </tr>\n    <tr>\n      <td>1807</td>\n      <td>0.000300</td>\n    </tr>\n    <tr>\n      <td>1808</td>\n      <td>0.001400</td>\n    </tr>\n    <tr>\n      <td>1809</td>\n      <td>0.000300</td>\n    </tr>\n    <tr>\n      <td>1810</td>\n      <td>0.000900</td>\n    </tr>\n    <tr>\n      <td>1811</td>\n      <td>0.000200</td>\n    </tr>\n    <tr>\n      <td>1812</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>1813</td>\n      <td>0.001400</td>\n    </tr>\n    <tr>\n      <td>1814</td>\n      <td>0.003800</td>\n    </tr>\n    <tr>\n      <td>1815</td>\n      <td>0.004200</td>\n    </tr>\n    <tr>\n      <td>1816</td>\n      <td>0.002000</td>\n    </tr>\n    <tr>\n      <td>1817</td>\n      <td>0.000300</td>\n    </tr>\n    <tr>\n      <td>1818</td>\n      <td>0.004500</td>\n    </tr>\n    <tr>\n      <td>1819</td>\n      <td>0.002700</td>\n    </tr>\n    <tr>\n      <td>1820</td>\n      <td>0.003200</td>\n    </tr>\n    <tr>\n      <td>1821</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1822</td>\n      <td>0.004400</td>\n    </tr>\n    <tr>\n      <td>1823</td>\n      <td>0.000700</td>\n    </tr>\n    <tr>\n      <td>1824</td>\n      <td>0.000900</td>\n    </tr>\n    <tr>\n      <td>1825</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1826</td>\n      <td>0.000200</td>\n    </tr>\n    <tr>\n      <td>1827</td>\n      <td>0.033200</td>\n    </tr>\n    <tr>\n      <td>1828</td>\n      <td>0.000300</td>\n    </tr>\n    <tr>\n      <td>1829</td>\n      <td>0.005100</td>\n    </tr>\n    <tr>\n      <td>1830</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>1831</td>\n      <td>0.000200</td>\n    </tr>\n    <tr>\n      <td>1832</td>\n      <td>0.013400</td>\n    </tr>\n    <tr>\n      <td>1833</td>\n      <td>0.002800</td>\n    </tr>\n    <tr>\n      <td>1834</td>\n      <td>0.001100</td>\n    </tr>\n    <tr>\n      <td>1835</td>\n      <td>0.001100</td>\n    </tr>\n    <tr>\n      <td>1836</td>\n      <td>0.012600</td>\n    </tr>\n    <tr>\n      <td>1837</td>\n      <td>0.000400</td>\n    </tr>\n    <tr>\n      <td>1838</td>\n      <td>0.000900</td>\n    </tr>\n    <tr>\n      <td>1839</td>\n      <td>0.002100</td>\n    </tr>\n    <tr>\n      <td>1840</td>\n      <td>0.005100</td>\n    </tr>\n    <tr>\n      <td>1841</td>\n      <td>0.002300</td>\n    </tr>\n    <tr>\n      <td>1842</td>\n      <td>0.002300</td>\n    </tr>\n    <tr>\n      <td>1843</td>\n      <td>0.000300</td>\n    </tr>\n    <tr>\n      <td>1844</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1845</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>1846</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>1847</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>1848</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>1849</td>\n      <td>0.008000</td>\n    </tr>\n    <tr>\n      <td>1850</td>\n      <td>0.001100</td>\n    </tr>\n    <tr>\n      <td>1851</td>\n      <td>0.002600</td>\n    </tr>\n    <tr>\n      <td>1852</td>\n      <td>0.000300</td>\n    </tr>\n    <tr>\n      <td>1853</td>\n      <td>0.002100</td>\n    </tr>\n    <tr>\n      <td>1854</td>\n      <td>0.007600</td>\n    </tr>\n    <tr>\n      <td>1855</td>\n      <td>0.000700</td>\n    </tr>\n    <tr>\n      <td>1856</td>\n      <td>0.007500</td>\n    </tr>\n    <tr>\n      <td>1857</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>1858</td>\n      <td>0.008800</td>\n    </tr>\n    <tr>\n      <td>1859</td>\n      <td>0.105400</td>\n    </tr>\n    <tr>\n      <td>1860</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1861</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>1862</td>\n      <td>0.002900</td>\n    </tr>\n    <tr>\n      <td>1863</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1864</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1865</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>1866</td>\n      <td>0.001800</td>\n    </tr>\n    <tr>\n      <td>1867</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>1868</td>\n      <td>0.003000</td>\n    </tr>\n    <tr>\n      <td>1869</td>\n      <td>0.002700</td>\n    </tr>\n    <tr>\n      <td>1870</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1871</td>\n      <td>0.005100</td>\n    </tr>\n    <tr>\n      <td>1872</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1873</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1874</td>\n      <td>0.006300</td>\n    </tr>\n    <tr>\n      <td>1875</td>\n      <td>0.003400</td>\n    </tr>\n    <tr>\n      <td>1876</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>1877</td>\n      <td>0.000300</td>\n    </tr>\n    <tr>\n      <td>1878</td>\n      <td>0.000200</td>\n    </tr>\n    <tr>\n      <td>1879</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>1880</td>\n      <td>0.001800</td>\n    </tr>\n    <tr>\n      <td>1881</td>\n      <td>0.002700</td>\n    </tr>\n    <tr>\n      <td>1882</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>1883</td>\n      <td>0.029100</td>\n    </tr>\n    <tr>\n      <td>1884</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>1885</td>\n      <td>0.002900</td>\n    </tr>\n    <tr>\n      <td>1886</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>1887</td>\n      <td>0.004200</td>\n    </tr>\n    <tr>\n      <td>1888</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>1889</td>\n      <td>0.000500</td>\n    </tr>\n    <tr>\n      <td>1890</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>1891</td>\n      <td>0.002100</td>\n    </tr>\n    <tr>\n      <td>1892</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>1893</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>1894</td>\n      <td>0.001200</td>\n    </tr>\n    <tr>\n      <td>1895</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1896</td>\n      <td>0.000600</td>\n    </tr>\n    <tr>\n      <td>1897</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1898</td>\n      <td>0.000800</td>\n    </tr>\n    <tr>\n      <td>1899</td>\n      <td>0.001100</td>\n    </tr>\n    <tr>\n      <td>1900</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>1901</td>\n      <td>0.002100</td>\n    </tr>\n    <tr>\n      <td>1902</td>\n      <td>0.002000</td>\n    </tr>\n    <tr>\n      <td>1903</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>1904</td>\n      <td>0.007400</td>\n    </tr>\n    <tr>\n      <td>1905</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>1906</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>1907</td>\n      <td>0.003100</td>\n    </tr>\n    <tr>\n      <td>1908</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1909</td>\n      <td>0.000300</td>\n    </tr>\n    <tr>\n      <td>1910</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1911</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1912</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1913</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>1914</td>\n      <td>0.001600</td>\n    </tr>\n    <tr>\n      <td>1915</td>\n      <td>0.000300</td>\n    </tr>\n    <tr>\n      <td>1916</td>\n      <td>0.000400</td>\n    </tr>\n    <tr>\n      <td>1917</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>1918</td>\n      <td>0.000600</td>\n    </tr>\n    <tr>\n      <td>1919</td>\n      <td>0.001500</td>\n    </tr>\n    <tr>\n      <td>1920</td>\n      <td>0.000500</td>\n    </tr>\n    <tr>\n      <td>1921</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1922</td>\n      <td>0.007700</td>\n    </tr>\n    <tr>\n      <td>1923</td>\n      <td>0.000300</td>\n    </tr>\n    <tr>\n      <td>1924</td>\n      <td>0.003400</td>\n    </tr>\n    <tr>\n      <td>1925</td>\n      <td>0.000300</td>\n    </tr>\n    <tr>\n      <td>1926</td>\n      <td>0.005500</td>\n    </tr>\n    <tr>\n      <td>1927</td>\n      <td>0.000900</td>\n    </tr>\n    <tr>\n      <td>1928</td>\n      <td>0.000600</td>\n    </tr>\n    <tr>\n      <td>1929</td>\n      <td>0.000200</td>\n    </tr>\n    <tr>\n      <td>1930</td>\n      <td>0.000300</td>\n    </tr>\n    <tr>\n      <td>1931</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1932</td>\n      <td>0.002000</td>\n    </tr>\n    <tr>\n      <td>1933</td>\n      <td>0.001400</td>\n    </tr>\n    <tr>\n      <td>1934</td>\n      <td>0.005400</td>\n    </tr>\n    <tr>\n      <td>1935</td>\n      <td>0.000400</td>\n    </tr>\n    <tr>\n      <td>1936</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>1937</td>\n      <td>0.001900</td>\n    </tr>\n    <tr>\n      <td>1938</td>\n      <td>0.008100</td>\n    </tr>\n    <tr>\n      <td>1939</td>\n      <td>0.000800</td>\n    </tr>\n    <tr>\n      <td>1940</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1941</td>\n      <td>0.000200</td>\n    </tr>\n    <tr>\n      <td>1942</td>\n      <td>0.002000</td>\n    </tr>\n    <tr>\n      <td>1943</td>\n      <td>0.004600</td>\n    </tr>\n    <tr>\n      <td>1944</td>\n      <td>0.004200</td>\n    </tr>\n    <tr>\n      <td>1945</td>\n      <td>0.004200</td>\n    </tr>\n    <tr>\n      <td>1946</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>1947</td>\n      <td>0.000200</td>\n    </tr>\n    <tr>\n      <td>1948</td>\n      <td>0.002300</td>\n    </tr>\n    <tr>\n      <td>1949</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>1950</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1951</td>\n      <td>0.001700</td>\n    </tr>\n    <tr>\n      <td>1952</td>\n      <td>0.002700</td>\n    </tr>\n    <tr>\n      <td>1953</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>1954</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>1955</td>\n      <td>0.004100</td>\n    </tr>\n    <tr>\n      <td>1956</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>1957</td>\n      <td>0.000400</td>\n    </tr>\n    <tr>\n      <td>1958</td>\n      <td>0.000200</td>\n    </tr>\n    <tr>\n      <td>1959</td>\n      <td>0.002700</td>\n    </tr>\n    <tr>\n      <td>1960</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1961</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>1962</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1963</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1964</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1965</td>\n      <td>0.004100</td>\n    </tr>\n    <tr>\n      <td>1966</td>\n      <td>0.003700</td>\n    </tr>\n    <tr>\n      <td>1967</td>\n      <td>0.001800</td>\n    </tr>\n    <tr>\n      <td>1968</td>\n      <td>0.004700</td>\n    </tr>\n    <tr>\n      <td>1969</td>\n      <td>0.003300</td>\n    </tr>\n    <tr>\n      <td>1970</td>\n      <td>0.001900</td>\n    </tr>\n    <tr>\n      <td>1971</td>\n      <td>0.011700</td>\n    </tr>\n    <tr>\n      <td>1972</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>1973</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1974</td>\n      <td>0.000400</td>\n    </tr>\n    <tr>\n      <td>1975</td>\n      <td>0.001800</td>\n    </tr>\n    <tr>\n      <td>1976</td>\n      <td>0.003300</td>\n    </tr>\n    <tr>\n      <td>1977</td>\n      <td>0.000800</td>\n    </tr>\n    <tr>\n      <td>1978</td>\n      <td>0.003000</td>\n    </tr>\n    <tr>\n      <td>1979</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1980</td>\n      <td>0.000300</td>\n    </tr>\n    <tr>\n      <td>1981</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>1982</td>\n      <td>0.005600</td>\n    </tr>\n    <tr>\n      <td>1983</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1984</td>\n      <td>0.000800</td>\n    </tr>\n    <tr>\n      <td>1985</td>\n      <td>0.000900</td>\n    </tr>\n    <tr>\n      <td>1986</td>\n      <td>0.003600</td>\n    </tr>\n    <tr>\n      <td>1987</td>\n      <td>0.002300</td>\n    </tr>\n    <tr>\n      <td>1988</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>1989</td>\n      <td>0.008700</td>\n    </tr>\n    <tr>\n      <td>1990</td>\n      <td>0.007800</td>\n    </tr>\n    <tr>\n      <td>1991</td>\n      <td>0.001000</td>\n    </tr>\n    <tr>\n      <td>1992</td>\n      <td>0.001000</td>\n    </tr>\n    <tr>\n      <td>1993</td>\n      <td>0.004100</td>\n    </tr>\n    <tr>\n      <td>1994</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1995</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>1996</td>\n      <td>0.003400</td>\n    </tr>\n    <tr>\n      <td>1997</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1998</td>\n      <td>0.000400</td>\n    </tr>\n    <tr>\n      <td>1999</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>0.001300</td>\n    </tr>\n    <tr>\n      <td>2001</td>\n      <td>0.001200</td>\n    </tr>\n    <tr>\n      <td>2002</td>\n      <td>0.000300</td>\n    </tr>\n    <tr>\n      <td>2003</td>\n      <td>0.011400</td>\n    </tr>\n    <tr>\n      <td>2004</td>\n      <td>0.002600</td>\n    </tr>\n    <tr>\n      <td>2005</td>\n      <td>0.000200</td>\n    </tr>\n    <tr>\n      <td>2006</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>2007</td>\n      <td>0.020400</td>\n    </tr>\n    <tr>\n      <td>2008</td>\n      <td>0.000100</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/peft/utils/other.py:716: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-680eb5d7-1b9028a65cad6cef1cd38a49;8603777e-9cb2-4434-a311-62848a59c9b2)\n\nCannot access gated repo for url https://huggingface.co/google/txgemma-2b-predict/resolve/main/config.json.\nAccess to model google/txgemma-2b-predict is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in google/txgemma-2b-predict.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:246: UserWarning: Could not find a config file in google/txgemma-2b-predict - will assume that the vocabulary was not modified.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/peft/utils/other.py:716: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-680ec27d-56921eb3558e7e60294db8f0;7ece4eea-6f76-41d2-b5ce-362cb4fc6048)\n\nCannot access gated repo for url https://huggingface.co/google/txgemma-2b-predict/resolve/main/config.json.\nAccess to model google/txgemma-2b-predict is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in google/txgemma-2b-predict.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:246: UserWarning: Could not find a config file in google/txgemma-2b-predict - will assume that the vocabulary was not modified.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/peft/utils/other.py:716: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-680ecca5-3b141ee74cf014144e4a4ddd;83cdb7a0-f38f-4fc0-9e12-e6b1408815e0)\n\nCannot access gated repo for url https://huggingface.co/google/txgemma-2b-predict/resolve/main/config.json.\nAccess to model google/txgemma-2b-predict is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in google/txgemma-2b-predict.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:246: UserWarning: Could not find a config file in google/txgemma-2b-predict - will assume that the vocabulary was not modified.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/peft/utils/other.py:716: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-680ed741-2a79e2fd534e117254fc5d7d;ce16219d-a43e-4597-a04f-302154ede206)\n\nCannot access gated repo for url https://huggingface.co/google/txgemma-2b-predict/resolve/main/config.json.\nAccess to model google/txgemma-2b-predict is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in google/txgemma-2b-predict.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:246: UserWarning: Could not find a config file in google/txgemma-2b-predict - will assume that the vocabulary was not modified.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/peft/utils/other.py:716: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-680ee195-02e65ccf717c742f39e2b35d;1c62f6e1-f28b-4738-b19c-9e7ba3613148)\n\nCannot access gated repo for url https://huggingface.co/google/txgemma-2b-predict/resolve/main/config.json.\nAccess to model google/txgemma-2b-predict is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in google/txgemma-2b-predict.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:246: UserWarning: Could not find a config file in google/txgemma-2b-predict - will assume that the vocabulary was not modified.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/peft/utils/other.py:716: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-680eebda-60be7f6d1d356318087159aa;b0217f07-7f78-4f8c-b03a-b6c996ee26c0)\n\nCannot access gated repo for url https://huggingface.co/google/txgemma-2b-predict/resolve/main/config.json.\nAccess to model google/txgemma-2b-predict is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in google/txgemma-2b-predict.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:246: UserWarning: Could not find a config file in google/txgemma-2b-predict - will assume that the vocabulary was not modified.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/peft/utils/other.py:716: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-680ef651-245e484645a8ec464e512b79;ead4007e-c455-4610-a0dd-00ff09b716a9)\n\nCannot access gated repo for url https://huggingface.co/google/txgemma-2b-predict/resolve/main/config.json.\nAccess to model google/txgemma-2b-predict is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in google/txgemma-2b-predict.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:246: UserWarning: Could not find a config file in google/txgemma-2b-predict - will assume that the vocabulary was not modified.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/peft/utils/other.py:716: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-680f00a7-1e2b22b82f20df3d4d8f0ea3;cbbc53b0-223b-4249-bf19-4139e5a91bc0)\n\nCannot access gated repo for url https://huggingface.co/google/txgemma-2b-predict/resolve/main/config.json.\nAccess to model google/txgemma-2b-predict is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in google/txgemma-2b-predict.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:246: UserWarning: Could not find a config file in google/txgemma-2b-predict - will assume that the vocabulary was not modified.\n  warnings.warn(\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_31/55002933.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m )\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2243\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2244\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2245\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2246\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2247\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2558\u001b[0m                     )\n\u001b[1;32m   2559\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2560\u001b[0;31m                         \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2561\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2562\u001b[0m                     if (\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3728\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3729\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3730\u001b[0;31m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3731\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_sagemaker_mp_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3732\u001b[0m             \u001b[0mloss_mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmp_forward_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient_accumulation_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/trl/trainer/grpo_trainer.py\u001b[0m in \u001b[0;36m_prepare_inputs\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    562\u001b[0m             \u001b[0;31m# Regular generation path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0munwrap_model_for_generation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0munwrapped_model\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 564\u001b[0;31m                 prompt_completion_ids = unwrapped_model.generate(\n\u001b[0m\u001b[1;32m    565\u001b[0m                     \u001b[0mprompt_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprompt_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgeneration_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneration_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m                 )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/peft/peft_model.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1836\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_enable_peft_forward_hooks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1837\u001b[0m                     \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspecial_peft_forward_args\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1838\u001b[0;31m                     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1839\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1840\u001b[0m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, **kwargs)\u001b[0m\n\u001b[1;32m   2461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2462\u001b[0m             \u001b[0;31m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2463\u001b[0;31m             result = self._sample(\n\u001b[0m\u001b[1;32m   2464\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2465\u001b[0m                 \u001b[0mlogits_processor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprepared_logits_processor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m_sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3430\u001b[0m                 \u001b[0mis_prefill\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3431\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3432\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3433\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3434\u001b[0m             \u001b[0;31m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    963\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 965\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    966\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_requested_to_return_tuple\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mis_configured_to_return_tuple\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_top_level_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/deprecation.py\u001b[0m in \u001b[0;36mwrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    170\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFutureWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/gemma2/modeling_gemma2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, cache_position, logits_to_keep, **loss_kwargs)\u001b[0m\n\u001b[1;32m    849\u001b[0m         )\n\u001b[1;32m    850\u001b[0m         \u001b[0;31m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 851\u001b[0;31m         outputs: BaseModelOutputWithPast = self.model(\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    963\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 965\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    966\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_requested_to_return_tuple\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mis_configured_to_return_tuple\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_top_level_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/gemma2/modeling_gemma2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, cache_position, last_cache_position, **flash_attn_kwargs)\u001b[0m\n\u001b[1;32m    632\u001b[0m                 )\n\u001b[1;32m    633\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 634\u001b[0;31m                 layer_outputs = decoder_layer(\n\u001b[0m\u001b[1;32m    635\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m                     \u001b[0mposition_embeddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mposition_embeddings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/gemma2/modeling_gemma2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, position_embeddings, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, last_cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m         \u001b[0;31m# Self Attention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 322\u001b[0;31m         hidden_states, self_attn_weights = self.self_attn(\n\u001b[0m\u001b[1;32m    323\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mposition_embeddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mposition_embeddings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/gemma2/modeling_gemma2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, position_embeddings, attention_mask, past_key_value, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    247\u001b[0m                 \u001b[0mattention_interface\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mALL_ATTENTION_FUNCTIONS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_attn_implementation\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m         attn_output, attn_weights = attention_interface(\n\u001b[0m\u001b[1;32m    250\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m             \u001b[0mquery_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/gemma2/modeling_gemma2.py\u001b[0m in \u001b[0;36meager_attention_forward\u001b[0;34m(module, query, key, value, attention_mask, dropout, scaling, softcap, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0;31m# upcast attention to fp32\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m     \u001b[0mattn_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m     \u001b[0mattn_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[0mattn_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36msoftmax\u001b[0;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[1;32m   2140\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2141\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2142\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2143\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":8},{"cell_type":"code","source":"!zip -r checkpoint_2000.zip /kaggle/working/outputs/checkpoint-2000","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T04:18:59.070553Z","iopub.execute_input":"2025-04-28T04:18:59.071026Z","iopub.status.idle":"2025-04-28T04:19:04.663016Z","shell.execute_reply.started":"2025-04-28T04:18:59.070998Z","shell.execute_reply":"2025-04-28T04:19:04.662251Z"}},"outputs":[{"name":"stdout","text":"  adding: kaggle/working/outputs/checkpoint-2000/ (stored 0%)\n  adding: kaggle/working/outputs/checkpoint-2000/scaler.pt (deflated 60%)\n  adding: kaggle/working/outputs/checkpoint-2000/README.md (deflated 66%)\n  adding: kaggle/working/outputs/checkpoint-2000/rng_state.pth (deflated 25%)\n  adding: kaggle/working/outputs/checkpoint-2000/scheduler.pt (deflated 56%)\n  adding: kaggle/working/outputs/checkpoint-2000/tokenizer.model","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":" (deflated 51%)\n  adding: kaggle/working/outputs/checkpoint-2000/trainer_state.json (deflated 90%)\n  adding: kaggle/working/outputs/checkpoint-2000/optimizer.pt (deflated 10%)\n  adding: kaggle/working/outputs/checkpoint-2000/adapter_config.json (deflated 54%)\n  adding: kaggle/working/outputs/checkpoint-2000/special_tokens_map.json (deflated 76%)\n  adding: kaggle/working/outputs/checkpoint-2000/tokenizer.json (deflated 84%)\n  adding: kaggle/working/outputs/checkpoint-2000/adapter_model.safetensors (deflated 7%)\n  adding: kaggle/working/outputs/checkpoint-2000/tokenizer_config.json (deflated 96%)\n  adding: kaggle/working/outputs/checkpoint-2000/training_args.bin (deflated 51%)\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"import torch\nfrom transformers import AutoModelForCausalLM, BitsAndBytesConfig\nfrom peft import PeftModel\n\n# 1) Quant config exactly as you trained\nquant_config = BitsAndBytesConfig(\n    load_in_4bit                     = True,\n    bnb_4bit_quant_type              = \"nf4\",\n    bnb_4bit_compute_dtype           = torch.float16,\n    llm_int8_enable_fp32_cpu_offload = False,    # for pure GPU inference\n)\n\nmodel_id = \"google/txgemma-2b-predict\"\n\n# 2) Reload the base quantized model from your cache onto GPU0\nbase = AutoModelForCausalLM.from_pretrained(\n    model_id,\n    quantization_config   = quant_config,\n    device_map            = {\"\": 0},       # everything on cuda:0\n    torch_dtype           = torch.float16,\n    local_files_only      = True,          # read from ~/.cache only\n    attn_implementation   = \"eager\",\n)\n\n# 3) Re-attach the adapters from checkpoint-100\npeft_model = PeftModel.from_pretrained(\n    base,\n    \"outputs/checkpoint-2000\",  # <-- this folder holds adapter_model.safetensors etc.\n    local_files_only=True,\n)\n\n# 4) Move to GPU0 & eval\ndevice = torch.device(\"cuda:0\")\npeft_model.to(device).eval()\n\n# 5) Prepare your prompt & tokenize\nprompt = SYSTEM_PROMPT.strip() + \"\\n\\nWill there be an adverse event for this trial?\"\ninputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n\n# 6) Generate\nout_ids = peft_model.generate(\n    **inputs,\n    do_sample       = True,\n    temperature     = 0.8,\n    top_p           = 0.95,\n    max_new_tokens  = 10,\n    pad_token_id    = tokenizer.eos_token_id,\n)\n\nprint(tokenizer.decode(out_ids[0], skip_special_tokens=True))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T04:21:34.055044Z","iopub.execute_input":"2025-04-28T04:21:34.055336Z","iopub.status.idle":"2025-04-28T04:21:43.967555Z","shell.execute_reply.started":"2025-04-28T04:21:34.055312Z","shell.execute_reply":"2025-04-28T04:21:43.966891Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a74e046328b04f1f8806a7bb0f419651"}},"metadata":{}},{"name":"stderr","text":"Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n","output_type":"stream"},{"name":"stdout","text":"Respond in the following format:\n<reasoning>\n...\n</reasoning>\n<answer>\n...\n</answer>\n\nWill there be an adverse event for this trial?\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"from tqdm import tqdm\n\nfor example in dataset.select(range(10)):\n    out = peft_model.generate(\n        **tokenizer(example[\"prompt\"], return_tensors=\"pt\").to(device),\n        do_sample=False,  # greedy for clarity\n        max_new_tokens=200,\n        pad_token_id=tokenizer.eos_token_id,\n    )\n    print(tokenizer.decode(out[0], skip_special_tokens=True))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T04:21:53.721793Z","iopub.execute_input":"2025-04-28T04:21:53.722122Z","iopub.status.idle":"2025-04-28T04:21:59.000895Z","shell.execute_reply.started":"2025-04-28T04:21:53.722100Z","shell.execute_reply":"2025-04-28T04:21:58.999890Z"}},"outputs":[{"name":"stdout","text":"Respond in the following format:\n<reasoning>\n...\n</reasoning>\n<answer>\n...\n</answer>\n\nFrom the following information about a clinical trial, predict whether it would have an adverse event.\n\nTitle: Safety, Tolerability and Pharmacokinetics of Single and Repeat Doses of GSK2292767 in Healthy Participants Who Smoke Cigarettes\nSummary: This study is the first administration of GSK2292767 to humans. The study will evaluate the safety, tolerability, pharmacokinetics (PK) and pharmacodynamics (PD) of single and repeat inhaled doses of GSK2292767 in healthy smokers. This study is intended to provide sufficient confidence in the safety of the molecule and preliminary information on target engagement to allow progression to further repeat dose and proof of mechanism studies. This is a two part, single site, randomized, double-blind (sponsor open), placebo controlled study. Part A will consist of two 3-period interlocking cohorts to evaluate the safety, tolerability and pharmacokinetics of ascending single doses of GSK2292767 administered as a dry powder inhalation. Part B is planned to follow Part A and progression will be based on an acceptable safety, tolerability and pharmacokinetic profiles. Subjects will receive repeat doses of GSK2292767 once daily for 14 days during Part B.     \nPhase: 1\nDisease: Asthma\nMinimum age: 18 Years\nMaximum age: 50 Years\nHealthy volunteers: Accepts Healthy Volunteers\nInterventions: GSK2292767 50 μg blended with lactose and magnesium stearate per blister as powder for inhalation; GSK2292767 500 μg blended with lactose and magnesium stearate per blister as powder for inhalation; Lactose as powder for inhalation\nDrug: Not available\n\nAnswer:148\nRespond in the following format:\n<reasoning>\n...\n</reasoning>\n<answer>\n...\n</answer>\n\nFrom the following information about a clinical trial, predict whether it would have an adverse event.\n\nTitle: Bioequivalence Study of Favipiravir 200 mg Film Tablet (ATABAY, Turkey) Under Fasting Conditions\nSummary: A single dose of Reference product containing 200 mg favipiravir and a single dose of Test product containing 200 mg favipiravir or vice versa; administered with 240 mL of water at room temperature, in each period under fasting conditions with current pandemic precautions.     \nPhase: 1\nDisease: Bioequivalence\nMinimum age: 20 Years\nMaximum age: 40 Years\nHealthy volunteers: Accepts Healthy Volunteers\nInterventions: FAVICOVIR 200 MG FT is containing 200 mg favipiravir manufactured by Atabay, Turkey.; AVIGAN 200 mg FT is containing 200 mg favipiravir manufactured by Toyama, Japan.\nDrug: Not available\n\nAnswer:189\nRespond in the following format:\n<reasoning>\n...\n</reasoning>\n<answer>\n...\n</answer>\n\nFrom the following information about a clinical trial, predict whether it would have an adverse event.\n\nTitle: A Study To Estimate The Effect of PF-06650833 On The Pharmacokinetics (PK) of Oral Contraceptive (OC)\nSummary: This is a Phase 1, open label, fixed sequence study of the effect of multiple dose PF-06650833 on single dose OC PK in healthy female subjects.     \nPhase: 1\nDisease: Healthy\nMinimum age: 18 Years\nMaximum age: 60 Years\nHealthy volunteers: Accepts Healthy Volunteers\nInterventions: 400 mg by mouth (PO) Once daily (QD) for 11 days; Single dose of Oral tablet containing 30 ug EE and 150 ug of LN\nDrug: CC[C@H]1[C@@H](COC2=C3C=C(OC)C(=CC3=CC=N2)C(N)=O)NC(=O)[C@H]1F.[H][C@@]12CC[C@H](O)[C@@]1(C)CC[C@]1([H])C3=C(CC[C@@]21[H])C=C(O)C=C3\n\nAnswer:188\nRespond in the following format:\n<reasoning>\n...\n</reasoning>\n<answer>\n...\n</answer>\n\nFrom the following information about a clinical trial, predict whether it would have an adverse event.\n\nTitle: The Maraviroc Darunavir/Ritonavir Once Daily Pharmacokinetic Study\nSummary: This is a phase I, open label, prospective, two phase pharmacokinetic study. Subjects currently attending for HIV care at St. Mary's Hospital, London will be eligible.     \nPhase: 1\nDisease: HIV\nMinimum age: 18 Years\nMaximum age: 65 Years\nHealthy volunteers: No\nInterventions: Maraviroc 150 mg daily; daily until 10. day then stop; daily until 10. day then stop\nDrug: CC(C)C1=NN=C(C)N1[C@H]1C[C@@H]2CC[C@H](C1)N2CC[C@H](NC(=O)C1CCC(F)(F)CC1)C1=CC=CC=C1.[H][C@@]12CCO[C@]1([H])OC[C@@H]2OC(=O)N[C@@H](CC1=CC=CC=C1)[C@H](O)CN(CC(C)C)S(=O)(=O)C1=CC=C(N)C=C1\n\nAnswer:188\nRespond in the following format:\n<reasoning>\n...\n</reasoning>\n<answer>\n...\n</answer>\n\nFrom the following information about a clinical trial, predict whether it would have an adverse event.\n\nTitle: A Study of LY3009120 in Participants With Advanced Cancer or Cancer That Has Spread to Other Parts of Their Body\nSummary: The main purpose of this study is to see how safe the investigational drug known as LY3009120 is and whether it will work to help people with advanced cancer or cancer that has spread to other parts of the body.     \nPhase: 1\nDisease: Neoplasms; Neoplasm Metastasis; Melanoma; Carcinoma, Non-Small-Cell Lung; Colorectal Neoplasms\nMinimum age: 18 Years\nMaximum age: Not available\nHealthy volunteers: No\nInterventions: Administered orally.\nDrug: Not available\n\nAnswer:299\nRespond in the following format:\n<reasoning>\n...\n</reasoning>\n<answer>\n...\n</answer>\n\nFrom the following information about a clinical trial, predict whether it would have an adverse event.\n\nTitle: Safety, Tolerability and PK of Multiple-ascending Doses of Emodepside\nSummary: The study evaluates safety, tolerability, pharmacokinetics (PK) and pharmacodynamics (PD) of emodepside, after administration as a Liquid Service Formulation (LSF), over 10 days, in healthy male caucasian subjects.     \nPhase: 1\nDisease: Filariasis\nMinimum age: 18 Years\nMaximum age: 45 Years\nHealthy volunteers: Accepts Healthy Volunteers\nInterventions: Emodepside administered as an LSF oral solution (1mg/mL)\nDrug: [H][C@]1(C)OC(=O)[C@]([H])(CC(C)C)N(C)C(=O)[C@@]([H])(CC2=CC=C(C=C2)N2CCOCC2)OC(=O)[C@]([H])(CC(C)C)N(C)C(=O)[C@@]([H])(C)OC(=O)[C@]([H])(CC(C)C)N(C)C(=O)[C@@]([H])(CC2=CC=C(C=C2)N2CCOCC2)OC(=O)[C@]([H])(CC(C)C)N(C)C1=O\n\nAnswer:188\nRespond in the following format:\n<reasoning>\n...\n</reasoning>\n<answer>\n...\n</answer>\n\nFrom the following information about a clinical trial, predict whether it would have an adverse event.\n\nTitle: Drug-drug Interaction Study of Gepotidacin\nSummary: This study is a drug-drug interaction (DDI), pharmacokinetics (PK), safety and tolerability study in adult healthy participants, including Japanese cohort. This study is designed to assess co-administration of probe substrates with gepotidacin in study cohorts 1 to 3 and establishing PK and safety in Japanese participants in cohort 4. Food effect will also be evaluated in cohort 4.     \nPhase: 1\nDisease: Infections, Bacterial\nMinimum age: 18 Years\nMaximum age: 50 Years\nHealthy volunteers: Accepts Healthy Volunteers\nInterventions: Gepotidacin tablets will be available as unit dose strength 750 mg and will be administered orally.; Cimetidine tablets will be available as unit dose strength 400 mg and will be administered orally.; Rifampicin Capsules will be available as unit dose strength 300 mg and will be administered orally.; Midazolam oral syrup 2 milligrams per milliliter (mg/mL) will be available to be administered orally.; Digoxin tablets will be available as unit dose strength 0.25 mg and will be administered orally.; Placebo matching to gepotidacin tablets will be administered orally.\nDrug: O=C1C=CC2=C3N1C[C@@H](CN1CCC(CC1)NCC1=CC4=C(OCCC4)C=N1)N3C(=O)C=N2.CN\\C(NCCSCC1=C(C)NC=N1)=N\\C#N.CO[C@H]1\\C=C\\O[C@@]2(C)OC3=C(C2=O)C2=C(O)C(\\C=N\\N4CCN(C)CC4)=C(NC(=O)\\C(C)=C/C=C/[C@H](C)[C@H](O)[C@@H](C)[C@@H](O)[C@@H](C)[C@H](OC(C)=O)[C@@H]1C)C(O)=C2C(O)=C3C.CC1=NC=C2CN=C(C3=CC=CC=C3F)C3=C(C=CC(Cl)=C3)N12.[H][C@]12CC[C@]3([H])[C@]([H])(C[C@@H](O)[C@]4(C)[C@H](CC[C@]34O)C3=CC(=O)OC3)[C@@]1(C)CC[C@@H](C2)O[C@H]1C[C@H](O)[C@H](O[C@H]2C[C@H](O)[C@H](O[C@H]3C[C@H](O)[C@H](O)[C@@H](C)O3)[C@@H](C)O2)[C@@H](C)O1.O=C1C=CC2=C3N1C[C@@H](CN1CCC(CC1)NCC1=CC4=C(OCCC4)C=N1)N3C(=O)C=N2\n\nAnswer:188\nRespond in the following format:\n<reasoning>\n...\n</reasoning>\n<answer>\n...\n</answer>\n\nFrom the following information about a clinical trial, predict whether it would have an adverse event.\n\nTitle: A Study of LY3819253 (LY-CoV555) in Healthy Participants\nSummary: The purpose of this study is to test the safety and tolerability of LY3819253 when it is given by injection just under the skin to healthy participants. Blood tests will be done to check how much LY3819253 is in the bloodstream and how long the body takes to eliminate it. Participation could last up to 16 weeks and may include up to six visits to the study center, with a one-week overnight stay.     \nPhase: 1\nDisease: Healthy\nMinimum age: 18 Years\nMaximum age: 60 Years\nHealthy volunteers: Accepts Healthy Volunteers\nInterventions: Administered SC.; Administered SC.\nDrug: Not available\n\nAnswer:16\nRespond in the following format:\n<reasoning>\n...\n</reasoning>\n<answer>\n...\n</answer>\n\nFrom the following information about a clinical trial, predict whether it would have an adverse event.\n\nTitle: Vincristine, Doxorubicin, And Dexamethasone + Ixazomib in Acute Lymphoblastic Leukemia (ALL), Lymphoblastic Lymphoma Or Mixed Phenotype Acute Leukemia\nSummary: This is a phase I study of vincristine, doxorubicin and dexamethasone (modified VXD) plus MLN9708 in adults with relapsed or refractory acute lymphoblastic leukemia/lymphoma, lymphoblastic lymphoma or mixed phenotype acute leukemia.     \nPhase: 1\nDisease: Relapsed or Refractory Acute Lymphoblastic Leukemia; Relapsed or Refractory Lymphoblastic Lymphoma; Mixed Phenotype Acute Leukemia\nMinimum age: 18 Years\nMaximum age: Not available\nHealthy volunteers: No\nInterventions: Not available\nDrug: [H][C@@]12N3CC[C@@]11C4=C(C=C(OC)C(=C4)[C@]4(C[C@H]5C[N@](C[C@](O)(CC)C5)CCC5=C4NC4=CC=CC=C54)C(=O)OC)N(C=O)[C@@]1([H])[C@](O)([C@H](OC(C)=O)[C@]2(CC)C=CC3)C(=O)OC.COC1=CC=CC2=C1C(=O)C1=C(O)C3=C(C[C@](O)(C[C@@H]3O[C@H]3C[C@H](N)[C@H](O)[C@H](C)O3)C(=O)CO)C(O)=C1C2=O.[H][C@@]12C[C@@H](C)[C@](O)(C(=O)CO)[C@@]1(C)C[C@H](O)[C@@]1(F)[C@@]2([H])CCC2=CC(=O)C=C[C@]12C\n\nAnswer:188\nRespond in the following format:\n<reasoning>\n...\n</reasoning>\n<answer>\n...\n</answer>\n\nFrom the following information about a clinical trial, predict whether it would have an adverse event.\n\nTitle: Effectiveness of Ivermectin as add-on Therapy in COVID-19 Management\nSummary: Comparing the effectiveness of Ivermectin( IVM) +Hydroxychloroquin + azithromycin (AZT) group to Hydroxychloroquin (HCQ) + azithromycin (AZT)     \nPhase: 1\nDisease: COVID 19\nMinimum age: 18 Years\nMaximum age: Not available\nHealthy volunteers: No\nInterventions: Ivermectin 0.2 mg /kg (single dose at once =2 tablets of 6mg/weekly\nDrug: [H][C@@]12OC\\C3=C/C=C/[C@H](C)[C@H](O[C@H]4C[C@H](OC)[C@@H](O[C@H]5C[C@H](OC)[C@@H](O)[C@H](C)O5)[C@H](C)O4)\\C(C)=C\\C[C@]4([H])C[C@@]([H])(C[C@]5(CC[C@H](C)[C@]([H])(O5)C(C)C)O4)OC(=O)[C@]([H])(C=C(C)[C@H]1O)[C@@]23O.[H][C@@]12OC\\C3=C/C=C/[C@H](C)[C@H](O[C@H]4C[C@H](OC)[C@@H](O[C@H]5C[C@H](OC)[C@@H](O)[C@H](C)O5)[C@H](C)O4)\\C(C)=C\\C[C@]4([H])C[C@@]([H])(C[C@]5(CC[C@H](C)[C@]([H])(O5)[C@@H](C)CC)O4)OC(=O)[C@]([H])(C=C(C)[C@H]1O)[C@@]23O\n\nAnswer:186\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\nfrom transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling\nfrom datasets import load_dataset, Dataset\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\nfrom trl import GRPOConfig, GRPOTrainer, SFTTrainer, SFTConfig\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport re\n\n# -----------------------------\n# 1) Load base model & tokenizer\n# -----------------------------\nmodel_id = \"google/txgemma-2b-predict\"\ntokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True,token=secret_value_0)\nquant_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.float16,\n    llm_int8_enable_fp32_cpu_offload=True,\n)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_id,\n    token=secret_value_0,\n    quantization_config=quant_config,\n    device_map={\"\": 0},\n    torch_dtype=torch.float16,\n    attn_implementation=\"eager\",\n)\n\n# -----------------------------\n# 2) Prepare & attach LoRA\n# -----------------------------\nlora_cfg = LoraConfig(\n    r=8,\n    target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n)\nmodel = prepare_model_for_kbit_training(model)\n\n# Manually cast any float32 submodules back into fp16\nfor name, module in model.named_modules():\n    # LayerNorm and LM head / final Linear get cast to fp32 above\n    if isinstance(module, torch.nn.LayerNorm) or isinstance(module, torch.nn.Linear):\n        module.to(torch.float16)\n        \npeft_model = get_peft_model(model, lora_cfg)\n\n# -----------------------------\n# 3) Load & format raw examples\n# -----------------------------\nSYSTEM_PROMPT = \"<reasoning>\\n...\\n</reasoning>\\n<answer>\\n...\\n</answer>\\n\"\ndef fmt(x):\n    return {\n        \"prompt\": SYSTEM_PROMPT.strip() + \"\\n\\n\" + x[\"input_text\"].strip(),\n        \"answer\": x[\"output_text\"].strip()\n    }\n\nraw = load_dataset(\n    \"json\",\n    data_files=\"/kaggle/input/txgemma-datasets-trialbench-adverse-event/txgemma_datasets_trialbench_adverse-event-rate-prediction_train.jsonl\",\n    split=\"train\",\n)\ndataset = raw.map(fmt)\n\n# -----------------------------\n# 4) Tokenize for SFT (batched)\n# -----------------------------\ndef tokenize_sft(examples):\n    full_texts = [\n        p.strip() + \" \" + a.strip()\n        for p, a in zip(examples[\"prompt\"], examples[\"answer\"])\n    ]\n    tokens = tokenizer(\n        full_texts,\n        truncation=True,\n        max_length=512,\n        padding=False,\n    )\n    labels = [ids.copy() for ids in tokens[\"input_ids\"]]\n    return {\n        \"input_ids\": tokens[\"input_ids\"],\n        \"attention_mask\": tokens[\"attention_mask\"],\n        \"labels\": labels,\n    }\n\nsft_dataset = dataset.map(\n    tokenize_sft,\n    batched=True,\n    remove_columns=[\"prompt\", \"answer\"],\n)\nsft_dataset.set_format(\n    type=\"torch\",\n    columns=[\"input_ids\", \"attention_mask\", \"labels\"],\n)\n\n# -----------------------------\n# 5) Supervised fine-tuning (SFT)\n# -----------------------------\nsft_trainer = SFTTrainer(\n    model=peft_model,\n    train_dataset=sft_dataset,\n    args=SFTConfig(\n        per_device_train_batch_size=1,\n        gradient_accumulation_steps=4,\n        warmup_steps=2,\n        max_steps=50,\n        learning_rate=2e-4,\n        fp16=True,\n        logging_steps=5,\n        max_seq_length=512,\n        output_dir=\"sft/outputs\",\n        optim=\"paged_adamw_8bit\",\n        report_to=\"none\",\n    ),\n    peft_config=lora_cfg,\n    formatting_func=None,\n)\nsft_trainer.train()\n\n# -----------------------------\n# 6) Extract & display SFT metrics\n# -----------------------------\nsft_history = pd.DataFrame(sft_trainer.state.log_history)\nprint(\"SFT Logged Metrics (first & last 5 rows):\")\ndisplay(sft_history[[\"step\", \"loss\", \"learning_rate\"]].head(5))\ndisplay(sft_history[[\"step\", \"loss\", \"learning_rate\"]].tail(5))\n\nprint(\"\\nSample Raw Training Examples:\")\nfor ex in dataset.select(range(3)):\n    print(\"Prompt:\", ex[\"prompt\"])\n    print(\"Answer:\", ex[\"answer\"])\n    print(\"-\" * 40)\n\n# -----------------------------\n# 7) Reinforcement via GRPO\n# -----------------------------\ndef extract_xml_answer(text: str) -> str:\n    # exactly as before\n    answer = text.split(\"<answer>\")[-1].split(\"</answer>\")[0]\n    return answer.strip()\n\ndef count_xml(text: str) -> float:\n    count = 0.0\n    if text.count(\"<reasoning>\\n\") == 1:        count += 0.125\n    if text.count(\"\\n</reasoning>\\n\") == 1:     count += 0.125\n    if text.count(\"\\n<answer>\\n\") == 1:\n        count += 0.125\n        count -= len(text.split(\"\\n</answer>\\n\")[-1]) * 0.001\n    if text.count(\"\\n</answer>\") == 1:\n        count += 0.125\n        count -= (len(text.split(\"\\n</answer>\")[-1]) - 1) * 0.001\n    return count\n\ndef xmlcount_reward_func(completions, **kwargs) -> list[float]:\n    # completions is List[str]\n    return [count_xml(c) for c in completions]\n\ndef strict_format_reward_func(completions, **kwargs) -> list[float]:\n    pattern = r\"^<reasoning>\\n.*?\\n</reasoning>\\n<answer>\\n.*?\\n</answer>\\n$\"\n    return [0.5 if re.match(pattern, c, re.DOTALL) else 0.0 for c in completions]\n\ndef soft_format_reward_func(completions, **kwargs) -> list[float]:\n    pattern = r\"<reasoning>.*?</reasoning>\\s*<answer>.*?</answer>\"\n    return [0.5 if re.search(pattern, c, re.DOTALL) else 0.0 for c in completions]\n\ndef int_reward_func(completions, **kwargs) -> list[float]:\n    extracted = [extract_xml_answer(c) for c in completions]\n    return [0.5 if r.isdigit() else 0.0 for r in extracted]\n\ndef correctness_reward_func(prompts, completions, answer, **kwargs) -> list[float]:\n    # `answer` is List[str] of ground-truth\n    extracted = [extract_xml_answer(c) for c in completions]\n    return [2.0 if r == a else 0.0 for r, a in zip(extracted, answer)]\n    \ngrpo_args = GRPOConfig(\n    use_vllm=False,\n    learning_rate=5e-6,\n    per_device_train_batch_size=2,\n    gradient_accumulation_steps=3,\n    num_generations=2,\n    max_prompt_length=256,\n    max_completion_length=200,\n    max_steps=5000,\n    save_steps=250,\n    output_dir=\"grpo/outputs\",\n    bf16=False,\n    fp16=True,\n    weight_decay=0.1,\n    warmup_ratio=0.1,\n    lr_scheduler_type=\"cosine\",\n    optim=\"paged_adamw_8bit\",\n    logging_steps=10,\n    report_to=\"none\",\n)\ngrpo_trainer = GRPOTrainer(\n    model=peft_model,\n    processing_class=tokenizer,\n    reward_funcs=[xmlcount_reward_func, soft_format_reward_func, strict_format_reward_func, int_reward_func, correctness_reward_func],\n    args=grpo_args,\n    train_dataset=dataset,  # raw string dataset for on-the-fly formatting\n)\ngrpo_trainer.train()\n\n# -----------------------------\n# 8) Extract & display GRPO metrics\n# -----------------------------\ngrpo_history = pd.DataFrame(grpo_trainer.state.log_history)\nprint(\"GRPO Logged Metrics (first & last 5 rows):\")\ndisplay(grpo_history[[\"step\", \"loss\", \"reward\", \"learning_rate\"]].head(5))\ndisplay(grpo_history[[\"step\", \"loss\", \"reward\", \"learning_rate\"]].tail(5))\n\nprint(\"\\nSample GRPO Generations:\")\nfor ex in dataset.select(range(3))[\"prompt\"]:\n    inputs = tokenizer(ex, return_tensors=\"pt\").to(peft_model.device)\n    outputs = peft_model.generate(**inputs, max_length=200)\n    print(\"Prompt:\", ex)\n    print(\"Generation:\", tokenizer.decode(outputs[0], skip_special_tokens=True))\n    print(\"-\" * 40)\n\n# -----------------------------\n# 9) Plot Training Curves\n# -----------------------------\nplt.figure()\nif \"loss\" in grpo_history.columns:\n    plt.plot(grpo_history[\"step\"], grpo_history[\"loss\"])\n    plt.xlabel(\"Step\"); plt.ylabel(\"Loss\"); plt.title(\"GRPO Loss\")\n    plt.show()\nif \"reward\" in grpo_history.columns:\n    plt.figure()\n    plt.plot(grpo_history[\"step\"], grpo_history[\"reward\"])\n    plt.xlabel(\"Step\"); plt.ylabel(\"Reward\"); plt.title(\"GRPO Reward\")\n    plt.show()\n\n# -----------------------------\n# 10) Show all logged metric keys\n# -----------------------------\nprint(\"Available Logged Keys:\")\ndisplay(pd.DataFrame(grpo_history.columns, columns=[\"Metric\"]))\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T22:41:35.224909Z","iopub.execute_input":"2025-04-28T22:41:35.225640Z","iopub.status.idle":"2025-04-29T00:23:44.613572Z","shell.execute_reply.started":"2025-04-28T22:41:35.225617Z","shell.execute_reply":"2025-04-29T00:23:44.612287Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aeadad1c58744213a1528c4a63f9520c"}},"metadata":{}},{"name":"stderr","text":"No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [50/50 01:59, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>5</td>\n      <td>16.669800</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>9.602200</td>\n    </tr>\n    <tr>\n      <td>15</td>\n      <td>6.031300</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>4.339900</td>\n    </tr>\n    <tr>\n      <td>25</td>\n      <td>3.700300</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>3.301500</td>\n    </tr>\n    <tr>\n      <td>35</td>\n      <td>2.898900</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>2.741700</td>\n    </tr>\n    <tr>\n      <td>45</td>\n      <td>2.717000</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>2.641100</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/peft/utils/other.py:716: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-681004a5-7d9bd17d078bb2296c7bead3;94e29eec-0759-4ea8-85d6-038324d7381a)\n\nCannot access gated repo for url https://huggingface.co/google/txgemma-2b-predict/resolve/main/config.json.\nAccess to model google/txgemma-2b-predict is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in google/txgemma-2b-predict.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:246: UserWarning: Could not find a config file in google/txgemma-2b-predict - will assume that the vocabulary was not modified.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"SFT Logged Metrics (first & last 5 rows):\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"   step     loss  learning_rate\n0     5  16.6698       0.000196\n1    10   9.6022       0.000175\n2    15   6.0313       0.000154\n3    20   4.3399       0.000133\n4    25   3.7003       0.000113","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>step</th>\n      <th>loss</th>\n      <th>learning_rate</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>5</td>\n      <td>16.6698</td>\n      <td>0.000196</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>10</td>\n      <td>9.6022</td>\n      <td>0.000175</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>15</td>\n      <td>6.0313</td>\n      <td>0.000154</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>20</td>\n      <td>4.3399</td>\n      <td>0.000133</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>25</td>\n      <td>3.7003</td>\n      <td>0.000113</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1458: RuntimeWarning: invalid value encountered in greater\n  has_large_values = (abs_vals > 1e6).any()\n/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in less\n  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in greater\n  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"    step    loss  learning_rate\n6     35  2.8989       0.000071\n7     40  2.7417       0.000050\n8     45  2.7170       0.000029\n9     50  2.6411       0.000008\n10    50     NaN            NaN","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>step</th>\n      <th>loss</th>\n      <th>learning_rate</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>6</th>\n      <td>35</td>\n      <td>2.8989</td>\n      <td>0.000071</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>40</td>\n      <td>2.7417</td>\n      <td>0.000050</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>45</td>\n      <td>2.7170</td>\n      <td>0.000029</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>50</td>\n      <td>2.6411</td>\n      <td>0.000008</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>50</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"name":"stderr","text":"No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n","output_type":"stream"},{"name":"stdout","text":"\nSample Raw Training Examples:\nPrompt: <reasoning>\n...\n</reasoning>\n<answer>\n...\n</answer>\n\nFrom the following information about a clinical trial, predict whether it would have an adverse event.\n\nTitle: Safety, Tolerability and Pharmacokinetics of Single and Repeat Doses of GSK2292767 in Healthy Participants Who Smoke Cigarettes\nSummary: This study is the first administration of GSK2292767 to humans. The study will evaluate the safety, tolerability, pharmacokinetics (PK) and pharmacodynamics (PD) of single and repeat inhaled doses of GSK2292767 in healthy smokers. This study is intended to provide sufficient confidence in the safety of the molecule and preliminary information on target engagement to allow progression to further repeat dose and proof of mechanism studies. This is a two part, single site, randomized, double-blind (sponsor open), placebo controlled study. Part A will consist of two 3-period interlocking cohorts to evaluate the safety, tolerability and pharmacokinetics of ascending single doses of GSK2292767 administered as a dry powder inhalation. Part B is planned to follow Part A and progression will be based on an acceptable safety, tolerability and pharmacokinetic profiles. Subjects will receive repeat doses of GSK2292767 once daily for 14 days during Part B.     \nPhase: 1\nDisease: Asthma\nMinimum age: 18 Years\nMaximum age: 50 Years\nHealthy volunteers: Accepts Healthy Volunteers\nInterventions: GSK2292767 50 μg blended with lactose and magnesium stearate per blister as powder for inhalation; GSK2292767 500 μg blended with lactose and magnesium stearate per blister as powder for inhalation; Lactose as powder for inhalation\nDrug: Not available\n\nAnswer:\nAnswer: No\n----------------------------------------\nPrompt: <reasoning>\n...\n</reasoning>\n<answer>\n...\n</answer>\n\nFrom the following information about a clinical trial, predict whether it would have an adverse event.\n\nTitle: Bioequivalence Study of Favipiravir 200 mg Film Tablet (ATABAY, Turkey) Under Fasting Conditions\nSummary: A single dose of Reference product containing 200 mg favipiravir and a single dose of Test product containing 200 mg favipiravir or vice versa; administered with 240 mL of water at room temperature, in each period under fasting conditions with current pandemic precautions.     \nPhase: 1\nDisease: Bioequivalence\nMinimum age: 20 Years\nMaximum age: 40 Years\nHealthy volunteers: Accepts Healthy Volunteers\nInterventions: FAVICOVIR 200 MG FT is containing 200 mg favipiravir manufactured by Atabay, Turkey.; AVIGAN 200 mg FT is containing 200 mg favipiravir manufactured by Toyama, Japan.\nDrug: Not available\n\nAnswer:\nAnswer: No\n----------------------------------------\nPrompt: <reasoning>\n...\n</reasoning>\n<answer>\n...\n</answer>\n\nFrom the following information about a clinical trial, predict whether it would have an adverse event.\n\nTitle: A Study To Estimate The Effect of PF-06650833 On The Pharmacokinetics (PK) of Oral Contraceptive (OC)\nSummary: This is a Phase 1, open label, fixed sequence study of the effect of multiple dose PF-06650833 on single dose OC PK in healthy female subjects.     \nPhase: 1\nDisease: Healthy\nMinimum age: 18 Years\nMaximum age: 60 Years\nHealthy volunteers: Accepts Healthy Volunteers\nInterventions: 400 mg by mouth (PO) Once daily (QD) for 11 days; Single dose of Oral tablet containing 30 ug EE and 150 ug of LN\nDrug: CC[C@H]1[C@@H](COC2=C3C=C(OC)C(=CC3=CC=N2)C(N)=O)NC(=O)[C@H]1F.[H][C@@]12CC[C@H](O)[C@@]1(C)CC[C@]1([H])C3=C(CC[C@@]21[H])C=C(O)C=C3\n\nAnswer:\nAnswer: No\n----------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='103' max='5000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 103/5000 1:38:29 < 79:35:10, 0.02 it/s, Epoch 0.02/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>10</td>\n      <td>0.216300</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>0.186400</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>0.130000</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>0.241200</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>0.165000</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>0.138400</td>\n    </tr>\n    <tr>\n      <td>70</td>\n      <td>0.164300</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>0.167600</td>\n    </tr>\n    <tr>\n      <td>90</td>\n      <td>0.163200</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>0.220100</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_31/2420799196.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# raw string dataset for on-the-fly formatting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m )\n\u001b[0;32m--> 204\u001b[0;31m \u001b[0mgrpo_trainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;31m# -----------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2243\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2244\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2245\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2246\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2247\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2558\u001b[0m                     )\n\u001b[1;32m   2559\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2560\u001b[0;31m                         \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2561\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2562\u001b[0m                     if (\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3728\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3729\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3730\u001b[0;31m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3731\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_sagemaker_mp_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3732\u001b[0m             \u001b[0mloss_mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmp_forward_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient_accumulation_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/trl/trainer/grpo_trainer.py\u001b[0m in \u001b[0;36m_prepare_inputs\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    562\u001b[0m             \u001b[0;31m# Regular generation path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0munwrap_model_for_generation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0munwrapped_model\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 564\u001b[0;31m                 prompt_completion_ids = unwrapped_model.generate(\n\u001b[0m\u001b[1;32m    565\u001b[0m                     \u001b[0mprompt_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprompt_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgeneration_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneration_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m                 )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/peft/peft_model.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1836\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_enable_peft_forward_hooks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1837\u001b[0m                     \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspecial_peft_forward_args\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1838\u001b[0;31m                     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1839\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1840\u001b[0m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, **kwargs)\u001b[0m\n\u001b[1;32m   2461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2462\u001b[0m             \u001b[0;31m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2463\u001b[0;31m             result = self._sample(\n\u001b[0m\u001b[1;32m   2464\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2465\u001b[0m                 \u001b[0mlogits_processor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprepared_logits_processor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m_sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3430\u001b[0m                 \u001b[0mis_prefill\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3431\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3432\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3433\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3434\u001b[0m             \u001b[0;31m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    963\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 965\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    966\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_requested_to_return_tuple\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mis_configured_to_return_tuple\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_top_level_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/deprecation.py\u001b[0m in \u001b[0;36mwrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    170\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFutureWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/gemma2/modeling_gemma2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, cache_position, logits_to_keep, **loss_kwargs)\u001b[0m\n\u001b[1;32m    849\u001b[0m         )\n\u001b[1;32m    850\u001b[0m         \u001b[0;31m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 851\u001b[0;31m         outputs: BaseModelOutputWithPast = self.model(\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    963\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 965\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    966\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_requested_to_return_tuple\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mis_configured_to_return_tuple\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_top_level_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/gemma2/modeling_gemma2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, cache_position, last_cache_position, **flash_attn_kwargs)\u001b[0m\n\u001b[1;32m    619\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    620\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient_checkpointing\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 621\u001b[0;31m                 layer_outputs = self._gradient_checkpointing_func(\n\u001b[0m\u001b[1;32m    622\u001b[0m                     \u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mflash_attn_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    623\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_compile.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     30\u001b[0m                 \u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dynamo_disable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdisable_fn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdisable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py\u001b[0m in \u001b[0;36m_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    743\u001b[0m             )\n\u001b[1;32m    744\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 745\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    746\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    747\u001b[0m                 \u001b[0m_maybe_set_eval_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprior\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py\u001b[0m in \u001b[0;36mcheckpoint\u001b[0;34m(function, use_reentrant, context_fn, determinism_check, debug, *args, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m                 \u001b[0;34m\"use_reentrant=False.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m             )\n\u001b[0;32m--> 489\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mCheckpointFunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreserve\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m         gen = _checkpoint_without_reentrant_generator(\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/function.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    573\u001b[0m             \u001b[0;31m# See NOTE: [functorch vjp and autograd interaction]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m             \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_functorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap_dead_wrappers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 575\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    576\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_setup_ctx_defined\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(ctx, run_function, preserve_rng_state, *args)\u001b[0m\n\u001b[1;32m    244\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_initialized\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m                 \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhad_device_in_fwd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 246\u001b[0;31m                 \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfwd_devices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfwd_device_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_device_states\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0;31m# Save non-tensor inputs in ctx, keep a placeholder None for tensors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py\u001b[0m in \u001b[0;36mget_device_states\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"meta\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m             \u001b[0mfwd_device_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m     \u001b[0mtree_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madd_device_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0mfwd_device_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/_pytree.py\u001b[0m in \u001b[0;36mtree_map\u001b[0;34m(func, tree, is_leaf, *rests)\u001b[0m\n\u001b[1;32m    989\u001b[0m     \u001b[0mleaves\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtreespec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtree_flatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_leaf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_leaf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    990\u001b[0m     \u001b[0mflat_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mleaves\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtreespec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten_up_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrests\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 991\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtreespec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mflat_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    992\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    993\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/_pytree.py\u001b[0m in \u001b[0;36munflatten\u001b[0;34m(self, leaves)\u001b[0m\n\u001b[1;32m    828\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0munflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleaves\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIterable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mPyTree\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    829\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleaves\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 830\u001b[0;31m             \u001b[0mleaves\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleaves\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    831\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleaves\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_leaves\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m             raise ValueError(\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py\u001b[0m in \u001b[0;36madd_device_ids\u001b[0;34m(arg)\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0;32mnonlocal\u001b[0m \u001b[0mfwd_device_ids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"meta\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m             \u001b[0mfwd_device_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m     \u001b[0mtree_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madd_device_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":9},{"cell_type":"code","source":"# -----------------------------\n# 8) Extract & display GRPO metrics\n# -----------------------------\ngrpo_history = pd.DataFrame(grpo_trainer.state.log_history)\nprint(\"GRPO Logged Metrics (first & last 5 rows):\")\ndisplay(grpo_history[[\"step\", \"loss\", \"reward\", \"learning_rate\"]].head(5))\ndisplay(grpo_history[[\"step\", \"loss\", \"reward\", \"learning_rate\"]].tail(5))\n\nprint(\"\\nSample GRPO Generations:\")\nfor ex in dataset.select(range(3))[\"prompt\"]:\n    inputs = tokenizer(ex, return_tensors=\"pt\").to(peft_model.device)\n    outputs = peft_model.generate(**inputs, max_length=500)\n    print(\"Prompt:\", ex)\n    print(\"Generation:\", tokenizer.decode(outputs[0], skip_special_tokens=True))\n    print(\"-\" * 40)\n\n# -----------------------------\n# 9) Plot Training Curves\n# -----------------------------\nplt.figure()\nif \"loss\" in grpo_history.columns:\n    plt.plot(grpo_history[\"step\"], grpo_history[\"loss\"])\n    plt.xlabel(\"Step\"); plt.ylabel(\"Loss\"); plt.title(\"GRPO Loss\")\n    plt.show()\nif \"reward\" in grpo_history.columns:\n    plt.figure()\n    plt.plot(grpo_history[\"step\"], grpo_history[\"reward\"])\n    plt.xlabel(\"Step\"); plt.ylabel(\"Reward\"); plt.title(\"GRPO Reward\")\n    plt.show()\n\n# -----------------------------\n# 10) Show all logged metric keys\n# -----------------------------\nprint(\"Available Logged Keys:\")\ndisplay(pd.DataFrame(grpo_history.columns, columns=[\"Metric\"]))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-29T00:25:01.467690Z","iopub.execute_input":"2025-04-29T00:25:01.468164Z","iopub.status.idle":"2025-04-29T00:25:03.596980Z","shell.execute_reply.started":"2025-04-29T00:25:01.468134Z","shell.execute_reply":"2025-04-29T00:25:03.596166Z"}},"outputs":[{"name":"stdout","text":"GRPO Logged Metrics (first & last 5 rows):\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"   step    loss    reward  learning_rate\n0    10  0.2163  0.241667   7.000000e-08\n1    20  0.1864  0.050000   1.700000e-07\n2    30  0.1300  0.075000   2.700000e-07\n3    40  0.2412  0.091667   3.600000e-07\n4    50  0.1650  0.141667   4.600000e-07","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>step</th>\n      <th>loss</th>\n      <th>reward</th>\n      <th>learning_rate</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>10</td>\n      <td>0.2163</td>\n      <td>0.241667</td>\n      <td>7.000000e-08</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>20</td>\n      <td>0.1864</td>\n      <td>0.050000</td>\n      <td>1.700000e-07</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>30</td>\n      <td>0.1300</td>\n      <td>0.075000</td>\n      <td>2.700000e-07</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>40</td>\n      <td>0.2412</td>\n      <td>0.091667</td>\n      <td>3.600000e-07</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>50</td>\n      <td>0.1650</td>\n      <td>0.141667</td>\n      <td>4.600000e-07</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"   step    loss    reward  learning_rate\n5    60  0.1384  0.183333   5.600000e-07\n6    70  0.1643  0.091667   6.600000e-07\n7    80  0.1676  0.266667   7.600000e-07\n8    90  0.1632  0.158333   8.600000e-07\n9   100  0.2201  0.225000   9.600000e-07","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>step</th>\n      <th>loss</th>\n      <th>reward</th>\n      <th>learning_rate</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>5</th>\n      <td>60</td>\n      <td>0.1384</td>\n      <td>0.183333</td>\n      <td>5.600000e-07</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>70</td>\n      <td>0.1643</td>\n      <td>0.091667</td>\n      <td>6.600000e-07</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>80</td>\n      <td>0.1676</td>\n      <td>0.266667</td>\n      <td>7.600000e-07</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>90</td>\n      <td>0.1632</td>\n      <td>0.158333</td>\n      <td>8.600000e-07</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>100</td>\n      <td>0.2201</td>\n      <td>0.225000</td>\n      <td>9.600000e-07</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"name":"stdout","text":"\nSample GRPO Generations:\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Prompt: <reasoning>\n...\n</reasoning>\n<answer>\n...\n</answer>\n\nFrom the following information about a clinical trial, predict whether it would have an adverse event.\n\nTitle: Safety, Tolerability and Pharmacokinetics of Single and Repeat Doses of GSK2292767 in Healthy Participants Who Smoke Cigarettes\nSummary: This study is the first administration of GSK2292767 to humans. The study will evaluate the safety, tolerability, pharmacokinetics (PK) and pharmacodynamics (PD) of single and repeat inhaled doses of GSK2292767 in healthy smokers. This study is intended to provide sufficient confidence in the safety of the molecule and preliminary information on target engagement to allow progression to further repeat dose and proof of mechanism studies. This is a two part, single site, randomized, double-blind (sponsor open), placebo controlled study. Part A will consist of two 3-period interlocking cohorts to evaluate the safety, tolerability and pharmacokinetics of ascending single doses of GSK2292767 administered as a dry powder inhalation. Part B is planned to follow Part A and progression will be based on an acceptable safety, tolerability and pharmacokinetic profiles. Subjects will receive repeat doses of GSK2292767 once daily for 14 days during Part B.     \nPhase: 1\nDisease: Asthma\nMinimum age: 18 Years\nMaximum age: 50 Years\nHealthy volunteers: Accepts Healthy Volunteers\nInterventions: GSK2292767 50 μg blended with lactose and magnesium stearate per blister as powder for inhalation; GSK2292767 500 μg blended with lactose and magnesium stearate per blister as powder for inhalation; Lactose as powder for inhalation\nDrug: Not available\n\nAnswer:\nGeneration: <reasoning>\n...\n</reasoning>\n<answer>\n...\n</answer>\n\nFrom the following information about a clinical trial, predict whether it would have an adverse event.\n\nTitle: Safety, Tolerability and Pharmacokinetics of Single and Repeat Doses of GSK2292767 in Healthy Participants Who Smoke Cigarettes\nSummary: This study is the first administration of GSK2292767 to humans. The study will evaluate the safety, tolerability, pharmacokinetics (PK) and pharmacodynamics (PD) of single and repeat inhaled doses of GSK2292767 in healthy smokers. This study is intended to provide sufficient confidence in the safety of the molecule and preliminary information on target engagement to allow progression to further repeat dose and proof of mechanism studies. This is a two part, single site, randomized, double-blind (sponsor open), placebo controlled study. Part A will consist of two 3-period interlocking cohorts to evaluate the safety, tolerability and pharmacokinetics of ascending single doses of GSK2292767 administered as a dry powder inhalation. Part B is planned to follow Part A and progression will be based on an acceptable safety, tolerability and pharmacokinetic profiles. Subjects will receive repeat doses of GSK2292767 once daily for 14 days during Part B.     \nPhase: 1\nDisease: Asthma\nMinimum age: 18 Years\nMaximum age: 50 Years\nHealthy volunteers: Accepts Healthy Volunteers\nInterventions: GSK2292767 50 μg blended with lactose and magnesium stearate per blister as powder for inhalation; GSK2292767 500 μg blended with lactose and magnesium stearate per blister as powder for inhalation; Lactose as powder for inhalation\nDrug: Not available\n\nAnswer:\nYes\n----------------------------------------\nPrompt: <reasoning>\n...\n</reasoning>\n<answer>\n...\n</answer>\n\nFrom the following information about a clinical trial, predict whether it would have an adverse event.\n\nTitle: Bioequivalence Study of Favipiravir 200 mg Film Tablet (ATABAY, Turkey) Under Fasting Conditions\nSummary: A single dose of Reference product containing 200 mg favipiravir and a single dose of Test product containing 200 mg favipiravir or vice versa; administered with 240 mL of water at room temperature, in each period under fasting conditions with current pandemic precautions.     \nPhase: 1\nDisease: Bioequivalence\nMinimum age: 20 Years\nMaximum age: 40 Years\nHealthy volunteers: Accepts Healthy Volunteers\nInterventions: FAVICOVIR 200 MG FT is containing 200 mg favipiravir manufactured by Atabay, Turkey.; AVIGAN 200 mg FT is containing 200 mg favipiravir manufactured by Toyama, Japan.\nDrug: Not available\n\nAnswer:\nGeneration: <reasoning>\n...\n</reasoning>\n<answer>\n...\n</answer>\n\nFrom the following information about a clinical trial, predict whether it would have an adverse event.\n\nTitle: Bioequivalence Study of Favipiravir 200 mg Film Tablet (ATABAY, Turkey) Under Fasting Conditions\nSummary: A single dose of Reference product containing 200 mg favipiravir and a single dose of Test product containing 200 mg favipiravir or vice versa; administered with 240 mL of water at room temperature, in each period under fasting conditions with current pandemic precautions.     \nPhase: 1\nDisease: Bioequivalence\nMinimum age: 20 Years\nMaximum age: 40 Years\nHealthy volunteers: Accepts Healthy Volunteers\nInterventions: FAVICOVIR 200 MG FT is containing 200 mg favipiravir manufactured by Atabay, Turkey.; AVIGAN 200 mg FT is containing 200 mg favipiravir manufactured by Toyama, Japan.\nDrug: Not available\n\nAnswer: Yes\n----------------------------------------\nPrompt: <reasoning>\n...\n</reasoning>\n<answer>\n...\n</answer>\n\nFrom the following information about a clinical trial, predict whether it would have an adverse event.\n\nTitle: A Study To Estimate The Effect of PF-06650833 On The Pharmacokinetics (PK) of Oral Contraceptive (OC)\nSummary: This is a Phase 1, open label, fixed sequence study of the effect of multiple dose PF-06650833 on single dose OC PK in healthy female subjects.     \nPhase: 1\nDisease: Healthy\nMinimum age: 18 Years\nMaximum age: 60 Years\nHealthy volunteers: Accepts Healthy Volunteers\nInterventions: 400 mg by mouth (PO) Once daily (QD) for 11 days; Single dose of Oral tablet containing 30 ug EE and 150 ug of LN\nDrug: CC[C@H]1[C@@H](COC2=C3C=C(OC)C(=CC3=CC=N2)C(N)=O)NC(=O)[C@H]1F.[H][C@@]12CC[C@H](O)[C@@]1(C)CC[C@]1([H])C3=C(CC[C@@]21[H])C=C(O)C=C3\n\nAnswer:\nGeneration: <reasoning>\n...\n</reasoning>\n<answer>\n...\n</answer>\n\nFrom the following information about a clinical trial, predict whether it would have an adverse event.\n\nTitle: A Study To Estimate The Effect of PF-06650833 On The Pharmacokinetics (PK) of Oral Contraceptive (OC)\nSummary: This is a Phase 1, open label, fixed sequence study of the effect of multiple dose PF-06650833 on single dose OC PK in healthy female subjects.     \nPhase: 1\nDisease: Healthy\nMinimum age: 18 Years\nMaximum age: 60 Years\nHealthy volunteers: Accepts Healthy Volunteers\nInterventions: 400 mg by mouth (PO) Once daily (QD) for 11 days; Single dose of Oral tablet containing 30 ug EE and 150 ug of LN\nDrug: CC[C@H]1[C@@H](COC2=C3C=C(OC)C(=CC3=CC=N2)C(N)=O)NC(=O)[C@H]1F.[H][C@@]12CC[C@H](O)[C@@]1(C)CC[C@]1([H])C3=C(CC[C@@]21[H])C=C(O)C=C3\n\nAnswer: Yes\n----------------------------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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\n"},"metadata":{}},{"name":"stdout","text":"Available Logged Keys:\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"                               Metric\n0                                loss\n1                           grad_norm\n2                       learning_rate\n3        rewards/xmlcount_reward_func\n4     rewards/soft_format_reward_func\n5   rewards/strict_format_reward_func\n6             rewards/int_reward_func\n7     rewards/correctness_reward_func\n8                              reward\n9                          reward_std\n10                  completion_length\n11                                 kl\n12                              epoch\n13                               step","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Metric</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>loss</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>grad_norm</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>learning_rate</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>rewards/xmlcount_reward_func</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>rewards/soft_format_reward_func</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>rewards/strict_format_reward_func</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>rewards/int_reward_func</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>rewards/correctness_reward_func</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>reward</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>reward_std</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>completion_length</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>kl</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>epoch</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>step</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":12}]}